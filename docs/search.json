[
  {
    "objectID": "session3/session3.html",
    "href": "session3/session3.html",
    "title": "Python Logistic Regression Demo",
    "section": "",
    "text": "Cancer Dataset\n\n  \n\nDownload Follow Along File\n\n\nWe are using the same dataset as in session 3! There is no need to re-download it!"
  },
  {
    "objectID": "session3/session3.html#links",
    "href": "session3/session3.html#links",
    "title": "Python Logistic Regression Demo",
    "section": "",
    "text": "Cancer Dataset\n\n  \n\nDownload Follow Along File\n\n\nWe are using the same dataset as in session 3! There is no need to re-download it!"
  },
  {
    "objectID": "session3/session3.html#getting-started",
    "href": "session3/session3.html#getting-started",
    "title": "Python Logistic Regression Demo",
    "section": "Getting Started",
    "text": "Getting Started\nBefore doing anything else, we should first activate the conda environment we want to use.\n\n\nRefresher: How to activate conda environment\n\n\n\n\nFrom terminal, type:\n\n&gt; conda activate ENVNAME\n\n\n\n\nWhen in VS code, you might get a popup message like the one below, confirming that the environment was activated:\n\nSelected conda environment was successfully activated, even though “(ENVNAME)” indicator may not be present in the terminal prompt.\n\nor\nIn Anaconda Navagator, click on the Environments tab on the left and select the environment you want to activate. Just selecting the environment should activate it.\n\nIf we want to make sure we have the packages we’ll need installed in the environment before we try to import them, we can either check on anaconda or use the terminal:\n\n&gt; conda list\n\n\n\n\nOtherwise, we will get an error message if we try to import packages that are not installed.\n\n\nRefresher: How to install packages\n\nTo install packages, we can either use the “anaconda” dashboard, or we can use the command line. Make sure your environment is active before installing packages or the packages will not be available in your environment.\nTo install from the command line, we open a terminal and type:\n\n&gt; conda install {package}\n\nor\n\n&gt; pip install {package}\n\nWhen working with conda environments, it’s best practice to install everything with conda and only use pip for packages that are not available through conda!"
  },
  {
    "objectID": "session3/session3.html#step-1-import-packages",
    "href": "session3/session3.html#step-1-import-packages",
    "title": "Python Logistic Regression Demo",
    "section": "Step 1: Import Packages",
    "text": "Step 1: Import Packages\nSimilar to library() in R, we’ll use import in Python. Fill in the blanks to import the necessary packages:\n\nimport pandas as ___\nimport numpy as ___\nimport seaborn as ___\nimport matplotlib.pyplot as ___\n\nimport statsmodels.api as __\nimport statsmodels.formula.api as ___\n\n# Import from sklearn\nfrom sklearn.model_selection import ___\nfrom sklearn.preprocessing import ____\nfrom sklearn.decomposition import ___\n\n\nfrom sklearn.linear_model import ___\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\n\n\n\nClick to reveal answers\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n## import from sklearn (scikit-learn)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nfrom sklearn.feature_selection import SelectKBest, f_classif"
  },
  {
    "objectID": "session3/session3.html#step-2-read-in-data-and-perform-data-cleaning",
    "href": "session3/session3.html#step-2-read-in-data-and-perform-data-cleaning",
    "title": "Python Logistic Regression Demo",
    "section": "Step 2: Read in Data and Perform Data Cleaning",
    "text": "Step 2: Read in Data and Perform Data Cleaning\nWe can use the read_csv() function from the pandas package to read in the dataset.\n\ndata = pd.read_csv(\"__________\")\n\n\n\n\nClick to reveal answers\n\n\ndata = pd.read_csv(\"example_data/Cancer_Data.csv\")\n\n\n\nWe can use the .info() function to show some basic information about the dataset like:\n* the number of rows\n* number of columns\n* column labels\n* column type\n* number of non-null values in each column\n\ndata._______()\n\n\n\n\nClick to reveal answers\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 33 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   id                       569 non-null    int64  \n 1   diagnosis                569 non-null    object \n 2   radius_mean              569 non-null    float64\n 3   texture_mean             569 non-null    float64\n 4   perimeter_mean           569 non-null    float64\n 5   area_mean                569 non-null    float64\n 6   smoothness_mean          569 non-null    float64\n 7   compactness_mean         569 non-null    float64\n 8   concavity_mean           569 non-null    float64\n 9   concave points_mean      569 non-null    float64\n 10  symmetry_mean            569 non-null    float64\n 11  fractal_dimension_mean   569 non-null    float64\n 12  radius_se                569 non-null    float64\n 13  texture_se               569 non-null    float64\n 14  perimeter_se             569 non-null    float64\n 15  area_se                  569 non-null    float64\n 16  smoothness_se            569 non-null    float64\n 17  compactness_se           569 non-null    float64\n 18  concavity_se             569 non-null    float64\n 19  concave points_se        569 non-null    float64\n 20  symmetry_se              569 non-null    float64\n 21  fractal_dimension_se     569 non-null    float64\n 22  radius_worst             569 non-null    float64\n 23  texture_worst            569 non-null    float64\n 24  perimeter_worst          569 non-null    float64\n 25  area_worst               569 non-null    float64\n 26  smoothness_worst         569 non-null    float64\n 27  compactness_worst        569 non-null    float64\n 28  concavity_worst          569 non-null    float64\n 29  concave points_worst     569 non-null    float64\n 30  symmetry_worst           569 non-null    float64\n 31  fractal_dimension_worst  569 non-null    float64\n 32  Unnamed: 32              0 non-null      float64\ndtypes: float64(31), int64(1), object(1)\nmemory usage: 146.8+ KB\n\n\n\n\nFrom the info, we can see that the column types make sense and most of the columns have no missing values.\nWe do have this extra column called “Unnamed: 32” with 0 non-null values… so let’s drop it (remove it from the dataframe). We can also replace spaces in column names with “_“, which will be useful later.\n\ndata.drop(columns=\"Unnamed: 32\", inplace=______)\ndata.columns = data.columns.str.replace(\"?\", \"?\")\n# Check that the column was removed and column names were changed.\nprint(data.info())\n\n\n\n\nClick to reveal answers\n\n\n## `inplace` means that we modify the original dataframe\ndata.drop(columns=\"Unnamed: 32\", inplace=True)\ndata.columns = data.columns.str.replace(\" \", \"_\")\n## check that the column was removed\nprint(data.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 32 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   id                       569 non-null    int64  \n 1   diagnosis                569 non-null    object \n 2   radius_mean              569 non-null    float64\n 3   texture_mean             569 non-null    float64\n 4   perimeter_mean           569 non-null    float64\n 5   area_mean                569 non-null    float64\n 6   smoothness_mean          569 non-null    float64\n 7   compactness_mean         569 non-null    float64\n 8   concavity_mean           569 non-null    float64\n 9   concave_points_mean      569 non-null    float64\n 10  symmetry_mean            569 non-null    float64\n 11  fractal_dimension_mean   569 non-null    float64\n 12  radius_se                569 non-null    float64\n 13  texture_se               569 non-null    float64\n 14  perimeter_se             569 non-null    float64\n 15  area_se                  569 non-null    float64\n 16  smoothness_se            569 non-null    float64\n 17  compactness_se           569 non-null    float64\n 18  concavity_se             569 non-null    float64\n 19  concave_points_se        569 non-null    float64\n 20  symmetry_se              569 non-null    float64\n 21  fractal_dimension_se     569 non-null    float64\n 22  radius_worst             569 non-null    float64\n 23  texture_worst            569 non-null    float64\n 24  perimeter_worst          569 non-null    float64\n 25  area_worst               569 non-null    float64\n 26  smoothness_worst         569 non-null    float64\n 27  compactness_worst        569 non-null    float64\n 28  concavity_worst          569 non-null    float64\n 29  concave_points_worst     569 non-null    float64\n 30  symmetry_worst           569 non-null    float64\n 31  fractal_dimension_worst  569 non-null    float64\ndtypes: float64(30), int64(1), object(1)\nmemory usage: 142.4+ KB\nNone\n\n\n\n\nThe column was successfully removed!\nNow, we can use .head(5) to show the first 5 rows of the dataset (rows 0-4). Remember that the first row is “0” not “1”!\n\ndata.head(5)\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave_points_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave_points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n842302\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n842517\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n84300903\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n84348301\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n84358402\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 32 columns\n\n\n\n\nRecoding a Variable\nFor our logistic regression, the diagnosis column, which is our outcome of interest, should be 0, 1 not B, M. To fix this, we can use a dictionary and .map(). We could also use a lambda function like we did in Session 3, but dictionaries can be more convenient if there are more than 2 values to be recoded.\n\n## define a dictionary\ny_recode = {\"B\": ___, \"M\": ___}\n\n## use .map to locate the keys in the column and replace with values\ndata[\"diagnosis\"] = data[\"diagnosis\"].map(________)\n\ndata.head(5)\n\n\n\n\nClick to reveal answers\n\n\n## define a dictionary\ny_recode = {\"B\": 0, \"M\": 1}\n\n## use .map() to locate the keys in the column and replace with values\n## B becomes 0, M becomes 1\ndata[\"diagnosis\"] = data[\"diagnosis\"].map(y_recode)\n\ndata.head(5)\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave_points_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave_points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n842302\n1\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n842517\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n84300903\n1\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n84348301\n1\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n84358402\n1\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 32 columns"
  },
  {
    "objectID": "session3/session3.html#step-3-exploratory-data-analysis",
    "href": "session3/session3.html#step-3-exploratory-data-analysis",
    "title": "Python Logistic Regression Demo",
    "section": "Step 3: Exploratory Data Analysis",
    "text": "Step 3: Exploratory Data Analysis\nNow that our data is cleaned and we have our outcome in numeric form, we can use .describe() to get summary statistics for each column of the dataset.\n\n___.___()\n\n\n\n\nClick to reveal answers\n\n\ndata.describe()\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave_points_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave_points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\ncount\n5.690000e+02\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n...\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n\n\nmean\n3.037183e+07\n0.372583\n14.127292\n19.289649\n91.969033\n654.889104\n0.096360\n0.104341\n0.088799\n0.048919\n...\n16.269190\n25.677223\n107.261213\n880.583128\n0.132369\n0.254265\n0.272188\n0.114606\n0.290076\n0.083946\n\n\nstd\n1.250206e+08\n0.483918\n3.524049\n4.301036\n24.298981\n351.914129\n0.014064\n0.052813\n0.079720\n0.038803\n...\n4.833242\n6.146258\n33.602542\n569.356993\n0.022832\n0.157336\n0.208624\n0.065732\n0.061867\n0.018061\n\n\nmin\n8.670000e+03\n0.000000\n6.981000\n9.710000\n43.790000\n143.500000\n0.052630\n0.019380\n0.000000\n0.000000\n...\n7.930000\n12.020000\n50.410000\n185.200000\n0.071170\n0.027290\n0.000000\n0.000000\n0.156500\n0.055040\n\n\n25%\n8.692180e+05\n0.000000\n11.700000\n16.170000\n75.170000\n420.300000\n0.086370\n0.064920\n0.029560\n0.020310\n...\n13.010000\n21.080000\n84.110000\n515.300000\n0.116600\n0.147200\n0.114500\n0.064930\n0.250400\n0.071460\n\n\n50%\n9.060240e+05\n0.000000\n13.370000\n18.840000\n86.240000\n551.100000\n0.095870\n0.092630\n0.061540\n0.033500\n...\n14.970000\n25.410000\n97.660000\n686.500000\n0.131300\n0.211900\n0.226700\n0.099930\n0.282200\n0.080040\n\n\n75%\n8.813129e+06\n1.000000\n15.780000\n21.800000\n104.100000\n782.700000\n0.105300\n0.130400\n0.130700\n0.074000\n...\n18.790000\n29.720000\n125.400000\n1084.000000\n0.146000\n0.339100\n0.382900\n0.161400\n0.317900\n0.092080\n\n\nmax\n9.113205e+08\n1.000000\n28.110000\n39.280000\n188.500000\n2501.000000\n0.163400\n0.345400\n0.426800\n0.201200\n...\n36.040000\n49.540000\n251.200000\n4254.000000\n0.222600\n1.058000\n1.252000\n0.291000\n0.663800\n0.207500\n\n\n\n\n8 rows × 32 columns\n\n\n\n\n\nThe count column tells us the number of non-null (non-missing) values in a column.\n\nCreating Descriptive Plots\nWe can also look at the number of each diagnosis reflected in the dataset in a plot using seaborn.\nYou can also save a plot to a variable (ex: ‘p’) if you want to display it later with plt.show(p).\n\nsns.countplot(x=\"_________\", hue=\"_________\", data=______)\nplt.title(\"Distribution of Diagnoses\")\n_____\n\n\n\n\nClick to reveal answers\n\n\nsns.countplot(x=\"diagnosis\", hue=\"diagnosis\", data=data)\nplt.title(\"Distribution of Diagnoses\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIf we want, we can change the colors of the plot. To make the plot a bit more useful, we can also change the y-scale from “count” to “percentage” and add labels so it is clear what “0” and “1” mean.\nTo help us pick colors, we can use sns.color_palette() which will display an image with the colors in the palette.\n\nsns.color_palette(\"colorblind\")\n\n\n\n\nTo change the colors of our plot, we can make a dictionary with the values of ‘diagnosis’ as keys and the hexcodes of the colors we want to use as values.\nWe can get the hex codes of colors from a seaborn palette using sns.color_palette().as_hex().\n\ncolor_hex = sns.color_palette(\"colorblind\")._____\n\nprint(\"The hexcodes for the 'colorblind' palette are:\\n\", ____)\n\n## if we want to make the columns green for benign and yellow for malignant\n\n## the \"-\" lets us index from the end of the list rather than the front. However, the '-1'th position is the last position (there is no '-0')\n\ncolors = {0: color_hex[__], 1: color_hex[__]}\n\n\n\n\nClick to reveal answers\n\n\ncolor_hex = sns.color_palette(\"colorblind\").as_hex()\n\nprint(\"The hexcodes for the 'colorblind' palette are:\\n\", color_hex)\n\n## if we want to make the columns green for benign and yellow for malignant\n\n## the \"-\" lets us index from the end of the list rather than the front.However, the '-1'th position is the last position (there is no '-0')\n\ncolors = {0: color_hex[2], 1: color_hex[-2]}\n\nThe hexcodes for the 'colorblind' palette are:\n ['#0173b2', '#de8f05', '#029e73', '#d55e00', '#cc78bc', '#ca9161', '#fbafe4', '#949494', '#ece133', '#56b4e9']\n\n\n\n\nWe then create the plot and tell seaborn to use ‘colors’ as the palette for the graph. We can also change the ‘stat’ to be “percent”, which can be more interpretable than raw counts.\nWe can also change the xtick labels to be “Benign” and “Malignant” instead of “0” and “1”. Because we assigned the plot to the variable ‘p’, we can use p.{} to change attributes of plot ‘p’.\nWe will also change the axis labels and set a title. Once we make these changes, we can show the finished plot.\n\np = sns.countplot(\n    x=\"___\",\n    hue=\"___\",\n    stat=\"___\",\n    data=data,\n    palette=colors,\n    legend=False,\n)\n\n## change the xticklabels to benign and malignant\np.set_xticks([0, 1])\np.set_xticklabels([\"___\", \"\"])\n\n## change the axes labels and title\np.set(xlabel=\"___\", ylabel=\"___\", title=\"Distribution of Diagnoses\")\n\n## add legend\nplt.legend(title=\"Diagnosis\", loc=\"upper right\", labels=[\"Benign\", \"Malignant\"])\n\n## show plot\nplt.show(p)\n\n\n\n\nClick to reveal answers\n\n\np = sns.countplot(\n    x=\"diagnosis\",\n    hue=\"diagnosis\",\n    stat=\"percent\",\n    data=data,\n    palette=colors,\n    legend=False,\n)\n\n## change the xticklabels to benign and malignant\np.set_xticks([0, 1])\np.set_xticklabels([\"Benign\", \"Malignant\"])\n\n## change the axes labels and title\np.set(xlabel=\"Diagnosis\", ylabel=\"Percent\", title=\"Distribution of Diagnoses\")\n\n## add legend\nplt.legend(title=\"Diagnosis\", loc=\"upper right\", labels=[\"Benign\", \"Malignant\"])\n\n## show plot\nplt.show(p)\n\n\n\n\n\n\n\n\n\n\nIf we wanted to, we could also make a correlation heatmap of our features using .corr() and sns.heatmap().\nFor this, all of our columns must be numeric, and we should remove the ‘id’ column as it is not useful for correlation. We use .select_dtypes() to select only the numeric columns from the dataset.\n\nnumeric_data = data.select_dtypes(include=___)\n\n## drop id column\nnumeric_data.drop(columns=___, inplace=___)\n\n## set figure size\nplt.figure(figsize=(20, 20))\n\n## use corr function and seaborn heatmap to create correlation heatmap\n## 'fmt' allows us to choose the number display format for the heatmap\n\nsns.heatmap(numeric_data.___, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n\n## set plot title and show plot\nplt.title(\"Feature Correlation Heatmap\")\n\nplt.___\n\n\n\n\nClick to reveal answers\n\n\nnumeric_data = data.select_dtypes(include=[np.number])\n\n## drop id column\nnumeric_data.drop(columns=\"id\", inplace=True)\n\n## set figure size\nplt.figure(figsize=(20, 20))\n\n## use corr function and seaborn heatmap to create correlation heatmap\n## 'fmt' allows us to choose the number display format for the heatmap\n\nsns.heatmap(numeric_data.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n\n## set plot title and show plot\nplt.title(\"Feature Correlation Heatmap\")\n\nplt.show()"
  },
  {
    "objectID": "session3/session3.html#step-4-creating-a-logistic-regression-model",
    "href": "session3/session3.html#step-4-creating-a-logistic-regression-model",
    "title": "Python Logistic Regression Demo",
    "section": "Step 4: Creating a Logistic Regression Model",
    "text": "Step 4: Creating a Logistic Regression Model\nHere we will explore two methods for creating a logistic regression model. The first, statsmodels, is more similar to R and is more user-friendly for statistical purposes. The second, scikit-learn, is more useful for machine learning and prediction models, but is a framework that is worth learning if you are going to use python often.\n\nMethod 1: Statsmodels\nThe statsmodels package is a python package for creating statistical models, conducting tests and performing data exploration. It is similar to packages used in R and creates an r-like model summary.\nIf we wanted to see if higher values of area_mean and texture_mean are associated with a higher probability of malignancy, we can use smf.logit() to fit a logistic regression model.\n\nlogit = smf.logit(\"___ ~ ___ + ___\", data=data).fit()\n\nprint(logit.summary())\n\n\n\n\nClick to reveal answers\n\n\nlogit = smf.logit(\"diagnosis ~ area_mean + texture_mean\", data=data).fit()\n\nprint(logit.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.253932\n         Iterations 8\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:              diagnosis   No. Observations:                  569\nModel:                          Logit   Df Residuals:                      566\nMethod:                           MLE   Df Model:                            2\nDate:                Mon, 11 Nov 2024   Pseudo R-squ.:                  0.6154\nTime:                        13:42:59   Log-Likelihood:                -144.49\nconverged:                       True   LL-Null:                       -375.72\nCovariance Type:            nonrobust   LLR p-value:                3.776e-101\n================================================================================\n                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept      -12.2437      1.151    -10.634      0.000     -14.500      -9.987\narea_mean        0.0120      0.001     10.172      0.000       0.010       0.014\ntexture_mean     0.2115      0.037      5.745      0.000       0.139       0.284\n================================================================================\n\n\nFrom the summary, we can see that the area_mean and texture_mean are both associated with an increased probability of malignancy.\n\n\n\n\n\nAside: We can also use feature selection tools from the scikit-learn package to select what features to use.\n\nScikit learn requires the outcome and predictor variables to be split into two data frames.\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nX_raw = data.loc[:, \"radius_mean\"::]\n## set only the diagnosis column as \"y\"\ny = data.loc[:, \"diagnosis\"]\n\n# Select top k features based on ANOVA F-value between feature and target\nselector = SelectKBest(f_classif, k=5)  # Choose 'k' to specify number of features\nX_selected = selector.fit_transform(X_raw, y)\nselected_feature_names = X_raw.columns[selector.get_support()]\n\n## make model eqn\nformula = \"diagnosis ~\" + \"+\".join(selected_feature_names)\nsm_model = smf.logit(formula, data=data).fit()\n\nprint(sm_model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.109447\n         Iterations 10\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:              diagnosis   No. Observations:                  569\nModel:                          Logit   Df Residuals:                      563\nMethod:                           MLE   Df Model:                            5\nDate:                Mon, 11 Nov 2024   Pseudo R-squ.:                  0.8343\nTime:                        13:42:59   Log-Likelihood:                -62.275\nconverged:                       True   LL-Null:                       -375.72\nCovariance Type:            nonrobust   LLR p-value:                3.129e-133\n========================================================================================\n                           coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nIntercept              -18.7401      2.741     -6.836      0.000     -24.113     -13.367\nperimeter_mean          -0.2537      0.073     -3.465      0.001      -0.397      -0.110\nconcave_points_mean     66.7337     22.129      3.016      0.003      23.362     110.105\nradius_worst             1.8164      0.545      3.336      0.001       0.749       2.884\nperimeter_worst          0.0651      0.081      0.802      0.422      -0.094       0.224\nconcave_points_worst    17.8708     11.007      1.624      0.104      -3.703      39.445\n========================================================================================\n\nPossibly complete quasi-separation: A fraction 0.21 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified.\n\n\n\n\n\n\nMethod 2: Scikit-learn\nThe scikit-learn pacakge is geared towards machine-learning and prediction-related tasks like classification, clustering and dimensionality reduction.\nFitting models with scikit-learn is a bit more complex than with statsmodels but is more along the lines of what most python projects will require.\nInstead of fitting a logistic regression model on the full dataset like we did with statsmodels, this time we are going to fit on a subset of our data and create a prediction model. We will test this prediction model on the remainder of the dataset.\n\n\nSplitting Training and Test Data\nTo fit a prediction model with sci-kit learn…\nWe first need to split the dataset into X (predictors/features) and y (outcomes). Then we use the train_test_split() function to split these datasets into a training dataset and a test dataset.\nWe use the .loc function and “:” to select all rows and any columns including and after “radius_mean”, and we assign these columns to x. This excludes the “diagnosis” and “id” columns.\nWe set y as simply the diagnosis column.\nWhen splitting our dataset, we can define ‘test_size’ which is the proportion of the data that will be set aside for testing the model. We can also set a random_state.\n\nUnlike R, Python allows for multi-argument returns from functions. This lets us assign each returned object to a different variable to be used later!\n\n\nX = data.loc[:, \"___\"::]\n\n## set only the diagnosis column as \"y\"\ny = data.loc[:, \"___\"]\n\n## here we assign each object returned from `train_test_split` to a different variable\n## we can use test_size to set the proportion of the dataset reserved for testing\nX_?, X_?, y_?, y_? = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nX_train.head(3)\n\n\n\n\nClick to reveal answers\n\n\nX = data.loc[:, \"radius_mean\"::]\n\n## set only the diagnosis column as \"y\"\ny = data.loc[:, \"diagnosis\"]\n\n## here we assign each object returned from `train_test_split` to a different variable\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nX_train.head(3)\n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave_points_mean\nsymmetry_mean\nfractal_dimension_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave_points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n68\n9.029\n17.33\n58.79\n250.5\n0.10660\n0.14130\n0.31300\n0.04375\n0.2111\n0.08046\n...\n10.31\n22.65\n65.50\n324.7\n0.14820\n0.4365\n1.2520\n0.17500\n0.4228\n0.1175\n\n\n181\n21.090\n26.57\n142.70\n1311.0\n0.11410\n0.28320\n0.24870\n0.14960\n0.2395\n0.07398\n...\n26.68\n33.48\n176.50\n2089.0\n0.14910\n0.7584\n0.6780\n0.29030\n0.4098\n0.1284\n\n\n63\n9.173\n13.86\n59.20\n260.9\n0.07721\n0.08751\n0.05988\n0.02180\n0.2341\n0.06963\n...\n10.01\n19.23\n65.59\n310.1\n0.09836\n0.1678\n0.1397\n0.05087\n0.3282\n0.0849\n\n\n\n\n3 rows × 30 columns\n\n\n\n\n\n\n\nScaling/Normalizing Data\nBecause all of our features have different scales, we need to standardize (normalize) our dataset. We can do this by creating an instance of the StandardScaler class called “scaler” and fitting that to the training data. We then use the same “scaler” to scale the test dataset.\n\n## standardize dataset\nscaler = ___()\n\n## fit the scaler to the _ data\nscaler.fit(___)\n\n## apply the scaler to the _ data and _ data\nX_train = scaler.transform(___)\nX_test = scaler.transform(___)\n\n\n\n\nClick to reveal answers\n\n\n## standardize dataset\nscaler = StandardScaler()\n\n## fit the scaler to the TRAINING data\nscaler.fit(X_train)\n\n## apply the scaler to BOTH the training and test data\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n\n\n\nAfter scaling the data, we can perform dimensional reduction with PCA\nPCA is often used for dimensional reduction with machine learning methods so we will demonstrate it here. We can set up the PCA transformer in the same way that we set the scaler above.\n\n## set up PCA transformer with the number of components you want and fit to training dataset\npca = PCA(n_components=__)\npca = pca.fit(___)\n\n## apply PCA transformer to training and test set\nX_train_pca = pca.transform(___)\nX_test_pca = pca.transform(___)\n\n\n\n\nClick to reveal answers\n\n\n## set up PCA transformer with the number of components you want and fit to training dataset\npca = PCA(n_components=10)\npca = pca.fit(X_train)\n\n## apply PCA transformer to training and test set\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\n\n\n\n\n## we can look at the cumulative explained variance\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Cumulative Explained Variance\")\nplt.show()"
  },
  {
    "objectID": "session3/session3.html#step-5-model-setup",
    "href": "session3/session3.html#step-5-model-setup",
    "title": "Python Logistic Regression Demo",
    "section": "Step 5: Model Setup",
    "text": "Step 5: Model Setup\nNext we have to set up the model itself by creating an instance of the LogisticRegression model class.\n\nlr = ___\n\n\n\n\nClick to reveal answers\n\n\nlr = LogisticRegression()\n\n\n\nThen, we can fit this model to the training data.\n\n## fit to training data\nlr.___(X_train_pca, y_train)\n\n\n\n\nClick to reveal answers\n\n\n## fit to training data\nlr.fit(X_train_pca, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression()"
  },
  {
    "objectID": "session3/session3.html#step-6-look-at-results",
    "href": "session3/session3.html#step-6-look-at-results",
    "title": "Python Logistic Regression Demo",
    "section": "Step 6: Look At Results",
    "text": "Step 6: Look At Results\nOnce the model is fit, we can use it to predict the outcome (diagnosis) based on the features of the test data.\n\nStore Results in a Dataframe\nWe can use pd.DataFrame() to create an empty pandas dataframe that we can fill with our results.\n\n## use model to predict test data\n## set up dataframe to review results\nresults = pd.___\n\n## get predicted\nresults.loc[:, 'Predicted']= lr.___(___)\n\n## get true y values for test dataset\nresults.loc[:, 'Truth'] = ___.___\n\n## get probability of being malignant\n## the output is one probability per outcome, we only want the second outcome (malignant)\nresults.loc[:, 'Probability: Malignant'] = pd.DataFrame(lr.___(X_test_pca))[_]\n\nresults.head(5)\n\n\n\n\nClick to reveal answers\n\n\n## use model to predict test data\n## set up dataframe to review results\nresults = pd.DataFrame()\n\n## get predicted\nresults.loc[:, \"Predicted\"] = lr.predict(X_test_pca)\n\n## get true y values for test dataset\nresults.loc[:, \"Truth\"] = y_test.values\n\n## get probability of being malignant\n## the output is one probability per outcome, we only want the second outcome (malignant). The second outcome uses index 1\nresults.loc[:, \"Probability: Malignant\"] = pd.DataFrame(lr.predict_proba(X_test_pca))[1]\n\nresults.head(5)\n\n\n\n\n\n\n\n\nPredicted\nTruth\nProbability: Malignant\n\n\n\n\n0\n0\n0\n0.098966\n\n\n1\n1\n1\n0.999987\n\n\n2\n1\n1\n0.997340\n\n\n3\n0\n0\n0.000974\n\n\n4\n0\n0\n0.000056\n\n\n\n\n\n\n\n\n\nWe can also get a quantitative “accuracy score” that will give us an idea of how well our model predicts our outcomes.\n\naccuracy = accuracy_score(results[\"Truth\"], results[\"Predicted\"])\n\nprint(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n\nAccuracy: 98.25%\n\n\n\n\nCreate ROC curve\nAs a figure, we can create an ROC curve and use quarto chunk options to add a figure caption.\n\n## make a plot to vizualize the ROC curve\n\n## get false pos rate, true pos rate and thresholds\n## there are 3 outputs so we need 3 variables to catch them\n___, ___, ___ = roc_curve(results[\"Truth\"], results[\"Predicted\"])\n\n## get AUC data\nroc_auc = auc(___, ___)\n\n## set up plot\nplt.figure(figsize=(8, 6))\n\n## using matplotlib this time, create line plot with 2pt line weight\n## add \"ROC Curve (AUC = AUC)\" as label for orange line\n## .2f is for display formatting, lw is linewidth\nplt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n\n## create another curve, this time blue with a dashed line labeled \"Random\"\n## as in random chance.\nplt.plot(___, ___, color=\"navy\", lw=2, linestyle=\"--\", label=\"Random\")\n\n## add xlabel, ylabel and title\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\n    \"Receiver Operating Characteristic (ROC) Curve\\nAccuracy: {:.2f}%\".format(\n        accuracy * 100\n    )\n)\n\n## add legend and show plot\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\nClick to reveal answers\n\n\n## make a plot to vizualize the ROC curve\n\n## get false pos rate, true pos rate and thresholds\nfpr, tpr, thresholds = roc_curve(results[\"Truth\"], results[\"Predicted\"])\n\n## get AUC data\nroc_auc = auc(fpr, tpr)\n\n## set up plot\nplt.figure(figsize=(8, 6))\n\n## using matplotlib this time, create line plot with 2pt line weight\n## add \"ROC Curve (AUC = AUC)\" as label for orange line\n## .2f is for display formatting, lw is linewidth\nplt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n\n## create another curve, this time blue with a dashed line labeled \"Random\"\n## as in random chance\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\", label=\"Random\")\n\n## add xlabel, ylabel and title\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\n    \"Receiver Operating Characteristic (ROC) Curve\\nAccuracy: {:.2f}%\".format(\n        accuracy * 100\n    )\n)\n\n## add legend and show plot\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\nAn ROC curve for our logistic regression model\n\n\n\n\n\n\nCongratulations! You have successfully done logistic regression in Python!\n\n\n\nCreate a Statsmodels-like model and summary with scikit-learn and statsmodels\n\nIt is also possible to fit a model with scikit-learn, extract the coefficients, and use them to create a statsmodels model and summary.\nTypically, you would want to pick which package (sklearn or statsmodels) you want to use and stick with it, but this is an option if necessary. Note: I am showing Lasso here as well because statsmodels will fail if there are highly correlated features like with this dataset, however this same method can be used on a scikit-learn logistic regression model without Lasso penalties.\nThis time, we are going to fit on the full data.\nFirst, we can select features to use for model (statsmodels does not perform regularization and therefore will fail to converge when there are highly correlated features). Scikit-learn gives us multiple ways to do this. Let’s use LASSO.\n\n## scale X\nX = scaler.transform(X_raw)\n\n## set up model for Lasso and fit it\nmodel = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=0.01)\nmodel.fit(X, y)\n\n# Get non-zero coefficient features\nselected_features = X_raw.columns[model.coef_[0] != 0]\nX_selected = X_raw[selected_features]\nprint(X_selected.columns)\n\nIndex(['concave_points_mean', 'perimeter_worst', 'concave_points_worst'], dtype='object')\n\n\nFit statsmodels model and get summary\n\n## Get coefficients\nintercept = model.intercept_[0]\ncoefficients = model.coef_[0][model.coef_[0] != 0]\n\n## make model eqn\nformula = \"diagnosis ~\" + \"+\".join(X_selected.columns)\nsm_model2 = smf.logit(formula, data=data).fit()\n\nsm_model2.params[:] = np.concatenate(\n    ([intercept], coefficients)\n)  # Set params from scikit-learn model\n\n# Display the summary\nprint(sm_model2.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.129410\n         Iterations 10\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:              diagnosis   No. Observations:                  569\nModel:                          Logit   Df Residuals:                      565\nMethod:                           MLE   Df Model:                            3\nDate:                Mon, 11 Nov 2024   Pseudo R-squ.:                  -42.68\nTime:                        13:43:00   Log-Likelihood:                -16411.\nconverged:                       True   LL-Null:                       -375.72\nCovariance Type:            nonrobust   LLR p-value:                     1.000\n========================================================================================\n                           coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nIntercept                     0      2.441          0      1.000      -4.785       4.785\nconcave_points_mean      0.0902     16.311      0.006      0.996     -31.878      32.058\nperimeter_worst          0.5279      0.021     25.471      0.000       0.487       0.569\nconcave_points_worst     0.4865      8.505      0.057      0.954     -16.184      17.157\n========================================================================================\n\nPossibly complete quasi-separation: A fraction 0.37 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified.\n\n\n\n\n\n\nCitations\n\nIcons\nCsv icons created by rizal2109 - Flaticon Ipynb icons created by JunGSa - Flaticon Coding icons created by juicy_fish - Flaticon"
  },
  {
    "objectID": "session2b/session2_2.html",
    "href": "session2b/session2_2.html",
    "title": "Python Machine Learning Demo",
    "section": "",
    "text": "Guide to Python Data Structures\n\n  \n\nCancer Dataset"
  },
  {
    "objectID": "session2b/session2_2.html#links",
    "href": "session2b/session2_2.html#links",
    "title": "Python Machine Learning Demo",
    "section": "",
    "text": "Guide to Python Data Structures\n\n  \n\nCancer Dataset"
  },
  {
    "objectID": "session2b/session2_2.html#topic-5-intro-to-pandas",
    "href": "session2b/session2_2.html#topic-5-intro-to-pandas",
    "title": "Python Machine Learning Demo",
    "section": "Topic 5: Intro to Pandas",
    "text": "Topic 5: Intro to Pandas\nPandas is a powerful open-source data analysis and manipulation library in Python. It provides data structures, primarily the DataFrame and Series, which are optimized for handling and analyzing large datasets efficiently.\nData Structures:\nSeries: A one-dimensional labeled array, suitable for handling single columns or rows of data.\n\nDataFrame: A two-dimensional table with labeled axes (rows and columns), much like a spreadsheet or SQL table, allowing you to work with data in rows and columns simultaneously.\nData Manipulation:\nPandas has functions for merging, reshaping, and aggregating datasets, which helps streamline data cleaning and preparation.\n\nIt can handle missing data, making it easy to filter or fill gaps in datasets.\nData Analysis:\nIt provides extensive functionality for descriptive statistics, grouping data, and handling time series.\n\nIntegrates well with other libraries, making it easy to move data between libraries like NumPy for numerical computations and Matplotlib or Seaborn for visualization.\nLoading the Dataset\n\nimport os\nimport pandas as pd\n\n# Load the dataset\ncancer_data = pd.read_csv(os.path.join('example_data', 'Cancer_Data.csv'))\n\n# Display the first few rows of the dataset\ncancer_data.head()\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\n...\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\nUnnamed: 32\n\n\n\n\n0\n842302\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\nNaN\n\n\n1\n842517\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\nNaN\n\n\n2\n84300903\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\nNaN\n\n\n3\n84348301\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\nNaN\n\n\n4\n84358402\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\nNaN\n\n\n\n\n5 rows × 33 columns\n\n\n\nViewing Basic Information a. Checking the Dataset’s Shape\n.shape returns a tuple with (number of rows, number of columns), which provides a basic overview of the dataset size.\n\n# Display the shape of the dataset\nprint(\"Dataset Shape:\", cancer_data.shape)\n\nDataset Shape: (569, 33)\n\n\n\nSummarizing Column Information\n\n.info() lists all columns, their data types, and counts of non-null values, helping identify any columns that may have missing data.\n\n# Display column names, data types, and non-null counts\ncancer_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 33 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   id                       569 non-null    int64  \n 1   diagnosis                569 non-null    object \n 2   radius_mean              569 non-null    float64\n 3   texture_mean             569 non-null    float64\n 4   perimeter_mean           569 non-null    float64\n 5   area_mean                569 non-null    float64\n 6   smoothness_mean          569 non-null    float64\n 7   compactness_mean         569 non-null    float64\n 8   concavity_mean           569 non-null    float64\n 9   concave points_mean      569 non-null    float64\n 10  symmetry_mean            569 non-null    float64\n 11  fractal_dimension_mean   569 non-null    float64\n 12  radius_se                569 non-null    float64\n 13  texture_se               569 non-null    float64\n 14  perimeter_se             569 non-null    float64\n 15  area_se                  569 non-null    float64\n 16  smoothness_se            569 non-null    float64\n 17  compactness_se           569 non-null    float64\n 18  concavity_se             569 non-null    float64\n 19  concave points_se        569 non-null    float64\n 20  symmetry_se              569 non-null    float64\n 21  fractal_dimension_se     569 non-null    float64\n 22  radius_worst             569 non-null    float64\n 23  texture_worst            569 non-null    float64\n 24  perimeter_worst          569 non-null    float64\n 25  area_worst               569 non-null    float64\n 26  smoothness_worst         569 non-null    float64\n 27  compactness_worst        569 non-null    float64\n 28  concavity_worst          569 non-null    float64\n 29  concave points_worst     569 non-null    float64\n 30  symmetry_worst           569 non-null    float64\n 31  fractal_dimension_worst  569 non-null    float64\n 32  Unnamed: 32              0 non-null      float64\ndtypes: float64(31), int64(1), object(1)\nmemory usage: 146.8+ KB\n\n\n\nViewing Column Names\n\n.columns lists column headers, while .tolist() converts it into a standard Python list for easier viewing.\n\n# Display column names\nprint(\"Column Names:\", cancer_data.columns.tolist())\n\nColumn Names: ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32']\n\n\nSummary Statistics\n.describe() generates essential statistics (mean, std, min, max, percentiles) for numeric columns, useful for identifying data distributions.\n\n# Generate summary statistics for numeric columns\ncancer_data.describe()\n\n\n\n\n\n\n\n\nid\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\n...\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\nUnnamed: 32\n\n\n\n\ncount\n5.690000e+02\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n...\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n0.0\n\n\nmean\n3.037183e+07\n14.127292\n19.289649\n91.969033\n654.889104\n0.096360\n0.104341\n0.088799\n0.048919\n0.181162\n...\n25.677223\n107.261213\n880.583128\n0.132369\n0.254265\n0.272188\n0.114606\n0.290076\n0.083946\nNaN\n\n\nstd\n1.250206e+08\n3.524049\n4.301036\n24.298981\n351.914129\n0.014064\n0.052813\n0.079720\n0.038803\n0.027414\n...\n6.146258\n33.602542\n569.356993\n0.022832\n0.157336\n0.208624\n0.065732\n0.061867\n0.018061\nNaN\n\n\nmin\n8.670000e+03\n6.981000\n9.710000\n43.790000\n143.500000\n0.052630\n0.019380\n0.000000\n0.000000\n0.106000\n...\n12.020000\n50.410000\n185.200000\n0.071170\n0.027290\n0.000000\n0.000000\n0.156500\n0.055040\nNaN\n\n\n25%\n8.692180e+05\n11.700000\n16.170000\n75.170000\n420.300000\n0.086370\n0.064920\n0.029560\n0.020310\n0.161900\n...\n21.080000\n84.110000\n515.300000\n0.116600\n0.147200\n0.114500\n0.064930\n0.250400\n0.071460\nNaN\n\n\n50%\n9.060240e+05\n13.370000\n18.840000\n86.240000\n551.100000\n0.095870\n0.092630\n0.061540\n0.033500\n0.179200\n...\n25.410000\n97.660000\n686.500000\n0.131300\n0.211900\n0.226700\n0.099930\n0.282200\n0.080040\nNaN\n\n\n75%\n8.813129e+06\n15.780000\n21.800000\n104.100000\n782.700000\n0.105300\n0.130400\n0.130700\n0.074000\n0.195700\n...\n29.720000\n125.400000\n1084.000000\n0.146000\n0.339100\n0.382900\n0.161400\n0.317900\n0.092080\nNaN\n\n\nmax\n9.113205e+08\n28.110000\n39.280000\n188.500000\n2501.000000\n0.163400\n0.345400\n0.426800\n0.201200\n0.304000\n...\n49.540000\n251.200000\n4254.000000\n0.222600\n1.058000\n1.252000\n0.291000\n0.663800\n0.207500\nNaN\n\n\n\n\n8 rows × 32 columns\n\n\n\nUsing value_counts() on a Single Column\nThis method is straightforward if you want to check the frequency distribution of one specific categorical column. Returns a pandas series object\n\n# Count occurrences of each unique value in the 'diagnosis' column\ndiagnosis_counts = cancer_data['diagnosis'].value_counts()\nprint(\"Diagnosis Counts:\\n\", diagnosis_counts)\n\nDiagnosis Counts:\n diagnosis\nB    357\nM    212\nName: count, dtype: int64\n\n\nTo see summary statistics grouped by a categorical variable in pandas, you can use the groupby() method along with describe() or specific aggregation functions like mean(), sum(), etc.\n\n# Group by 'diagnosis' and get summary statistics for each group\ngrouped_summary = cancer_data.groupby('diagnosis').mean()\nprint(grouped_summary)\n\n\n#Group by 'diagnosis' and get summary statistics for only one variable\ngrouped_radius_mean = cancer_data.groupby('diagnosis')['radius_mean'].mean()\nprint(grouped_radius_mean)\n\n                     id  radius_mean  texture_mean  perimeter_mean  \\\ndiagnosis                                                            \nB          2.654382e+07    12.146524     17.914762       78.075406   \nM          3.681805e+07    17.462830     21.604906      115.365377   \n\n            area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\ndiagnosis                                                                  \nB          462.790196         0.092478          0.080085        0.046058   \nM          978.376415         0.102898          0.145188        0.160775   \n\n           concave points_mean  symmetry_mean  ...  texture_worst  \\\ndiagnosis                                      ...                  \nB                     0.025717       0.174186  ...      23.515070   \nM                     0.087990       0.192909  ...      29.318208   \n\n           perimeter_worst   area_worst  smoothness_worst  compactness_worst  \\\ndiagnosis                                                                      \nB                87.005938   558.899440          0.124959           0.182673   \nM               141.370330  1422.286321          0.144845           0.374824   \n\n           concavity_worst  concave points_worst  symmetry_worst  \\\ndiagnosis                                                          \nB                 0.166238              0.074444        0.270246   \nM                 0.450606              0.182237        0.323468   \n\n           fractal_dimension_worst  Unnamed: 32  \ndiagnosis                                        \nB                         0.079442          NaN  \nM                         0.091530          NaN  \n\n[2 rows x 32 columns]\ndiagnosis\nB    12.146524\nM    17.462830\nName: radius_mean, dtype: float64\n\n\nRenaming Columns To make column names more readable or consistent, you can use rename() to change specific names. Here’s how to rename columns like radius_mean to Radius Mean.\n\n# Rename specific columns for readability\n\nnew_columns={\n    'radius_mean': 'Radius Mean',\n    'texture_mean': 'Texture Mean',\n    'perimeter_mean': 'Perimeter Mean'\n}\n\ncancer_data = cancer_data.rename(columns=new_columns)\n\n# Display the new column names to verify the changes\nprint(\"\\nUpdated Column Names:\", cancer_data.columns.tolist())\n\n\nUpdated Column Names: ['id', 'diagnosis', 'Radius Mean', 'Texture Mean', 'Perimeter Mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32']\n\n\nTo find missing values, you can use isnull() with sum() to calculate the total number of missing values in each column.\n\n# Count missing values in each column\nmissing_values = cancer_data.isnull().sum()\nprint(\"Missing Values per Column:\")\nprint(missing_values)\n\nMissing Values per Column:\nid                           0\ndiagnosis                    0\nRadius Mean                  0\nTexture Mean                 0\nPerimeter Mean               0\narea_mean                    0\nsmoothness_mean              0\ncompactness_mean             0\nconcavity_mean               0\nconcave points_mean          0\nsymmetry_mean                0\nfractal_dimension_mean       0\nradius_se                    0\ntexture_se                   0\nperimeter_se                 0\narea_se                      0\nsmoothness_se                0\ncompactness_se               0\nconcavity_se                 0\nconcave points_se            0\nsymmetry_se                  0\nfractal_dimension_se         0\nradius_worst                 0\ntexture_worst                0\nperimeter_worst              0\narea_worst                   0\nsmoothness_worst             0\ncompactness_worst            0\nconcavity_worst              0\nconcave points_worst         0\nsymmetry_worst               0\nfractal_dimension_worst      0\nUnnamed: 32                569\ndtype: int64\n\n\nDropping Columns with Excessive Missing Data Since Unnamed: 32 has no data, it can be dropped from the DataFrame using .drop().\n\n# Drop the 'Unnamed: 32' column if it contains no data\ncancer_data = cancer_data.drop(columns=['Unnamed: 32'])\n\n# Verify the column has been dropped\nprint(\"\\nColumns after dropping 'Unnamed: 32':\", cancer_data.columns.tolist())\n\n\nColumns after dropping 'Unnamed: 32': ['id', 'diagnosis', 'Radius Mean', 'Texture Mean', 'Perimeter Mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n\n\nColumn Selection Selecting specific columns is essential for focusing on particular aspects of the dataset. Here are some examples of both single and multiple column selections.\n\n# Select the 'diagnosis' column - diagnosis_column will be a series\ndiagnosis_column = cancer_data['diagnosis']\nprint(\"Diagnosis Column:\\n\", diagnosis_column.head())\n\nDiagnosis Column:\n 0    M\n1    M\n2    M\n3    M\n4    M\nName: diagnosis, dtype: object\n\n\nAlternatively, you can select multiple columns.\n\n# Select multiple columns: 'diagnosis', 'radius_mean', and 'area_mean' - selected_columns will be a pandas DataFrame\n\nselected_columns = cancer_data[['diagnosis', 'Radius Mean', 'area_mean']]\nprint(\"Selected Columns:\\n\", selected_columns.head())\n\nSelected Columns:\n   diagnosis  Radius Mean  area_mean\n0         M        17.99     1001.0\n1         M        20.57     1326.0\n2         M        19.69     1203.0\n3         M        11.42      386.1\n4         M        20.29     1297.0\n\n\nRow Selection Selecting rows based on labels or positions is helpful for inspecting specific data points or subsets.\n\nLabel-Based Indexing with loc loc allows selection based on labels (e.g., column names or index labels) and is particularly useful for data subsets.\n\n\n# Select rows by labels (assuming integer index here) and specific columns\nselected_rows_labels = cancer_data.loc[0:4, ['diagnosis', 'Radius Mean', 'area_mean']]\nprint(\"Selected Rows with loc:\\n\", selected_rows_labels)\n\nSelected Rows with loc:\n   diagnosis  Radius Mean  area_mean\n0         M        17.99     1001.0\n1         M        20.57     1326.0\n2         M        19.69     1203.0\n3         M        11.42      386.1\n4         M        20.29     1297.0\n\n\n\nInteger-Based Indexing with iloc iloc allows selection based purely on integer positions, making it convenient for slicing and position-based operations.\n\n\n# Select rows by integer position and specific columns\nselected_rows_position = cancer_data.iloc[0:5, [1, 2, 3]]  # Select first 5 rows and columns at position 1, 2, 3\nprint(\"Selected Rows with iloc:\\n\", selected_rows_position)\n\nSelected Rows with iloc:\n   diagnosis  Radius Mean  Texture Mean\n0         M        17.99         10.38\n1         M        20.57         17.77\n2         M        19.69         21.25\n3         M        11.42         20.38\n4         M        20.29         14.34\n\n\nFiltering enables you to create subsets of data that match specific conditions. For example, we can filter by diagnosis to analyze only malignant (M) or benign (B) cases.\n\n# Filter rows where 'diagnosis' is \"M\" (Malignant)\nmalignant_cases = cancer_data[cancer_data['diagnosis'] == 'M']\nprint(\"Malignant Cases:\\n\", malignant_cases.head(20))\n\nMalignant Cases:\n           id diagnosis  Radius Mean  Texture Mean  Perimeter Mean  area_mean  \\\n0     842302         M        17.99         10.38          122.80     1001.0   \n1     842517         M        20.57         17.77          132.90     1326.0   \n2   84300903         M        19.69         21.25          130.00     1203.0   \n3   84348301         M        11.42         20.38           77.58      386.1   \n4   84358402         M        20.29         14.34          135.10     1297.0   \n5     843786         M        12.45         15.70           82.57      477.1   \n6     844359         M        18.25         19.98          119.60     1040.0   \n7   84458202         M        13.71         20.83           90.20      577.9   \n8     844981         M        13.00         21.82           87.50      519.8   \n9   84501001         M        12.46         24.04           83.97      475.9   \n10    845636         M        16.02         23.24          102.70      797.8   \n11  84610002         M        15.78         17.89          103.60      781.0   \n12    846226         M        19.17         24.80          132.40     1123.0   \n13    846381         M        15.85         23.95          103.70      782.7   \n14  84667401         M        13.73         22.61           93.60      578.3   \n15  84799002         M        14.54         27.54           96.73      658.8   \n16    848406         M        14.68         20.13           94.74      684.5   \n17  84862001         M        16.13         20.68          108.10      798.8   \n18    849014         M        19.81         22.15          130.00     1260.0   \n22   8511133         M        15.34         14.26          102.50      704.4   \n\n    smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0           0.11840           0.27760         0.30010              0.14710   \n1           0.08474           0.07864         0.08690              0.07017   \n2           0.10960           0.15990         0.19740              0.12790   \n3           0.14250           0.28390         0.24140              0.10520   \n4           0.10030           0.13280         0.19800              0.10430   \n5           0.12780           0.17000         0.15780              0.08089   \n6           0.09463           0.10900         0.11270              0.07400   \n7           0.11890           0.16450         0.09366              0.05985   \n8           0.12730           0.19320         0.18590              0.09353   \n9           0.11860           0.23960         0.22730              0.08543   \n10          0.08206           0.06669         0.03299              0.03323   \n11          0.09710           0.12920         0.09954              0.06606   \n12          0.09740           0.24580         0.20650              0.11180   \n13          0.08401           0.10020         0.09938              0.05364   \n14          0.11310           0.22930         0.21280              0.08025   \n15          0.11390           0.15950         0.16390              0.07364   \n16          0.09867           0.07200         0.07395              0.05259   \n17          0.11700           0.20220         0.17220              0.10280   \n18          0.09831           0.10270         0.14790              0.09498   \n22          0.10730           0.21350         0.20770              0.09756   \n\n    ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n0   ...         25.38          17.33           184.60      2019.0   \n1   ...         24.99          23.41           158.80      1956.0   \n2   ...         23.57          25.53           152.50      1709.0   \n3   ...         14.91          26.50            98.87       567.7   \n4   ...         22.54          16.67           152.20      1575.0   \n5   ...         15.47          23.75           103.40       741.6   \n6   ...         22.88          27.66           153.20      1606.0   \n7   ...         17.06          28.14           110.60       897.0   \n8   ...         15.49          30.73           106.20       739.3   \n9   ...         15.09          40.68            97.65       711.4   \n10  ...         19.19          33.88           123.80      1150.0   \n11  ...         20.42          27.28           136.50      1299.0   \n12  ...         20.96          29.94           151.70      1332.0   \n13  ...         16.84          27.66           112.00       876.5   \n14  ...         15.03          32.01           108.80       697.7   \n15  ...         17.46          37.13           124.10       943.2   \n16  ...         19.07          30.88           123.40      1138.0   \n17  ...         20.96          31.48           136.80      1315.0   \n18  ...         27.32          30.88           186.80      2398.0   \n22  ...         18.07          19.08           125.10       980.9   \n\n    smoothness_worst  compactness_worst  concavity_worst  \\\n0             0.1622             0.6656           0.7119   \n1             0.1238             0.1866           0.2416   \n2             0.1444             0.4245           0.4504   \n3             0.2098             0.8663           0.6869   \n4             0.1374             0.2050           0.4000   \n5             0.1791             0.5249           0.5355   \n6             0.1442             0.2576           0.3784   \n7             0.1654             0.3682           0.2678   \n8             0.1703             0.5401           0.5390   \n9             0.1853             1.0580           1.1050   \n10            0.1181             0.1551           0.1459   \n11            0.1396             0.5609           0.3965   \n12            0.1037             0.3903           0.3639   \n13            0.1131             0.1924           0.2322   \n14            0.1651             0.7725           0.6943   \n15            0.1678             0.6577           0.7026   \n16            0.1464             0.1871           0.2914   \n17            0.1789             0.4233           0.4784   \n18            0.1512             0.3150           0.5372   \n22            0.1390             0.5954           0.6305   \n\n    concave points_worst  symmetry_worst  fractal_dimension_worst  \n0                0.26540          0.4601                  0.11890  \n1                0.18600          0.2750                  0.08902  \n2                0.24300          0.3613                  0.08758  \n3                0.25750          0.6638                  0.17300  \n4                0.16250          0.2364                  0.07678  \n5                0.17410          0.3985                  0.12440  \n6                0.19320          0.3063                  0.08368  \n7                0.15560          0.3196                  0.11510  \n8                0.20600          0.4378                  0.10720  \n9                0.22100          0.4366                  0.20750  \n10               0.09975          0.2948                  0.08452  \n11               0.18100          0.3792                  0.10480  \n12               0.17670          0.3176                  0.10230  \n13               0.11190          0.2809                  0.06287  \n14               0.22080          0.3596                  0.14310  \n15               0.17120          0.4218                  0.13410  \n16               0.16090          0.3029                  0.08216  \n17               0.20730          0.3706                  0.11420  \n18               0.23880          0.2768                  0.07615  \n22               0.23930          0.4667                  0.09946  \n\n[20 rows x 32 columns]\n\n\nYou can also filter based on multiple conditions, such as finding rows where the diagnosis is “M” and radius_mean is greater than 15.\nNote: You can’t use ‘and’ python operator here, because ‘and’ is a keyword for Python’s boolean operations, which work with single True or False values, not arrays or Series.\n\n# Filter for Malignant cases with radius_mean &gt; 15\nlarge_malignant_cases = cancer_data[(cancer_data['diagnosis'] == 'M') & (cancer_data['Radius Mean'] &gt; 15)]\nprint(\"Large Malignant Cases (Radius Mean &gt; 15):\\n\", large_malignant_cases.head())\n\nLarge Malignant Cases (Radius Mean &gt; 15):\n          id diagnosis  Radius Mean  Texture Mean  Perimeter Mean  area_mean  \\\n0    842302         M        17.99         10.38           122.8     1001.0   \n1    842517         M        20.57         17.77           132.9     1326.0   \n2  84300903         M        19.69         21.25           130.0     1203.0   \n4  84358402         M        20.29         14.34           135.1     1297.0   \n6    844359         M        18.25         19.98           119.6     1040.0   \n\n   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0          0.11840           0.27760          0.3001              0.14710   \n1          0.08474           0.07864          0.0869              0.07017   \n2          0.10960           0.15990          0.1974              0.12790   \n4          0.10030           0.13280          0.1980              0.10430   \n6          0.09463           0.10900          0.1127              0.07400   \n\n   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n0  ...         25.38          17.33            184.6      2019.0   \n1  ...         24.99          23.41            158.8      1956.0   \n2  ...         23.57          25.53            152.5      1709.0   \n4  ...         22.54          16.67            152.2      1575.0   \n6  ...         22.88          27.66            153.2      1606.0   \n\n   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n0            0.1622             0.6656           0.7119                0.2654   \n1            0.1238             0.1866           0.2416                0.1860   \n2            0.1444             0.4245           0.4504                0.2430   \n4            0.1374             0.2050           0.4000                0.1625   \n6            0.1442             0.2576           0.3784                0.1932   \n\n   symmetry_worst  fractal_dimension_worst  \n0          0.4601                  0.11890  \n1          0.2750                  0.08902  \n2          0.3613                  0.08758  \n4          0.2364                  0.07678  \n6          0.3063                  0.08368  \n\n[5 rows x 32 columns]\n\n\nAdding and Modifying Columns\nAdding New Columns You can create new columns in a DataFrame based on calculations using existing columns. For example, we can calculate the area_ratio by dividing area_worst by area_mean.\n\n# Add a new column 'area_ratio' by dividing 'area_worst' by 'area_mean'\ncancer_data['area_ratio'] = cancer_data['area_worst'] / cancer_data['area_mean']\nprint(\"New Column 'area_ratio':\\n\", cancer_data[['area_worst', 'area_mean', 'area_ratio']].head())\n\nNew Column 'area_ratio':\n    area_worst  area_mean  area_ratio\n0      2019.0     1001.0    2.016983\n1      1956.0     1326.0    1.475113\n2      1709.0     1203.0    1.420615\n3       567.7      386.1    1.470344\n4      1575.0     1297.0    1.214341\n\n\nChanging a Value Using .at Suppose you have a DataFrame and want to update the value in the radius_mean column for a particular index.\n\n# Access and print the original value at index 0 and column 'radius_mean'\noriginal_value = cancer_data.at[0, 'Radius Mean']\nprint(\"Original Radius Mean at index 0:\", original_value)\n\n\n# Change the value at index 0 and column 'radius_mean' to 18.5\ncancer_data.at[0, 'Radius Mean'] = 18.5\n\n\n# Verify the updated value\nupdated_value = cancer_data.at[0, 'Radius Mean']\nprint(\"Updated Radius Mean at index 0:\", updated_value)\n\nOriginal Radius Mean at index 0: 17.99\nUpdated Radius Mean at index 0: 18.5\n\n\nSorting by Columns You can sort a dataset by columns. Here’s how to sort by diagnosis first and then by area_mean in ascending order.\n\n# Sort by 'diagnosis' first, then by 'area_mean' within each diagnosis group\nsorted_by_diagnosis_area = cancer_data.sort_values(by=['diagnosis', 'area_mean'], ascending=[True, True])\nprint(\"Data sorted by Diagnosis and Area Mean:\\n\", sorted_by_diagnosis_area[['diagnosis', 'area_mean', 'Radius Mean']].head())\n\nData sorted by Diagnosis and Area Mean:\n     diagnosis  area_mean  Radius Mean\n101         B      143.5        6.981\n539         B      170.4        7.691\n538         B      178.8        7.729\n568         B      181.0        7.760\n46          B      201.9        8.196\n\n\nReordering Columns to Move a Column to the End You might also want to move a specific column to the end of the DataFrame, such as moving area_ratio to the last position.\n\n# Move 'area_ratio' to the end of the DataFrame\ncolumns_reordered = [col for col in cancer_data.columns if col != 'area_ratio'] + ['area_ratio']\ncancer_data_with_area_ratio_last = cancer_data[columns_reordered]\n\n# Display the reordered columns\nprint(\"Data with 'area_ratio' at the end:\\n\", cancer_data_with_area_ratio_last.head())\n\nData with 'area_ratio' at the end:\n          id diagnosis  Radius Mean  Texture Mean  Perimeter Mean  area_mean  \\\n0    842302         M        18.50         10.38          122.80     1001.0   \n1    842517         M        20.57         17.77          132.90     1326.0   \n2  84300903         M        19.69         21.25          130.00     1203.0   \n3  84348301         M        11.42         20.38           77.58      386.1   \n4  84358402         M        20.29         14.34          135.10     1297.0   \n\n   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0          0.11840           0.27760          0.3001              0.14710   \n1          0.08474           0.07864          0.0869              0.07017   \n2          0.10960           0.15990          0.1974              0.12790   \n3          0.14250           0.28390          0.2414              0.10520   \n4          0.10030           0.13280          0.1980              0.10430   \n\n   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n0  ...          17.33           184.60      2019.0            0.1622   \n1  ...          23.41           158.80      1956.0            0.1238   \n2  ...          25.53           152.50      1709.0            0.1444   \n3  ...          26.50            98.87       567.7            0.2098   \n4  ...          16.67           152.20      1575.0            0.1374   \n\n   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n0             0.6656           0.7119                0.2654          0.4601   \n1             0.1866           0.2416                0.1860          0.2750   \n2             0.4245           0.4504                0.2430          0.3613   \n3             0.8663           0.6869                0.2575          0.6638   \n4             0.2050           0.4000                0.1625          0.2364   \n\n   fractal_dimension_worst  area_ratio  \n0                  0.11890    2.016983  \n1                  0.08902    1.475113  \n2                  0.08758    1.420615  \n3                  0.17300    1.470344  \n4                  0.07678    1.214341  \n\n[5 rows x 33 columns]\n\n\nApplying Functions to Columns\nUsing apply() to Apply Custom Functions The .apply() method in pandas lets you apply a custom function to each element in a Series (column) or DataFrame. Here’s how to use it to categorize tumors based on area_mean.\nExample: Categorizing Tumors by Size Let’s create a custom function to categorize tumors as “Small”, “Medium”, or “Large” based on area_mean.\n\n# Define a custom function to categorize tumors by area_mean\ndef categorize_tumor(size):\n    if size &lt; 500:\n        return 'Small'\n    elif 500 &lt;= size &lt; 1000:\n        return 'Medium'\n    else:\n        return 'Large'\n\n# Apply the function to the 'area_mean' column and create a new column 'tumor_size_category'\ncancer_data['tumor_size_category'] = cancer_data['area_mean'].apply(categorize_tumor)\n\n# Display the new column to verify the transformation\nprint(\"Tumor Size Categories:\\n\", cancer_data[['area_mean', 'tumor_size_category']].head())\n\nTumor Size Categories:\n    area_mean tumor_size_category\n0     1001.0               Large\n1     1326.0               Large\n2     1203.0               Large\n3      386.1               Small\n4     1297.0               Large\n\n\nUsing Lambda Functions for Quick Transformations Lambda functions are useful for simple, one-line operations. For example, we can use a lambda function to convert diagnosis into numerical codes (0 for Benign, 1 for Malignant).\n\n# Apply a lambda function to classify 'diagnosis' into numerical codes\ncancer_data['diagnosis_code'] = cancer_data['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n\n# Display the new column to verify the transformation\nprint(\"Diagnosis Codes:\\n\", cancer_data[['diagnosis', 'diagnosis_code']].head())\n\nDiagnosis Codes:\n   diagnosis  diagnosis_code\n0         M               1\n1         M               1\n2         M               1\n3         M               1\n4         M               1\n\n\nApplying Multiple Conditions with apply()\nYou can also use apply() with a lambda function for more complex, multi-condition classifications.\nExample: Adding a Column with Risk Levels Suppose we want to create a new column, risk_level, based on both diagnosis and area_mean:\n“High Risk” for Malignant tumors with area_mean above 1000. “Moderate Risk” for Malignant tumors with area_mean below 1000. “Low Risk” for Benign tumors.\n\n# Apply a lambda function with multiple conditions to create a 'risk_level' column\ncancer_data['risk_level'] = cancer_data.apply(\n    lambda row: 'High Risk' if row['diagnosis'] == 'M' and row['area_mean'] &gt; 1000 \n    else ('Moderate Risk' if row['diagnosis'] == 'M' else 'Low Risk'), axis=1\n)\n\n# Display the new column to verify the transformation\nprint(\"Risk Levels:\\n\", cancer_data[['diagnosis', 'area_mean', 'risk_level']].head())\n\n#Axis=1 tells the function to apply it to the rows. axis=0 (default) applies function to the columns\n\nRisk Levels:\n   diagnosis  area_mean     risk_level\n0         M     1001.0      High Risk\n1         M     1326.0      High Risk\n2         M     1203.0      High Risk\n3         M      386.1  Moderate Risk\n4         M     1297.0      High Risk"
  },
  {
    "objectID": "sessions.html",
    "href": "sessions.html",
    "title": "Sessions",
    "section": "",
    "text": "Session links\nTutorial: Get Started with Python \nGo to Session2a: Intro to Python Data Structures \nGo to Session2b: Intro to Pandas \nGo to ML Demo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python Workshops: Intro to Python",
    "section": "",
    "text": "Welcome to the Intro to Python Workshops\nOur Intro to Python Workshops are geared towards R and SAS users who want to explore using Python. Python is a versatile language and is especially powerful for tasks such as machine learning, big data processing, and broader data science applications. We will cover installing and setting up python with anaconda, basic python datastructures, and finally a demo logistic regression model with a cancer dataset. We will upload session videos, python demos and other things here.\nWe value your feedback and aim to continually improve these workshops. If you have any suggestions, questions, or ideas, please feel free to share them on our  GitHub Discussions page. We look forward to hearing from you!\n\n\nLinks\n \n\nInstall Anaconda\n\n \n\nInstall VS Code\n\n \n\nInstall Quarto\n\n\n \n\nDownload environment YML file\n\n \n\nDownload Follow Along Files\n\n\n \n\nDownload Intro to Pandas Dataset\n\n\n\n\nSessions\nTutorial: Get Started with Python \nGo to Session2a: Intro to Python Data Structures \nGo to Session2b: Intro to Pandas \nGo to ML Demo"
  },
  {
    "objectID": "FAQ.html",
    "href": "FAQ.html",
    "title": "FAQ",
    "section": "",
    "text": "Python Workshops: Frequently Asked Questions\n\nManaging Virtual Environments with Conda\n\nIs it possible (or recommended) to create a virtual environment in a specific project folder? Are there advantages or disadvantages to keeping the virtual environment in the default location versus a network drive (e.g., H drive)?\n\nAnswer: It is definitely possible to create an environment in a specific project folder/on the H drive (using conda create --prefix /path/to/your/project_folder/env_name)! Note: to activate the environment, you need to activate it by the path, not by a name (ex:conda activate /path/to/your/project_folder/env_name)\nAdvantages:\n\nBackup and Portability: Keeping the environment with the project makes it easier to back up as a single unit and enables access on multiple devices without recreating the environment each time.\nDisadvantages:\nPerformance: Access speed may be slower on a network drive, and network or permissions issues could make the environment temporarily inaccessible. If you typically work off a network drive, storing your environments within the project folder can be beneficial for organization and consistency.\n\n\nWill creating a virtual environment for each project lead to memory or storage issues? What is the best practice for closing out a project environment?\n\nAnswer: In general, having a virtual environment for each active project shouldn’t cause memory issues. However, it’s good practice to clean up environments when a project concludes.\nHere’s a recommended process:\n\nActivating the environment you want to export\nExporting the environment to a .yaml file with conda env export &gt; environment.yaml.\nAfter the .yaml file is created (make sure to check before deleting the environment!), you can delete the environment with conda env remove --name env_name (or conda env remove --prefix path/to/env_name if using a specific path).\n\n\n\nTip: By exporting the environment before deletion, you can always recreate the environment later if needed by using:\nconda env create --prefix /path/to/your/project_folder/env_name -f environment.yaml\nconda activate /path/to/your/project_folder/env_name"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "link.html",
    "href": "link.html",
    "title": "Links",
    "section": "",
    "text": "Links for downloading and installing mentioned software/files.\n \n\nInstall Anaconda\n\n\n \n\nDownload environment YML file\n\n\n \n\nDownload Follow Along File\n\n\n \n\nDownload Dataset for Intro to Pandas"
  },
  {
    "objectID": "session1/session1.html",
    "href": "session1/session1.html",
    "title": "Session 1. Python Installation",
    "section": "",
    "text": "Welcome to the first session of the Introduction to Python Workshop series!\nThis guide will you walk through the steps to install Python to your computer and work interactively with it in Visual Studio Code. After the session, you will acquire basic knowledge of the following:"
  },
  {
    "objectID": "session1/session1.html#anaconda",
    "href": "session1/session1.html#anaconda",
    "title": "Session 1. Python Installation",
    "section": "Anaconda",
    "text": "Anaconda\n\n\n\n\n\n\nAnaconda is an open-source distribution of Python, designed for scientific computing, data science, machine learning, and AI development.\n\nThe Anaconda distribution comes with the desktop Anaconda Navigator application, the latest version of Python, and ~150 pre-downloaded libraries.\nIt also comes with conda, a cross-platform package and environment manager. Conda is the virtual environment manager that helps install packages from the Anaconda repositories (also called channels). It supports more than just Python packages but numerous other programming languages like R, Java, etc.\n\n\n\n\n\n\nDifference Between Conda and Pip\n\n\n\nLong story short: Pip is for Python libraries only, while conda can install packages for any software (including python).\nIt is generally recommended that you only use conda install when in a conda environment, as anything installed via pip won’t be recognized by conda and vice versa. Using the two interchangeably might overwrite or break packages and mess up the environment.\nWhat if the Python package is unavailable through conda?\nIn these scenarios where the package you need is not built under conda, it makes sense to use pip to install packages within the conda environment. The best practice is to install everything with conda first, then use pip if needed.\nCheck out this blog for more information on using pip in a conda environment.\n\n\n\nConda, Miniconda, and Anaconda\n\n\n\nDifference between conda, miniconda, and Anaconda\n\n\nIn this tutorial, we are going to install the full Anaconda distribution and learn to use the features of its Anaconda Navigator desktop application. This allows us to manage packages and environments without necessarily needing to know the conda terminal commands."
  },
  {
    "objectID": "session1/session1.html#visual-studio-code",
    "href": "session1/session1.html#visual-studio-code",
    "title": "Session 1. Python Installation",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\n\n\nVisual Studio Code (VS Code) is one of the most popular open-source code editors with many features.\n\n\nMulti-Language Programming. You can code in almost any major programming languages in VS Code including Python, R, C/C++, JavaScript, etc.\nBuild-In Git Source Control. VS Code automatically recognizes and uses the computer’s Git installation to allow project version control. You can easily track changes, stage, and commit changes to your working branch.\nVariety of Project Development Support. You can add extra features such as language packs, debugging tools, Git/Github features, and remote server connector by installing extensions from the Extension Marketplace.\n\nGo to 1.4 Work with Jupyter/Quarto in VS Code for guide on how to get started with VS Code."
  },
  {
    "objectID": "session1/session1.html#quarto",
    "href": "session1/session1.html#quarto",
    "title": "Session 1. Python Installation",
    "section": "Quarto",
    "text": "Quarto\n\n\nQuarto is an open-source scientific and technical publishing system.\n\nThink of Quarto .qmd as similar to R Markdown .Rmd files. Both combine executable code chunks with text components and figures and allow generating outputs as PDF, HTML, Docx, and even slideshow presentations.\nBut Quarto has more:\n\nCompatibility with multiple IDEs. You can not only work with Quarto .qmd in RStudio, but also VS Code, Jupyter Lab, etc.\nMutli-lingual support. Unlike R Markdown which is dependent on R, Quarto does not require R. It supports embedded Python, JavaScript, and Julia executable code by simply specifying the language name in the braces on top of a code chunk (e.g., ```{python}).\nMulti-engine support. Don’t worry if you are an R Markdown or Jupyter Notebook user! Qurato also works with .Rmd and .ipynb files and will automatically deploy either the knitr or jupyter engine depending on the file type that you are rendering.\n\nIn summary, Quarto is easy for R users to transition into due to similarities with the R Markdown. It also includes more functionality and flexibility, making it a great tool for learning Python."
  },
  {
    "objectID": "session1/session1.html#what-is-a-virtual-environment",
    "href": "session1/session1.html#what-is-a-virtual-environment",
    "title": "Session 1. Python Installation",
    "section": "What is a virtual environment?",
    "text": "What is a virtual environment?\nEnvironments are isolated, independent installations of a programming language and groups of packages that don’t interfere with each other.\nFor example, you may have a Python version 3.8 installed on your computer as the System Python. Meanwhile, you can install as many virtual environments as you want with the same or different Python versions and set of packages.\nYou can switch between environments for different projects, create environment files and share them with others."
  },
  {
    "objectID": "session1/session1.html#why-virtual-environments",
    "href": "session1/session1.html#why-virtual-environments",
    "title": "Session 1. Python Installation",
    "section": "Why virtual environments?",
    "text": "Why virtual environments?\nYou may find the flexibility of environments useful in many cases.\n\nAvoid Conflicts. When you need libraries that are not compatibles with your system settings, such as an older Python version or conflicting dependencies. Creating a virtual environment can resolve the conflicts and changing it won’t affect your other environments.\nSharing Environment Setting. You can also share your environment and the list of dependencies with someone with a copy of the environment.yaml file.\nEasy Management. When your work is temporary or that you simply want to experiment things without having to worry about things breaking, you can work within a virtual environemnt and later delete it when needed."
  },
  {
    "objectID": "session1/session1.html#creating-a-virtual-environment-with-gui",
    "href": "session1/session1.html#creating-a-virtual-environment-with-gui",
    "title": "Session 1. Python Installation",
    "section": "1.3.1 Creating a Virtual Environment (with GUI)",
    "text": "1.3.1 Creating a Virtual Environment (with GUI)\nOne option to create an environment in through the Anaconda Navigator graphical user interface (GUI). This approach is straightforward and does not require command line skills.\n\nOpen the Anaconda Navigator application.\n\nThe Home page shows tabs for software available to be installed or launched in the Navigator (E.g., RStudio, VS Code, JupyterLab, and Jupyter Notebook). The first dropdown menu at the top allows filtering applications shown below.\nFrom the second dropdown menu, you may switch to other conda environments that you have created. Then you may launch applications from within the environment you selected.\nCreate the first virtual environment.\nSelect the Environments tab on the left. Click Create as shown below:\n\nCustomizing environment name and Python version.\nIn the environment Name field, type a descriptive name for your environment. Then choose the Python version you want (default is the latest version). For example, create an environment with Python 3.10 and name it python310.\nClick Create. Navigator creates the new environment and activates it.\nNow you have two environments: the default environment base (root) and the one you just created.\nInstalling packages.\n\nStay in the virtual environment you just created. Open the dropdown filter and select Not Installed. Type the name of the package you want to install into the upper right search box. E.g., seaborn.\nSelect the checkbox and click Apply. The selected package will be installed.\nNow you can see the package name listed under the Installed category.\nInstalling packages available outside of the default channel.\nBy default, the Navigator shows packages under the defaults channel. There are many more channels available in Anaconda.org and Anaconda.cloud with a wider range of packages to install.\nFor example, if we want to install a package from the conda-forge channel, we will first need to add it to Navigator.\n\nClick Channels\n\nClick Add….\nType conda-forge in the text box and press Enter.\nClick Update channels.\n\nNow your package search will also include packages on the conda-forge channel.\nRemoving an environment.\nOn the Environments page, select the environment you want to remove. Click Remove.\n\nThe entire environment, including the packages installed to it, will be deleted."
  },
  {
    "objectID": "session1/session1.html#creating-a-virutal-environment-with-command-line",
    "href": "session1/session1.html#creating-a-virutal-environment-with-command-line",
    "title": "Session 1. Python Installation",
    "section": "1.3.2 Creating a Virutal Environment (with Command Line)",
    "text": "1.3.2 Creating a Virutal Environment (with Command Line)\nAlternatively, you can use the conda command line interface (CLI) to create virtual environments.\n\n\n\n\n\n\nTip\n\n\n\nFor users comfortable with command line, this approach is generally more recommended than Anaconda Navigator, as it is faster, more robust to broken environments (reported by some who used the GUI), and offers greater flexibility and functionality for environment management.\n\n\n\nCreate the virtual environment.\nconda create --name &lt;env-name&gt;\nReplace &lt;env-name&gt; with the name you want to give your environment.\nNote: you can use -n (shorthand) and --name interchangeably.\nYou can also specify the Python version and packages you want to install to your environment.\nconda create -n &lt;env-name&gt; python=3.10 scipy=0.17.3 pandas matplotlib\nOr:\nconda create -n &lt;env-name&gt; python=3.10\nconda install -n &lt;env-name&gt; scipy pandas matplotlib\nYou can also specify channel other than the defaults channel (for multiple channels, pass the argument multiple times):\nconda install -n &lt;env-name&gt; scipy --channel conda-forge --channel bioconda\nNow, activate your environment.\nconda activate &lt;env-name&gt;\nYou can also verify that your installation was successful by looking up the list of all current environments on your computer.\nconda env list\nThe default location for the installed conda environments (except for the base conda environment) is ..\\anaconda3\\envs\\&lt;env-name&gt;\nDeactivate the conda environment.\nSimply use conda deactivate.\n\n\n\n\n\n\nNote: Avoid activating on top of another virtual environment!\n\n\n\nBe careful when activating environments. Remember to always deactivate the current environment before going into another one because environments can be stacked. This can lead to chaos in the packages in both environments. So make sure that you see (base) at the beginning of the terminal prompt line when you are about to activate an environment.\n\n\nRemoving an environment.\n\nRemove by environment name:\nconda env remove -n &lt;env-name&gt;\nRemove by environment folder path:\nconda env remove --prefix &lt;/path/to/your/env&gt;\n\n\n\n\nCreating an environment from an environment.yml file\n\nWe can also create a virtual environment from a .yml file.\nconda env create -f environment.yml\nSimilarly, after installation, activate the new environment:\nconda activate &lt;env-name&gt;\nThis way, we can easily recreate an environment that is shared by others or share our environment settings with others.\nFor example, a simple environment file that has information about the environment name and dependencies:\nname: python310\nchannels:\n  - defaults\ndependencies:\n  - python==3.10\n  - pandas\n  - numpy\nDownload the YML file for this Python workshop series here."
  },
  {
    "objectID": "session1/session1.html#prerequisites",
    "href": "session1/session1.html#prerequisites",
    "title": "Session 1. Python Installation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nInstall Everything\n\nInstall VS Code: Download and install VS Code\nInstall Quarto: Install from the official website\nInstall Conda: Go to previous section 1.2 Install Anaconda."
  },
  {
    "objectID": "session1/session1.html#setting-up-vs-code",
    "href": "session1/session1.html#setting-up-vs-code",
    "title": "Session 1. Python Installation",
    "section": "Setting up VS Code",
    "text": "Setting up VS Code\n\nOpen VS Code.\nInstall Extensions from Extension Marketplace. Click Extensions from the left toolbar or click Ctrl+Shift+X (or Cmd+Shift+X on Mac).\n\nPython: To support Python language, debugging, documentations, etc.\nQuarto: To support Quarto document editing and previewing.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou still need to install conda (Python) and Quarto to your computer to enable these extensions.\n\n\nCreate a Conda Virtual Environment. You may also use existing ones you created.\nConfigure the Environment in VS Code.\n\nOpen Comman Palette by pressing Ctrl+Shift+P (or Cmd+Shift+X on Mac).\nType “Python: Select Interpreter”.\nChoose the Conda python environment you created. If you don’t see its name pop up, choose Enter interpreter path… and manually type in the location of your conda virtual environment python executable.\nE.g., C:\\Users\\&lt;username&gt;\\AppData\\Local\\anaconda3\\envs\\&lt;env-name&gt;\\python.exe for Windows or Users/&lt;username&gt;/anaconda/envs/&lt;env-name&gt;/python for macOS.\n\n\n\n\n\n\nFind your conda Python executable path\n\n\n\nYou can search for the conda Python interpreter location on your computer. Here is an example of how to do it using command line.\nFor both Windows and macOS, open the Anaconda Prompt or terminal and activate the conda Python environment with conda activate &lt;env-name&gt;.\nThen, locate your Python executable by typing the following:\n\nWindows: where python\nmacOS: which python\n\n\n\n\n\nNow you are ready to go. Let’s start with creating a new Quarto file."
  },
  {
    "objectID": "session1/session1.html#create-a-quarto-file",
    "href": "session1/session1.html#create-a-quarto-file",
    "title": "Session 1. Python Installation",
    "section": "Create a Quarto File",
    "text": "Create a Quarto File\nGo to File &gt; New File and select Quarto Document. This will generate an empty .qmd file with the following YAML metadata. You can change the title and file output format as needed.\n---\ntitle: \"First Day\"\nformat: html\n---\nIn Quarto files, you can mix Markdown with code blocks just like in R Markdown. For example:\n# Heading 1\nThis is a demo Quarto file. Here is the text section.\n\n## Heading 2\nThis is a subsection.\nTo add an executable code block, specify the language name inside two curly braces (e.g., {python}, {r}, etc.). For example, let’s print something:\n```{python}\nprint(\"This is some Python output.\")\n```\n\n\nThis is some Python output.\n\n\nTo render or preview the html output, click on the Quarto Preview button at the top right (or press Ctrl+Shift+K).\nAlternatively, run quarto render in your terminal to generate the output.\nquarto render &lt;yourfilename&gt;.qmd\nThis is an example of the html output preview showing next to the source .qmd file.\n\nWe are done with setting everything up."
  },
  {
    "objectID": "session2a/session2.html",
    "href": "session2a/session2.html",
    "title": "Python Machine Learning Demo",
    "section": "",
    "text": "Guide to Python Data Structures\n\n  \n\nDownload Follow Along File"
  },
  {
    "objectID": "session2a/session2.html#links",
    "href": "session2a/session2.html#links",
    "title": "Python Machine Learning Demo",
    "section": "",
    "text": "Guide to Python Data Structures\n\n  \n\nDownload Follow Along File"
  },
  {
    "objectID": "session2a/session2.html#getting-started",
    "href": "session2a/session2.html#getting-started",
    "title": "Python Machine Learning Demo",
    "section": "Getting Started",
    "text": "Getting Started\nBefore doing anything else, we should first activate the conda environment we want to use.\n\n\nRefresher: How to activate conda environment\n\n\n\n\nFrom terminal, type:\n\n&gt; conda activate ENVNAME\n\n\n\n\nWhen in VS code, you might get a popup message like the one below, confirming that the environment was activated:\n\nSelected conda environment was successfully activated, even though “(ENVNAME)” indicator may not be present in the terminal prompt.\n\n\nIf we want to make sure we have the packages we’ll need installed in the environment before we try to import them, we can either check on anaconda or use the terminal:\n\n&gt; conda list\n\n\n\n\nOtherwise, we will get an error message if we try to import packages that are not installed.\n\n\nRefresher: How to install packages\n\nTo install packages, we can either use the “anaconda” dashboard, or we can use the command line. Make sure your environment is active before installing packages or the packages will not be available in your environment.\nTo install from the command line, we open a terminal and type:\n\n&gt; conda install {package}\n\nor\n\n&gt; pip install {package}\n\nIf a package is not available via conda it might be available via pip."
  },
  {
    "objectID": "session2a/session2.html#topic-1-lists",
    "href": "session2a/session2.html#topic-1-lists",
    "title": "Python Machine Learning Demo",
    "section": "Topic 1: Lists",
    "text": "Topic 1: Lists\nThe most important thing about lists:\nIn Python, lists are mutable, meaning their elements can be changed after the list is created, allowing for modification such as adding, removing, or updating items. This flexibility makes lists powerful for handling dynamic collections of data.\nWe will learn how to do things such as: Create lists Modify lists Sort lists, Loop over elements of a list with a for-loop or using list comprehension, Slice a list, Append to a list\nTo create a list:\n\nmy_list=[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\nprint(my_list)\n\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\n\n\nA list is simply a collection of objects. We can find the length of a list using the len() function\n\nmy_list=[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\nlen(my_list)\n\n7\n\n\nIn Python, lists are objects like all other data types, and the class for lists is named ‘list’ with a lowercase ‘L’. To transform another Python object into a list, you can use the list() function, which is essentially the constructor of the list class. This function accepts a single argument: an iterable. So, you can use it to turn any iterable, such as a range, set, or tuple, into a list of concrete values.\nAside: Python indices begin at 0. In addition, certain built-in python functions such as range will terminate at n-1 in the second argument.\n\nfirst_range=range(5)\nfirst_range_list=list(first_range)\nprint(first_range_list)\n\nsecond_range=range(5,10)\nsecond_range_list=list(second_range)\nprint(second_range_list)\n\n[0, 1, 2, 3, 4]\n[5, 6, 7, 8, 9]\n\n\nAccessing Python list elements\nTo access an individual list element, you need to know its position. Since python starts counting at 0, the first element is in position 0, and the second element is in position 1. You can also access nested elements within a list, or access the list in reverse.\nExamples using my_list from above\n\nprint(my_list)\n\nexample_1=my_list[0]\n\nexample_2=my_list[6][1]\n\nexample_3=my_list[-1]\n\nprint(example_1, example_2, example_3)\n\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\n1 z [4, 'z']\n\n\nMutability of lists\nSince lists are mutable objects, we can directly change their elements.\n\nsome_list=[1,2,3]\nprint(some_list)\n\nsome_list[0]=\"hello\"\nprint(some_list)\n\n[1, 2, 3]\n['hello', 2, 3]\n\n\nAppending an element to a list\nWhen calling append on a list, we append an object to the end of the list:\n\nprint(my_list)\n\nmy_list.append(5)\n\nprint(my_list)\n\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z'], 5]\n\n\nCombining lists\nWe can combine lists with the “+” operator. This keeps the original lists intact\n\nlist_1=[1,2,3]\n\nlist_2=['a','b','c']\n\ncombined_lists=list_1+list_2\n\nprint(combined_lists)\n\n[1, 2, 3, 'a', 'b', 'c']\n\n\nAnother method is to extend one list onto another.\n\nlist_1=[1,2,3]\n\nlist_2=['a','b','c']\n\nlist_1.extend(list_2)\n\nprint(list_1)\n\n[1, 2, 3, 'a', 'b', 'c']\n\n\nThe pop() method removes and returns the last item by default unless you give it an index argument. If you’re familiar with stacks, this method as well as .append() can be used to create one!\n\nlist_1=[1,2,3]\n\nelement_1=list_1.pop()\nelement_2=list_1.pop(1)\n\nprint(element_1, element_2)\n\n3 2\n\n\nDeleting items by index\ndel removes an item without returning anything. In fact, you can delete any object, including the entire list, using del:\n\nlist_1=[1,2,3]\n\ndel list_1[0]\n\nprint(list_1)\n\n[2, 3]\n\n\nDeleting items by value\nThe .remove() method deletes a specific value from the list. This method will remove the first occurrence of the given object in a list.\n\nlist_1=[1,2,3]\n\nlist_1.remove(1)\n\nprint(list_1)\n\n[2, 3]\n\n\nLists vs. sets, and deleting duplicates from a list:\nThe difference between a list and a set is that a set is an unordered collection of distinct elements, while a list is ordered and can contain repeats of an element. Sets are denoted by curly brackets {}. We can use this knowledge to easily delete duplicates from a list, since there is no built-in method to do so.\n\nlist_1=[1,2,3,1,2]\nprint(list_1)\n\nset_1=set(list_1)\nprint(set_1)\n\nlist_2=list(set_1)\nprint (list_2)\n\n[1, 2, 3, 1, 2]\n{1, 2, 3}\n[1, 2, 3]\n\n\nSorting a list\nThere are two ways to sort a list in Python:\n.sort() modifies the original list itself. Nothing is returned. sorted() returns a new list, which is a sorted version of the original list.\nreverse=True: Use this parameter to sort the list in reverse order.\n\nnumber_list_1=[3,5,2,1,6,19]\nnumber_list_1.sort()\nprint(number_list_1)\n\nnumber_list_2=sorted(number_list_1, reverse=True)\n\n\nalphabet_list_1=['a','z','e','b']\nalphabet_list_1.sort()\nprint(alphabet_list_1)\n\n\nalphabet_list_2=sorted(alphabet_list_1, reverse=True)\nprint(alphabet_list_2)\n\n[1, 2, 3, 5, 6, 19]\n['a', 'b', 'e', 'z']\n['z', 'e', 'b', 'a']\n\n\n\nmixed_list_1=[1,5,3,'a','c','b']\ntry:\n    mixed_list_1.sort()\n    print(mixed_list_1)\nexcept TypeError:\n    print(\"Can't sort a list of mixed elements\")\n\nCan't sort a list of mixed elements\n\n\nList comprehension\nList comprehension offers a shorter syntax when you want to create a new list based on the values of an existing list (or other object)\n\n#Longer syntax with for loop\n\n\n#Example 1:\n\nsome_list=[1,2,3,'a', 'b', 'c']\nnew_list=[]\nfor item in some_list:\n    if type(item)==str:\n        new_list.append(item)\nprint(new_list)\n\n\n#Example 2:\n\nlowercase_list=['joe', 'sarah', 'emily']\ncapital_list=[]\nfor item in lowercase_list:\n    capital_item=item.upper()\n    capital_list.append(capital_item)\nprint(capital_list)\n\n\n#Example 3:\n\nsome_string=\"patrick\"\npatrick_list=[]\nfor letter in some_string:\n    if letter=='t' or letter=='a':\n        patrick_list.append(letter)\nprint(patrick_list)\n\n['a', 'b', 'c']\n['JOE', 'SARAH', 'EMILY']\n['a', 't']\n\n\n\n#Shorter syntax with list comprehension\n\n\n#Example 1:\nsome_list=[1,2,3,'a', 'b', 'c']\nnew_list=[x for x in some_list if type(x)==str]\nprint(new_list)\n\n\n#Example 2:\n\n\nlowercase_list=['joe', 'sarah', 'emily']\ncapital_list=[name.upper() for name in lowercase_list]\nprint(capital_list)\n\n\n#Example 3:\nsome_string=\"patrick\"\npatrick_list=[x for x in some_string if x=='t' or x=='a']\nprint(patrick_list)\n\n['a', 'b', 'c']\n['JOE', 'SARAH', 'EMILY']\n['a', 't']"
  },
  {
    "objectID": "session2a/session2.html#topic-2-tuples",
    "href": "session2a/session2.html#topic-2-tuples",
    "title": "Python Machine Learning Demo",
    "section": "Topic 2: Tuples",
    "text": "Topic 2: Tuples\nA tuple is similar to a list, but with one key difference: tuples are immutable. This means that once you create a tuple, you cannot modify its elements. Tuples are useful for storing data that should not be changed after creation, such as coordinates, days of the week, or fixed pairs.\nJust like lists, tuples are objects, and the class for tuples is tuple. To transform another Python object into a tuple, you can use the tuple() constructor. It accepts a single iterable, such as a list, range, or string.\nTo create a tuple, you use parentheses () rather than square brackets [].\n\n# Creating a tuple\nmy_tuple = (10, 20, 30)\n\n# Accessing elements by index\nprint(\"First element:\", my_tuple[0])\nprint(\"Last element:\", my_tuple[-1])\n\nFirst element: 10\nLast element: 30\n\n\nTuples are immutable, so you can’t modify their elements. Attempting to change a tuple will result in an error.\n\n# Trying to modify a tuple element (this will raise an error)\ntry:\n    my_tuple[1] = 99\nexcept TypeError:\n    print(\"Tuples are immutable and cannot be changed!\")\n\nTuples are immutable and cannot be changed!\n\n\nFunctions can return multiple values as a tuple. This is useful for returning multiple results in a single function call.\n\n# Function that returns multiple values as a tuple\ndef min_max(nums):\n    return min(nums), max(nums)  # Returns a tuple of (min, max)\n\n# Calling the function and unpacking the tuple\n\nnumbers = [3, 7, 1, 5]\n\nour_tuple = min_max(numbers)\n\nmin_val, max_val = min_max(numbers) #Unpacking in the function call\n\nprint(our_tuple)\nprint(\"Min:\", min_val)\nprint(\"Max:\", max_val)\n\n(1, 7)\nMin: 1\nMax: 7"
  },
  {
    "objectID": "session2a/session2.html#topic-3-strings",
    "href": "session2a/session2.html#topic-3-strings",
    "title": "Python Machine Learning Demo",
    "section": "Topic 3: Strings",
    "text": "Topic 3: Strings\nCreating a string - You can use single or double quotes to define a string (but keep it consistent!)\n\nmy_string = \"Hello, World!\"\nprint(my_string)\n\nHello, World!\n\n\nYou can also create a multiline string using triple quotes:\n\nmulti_line_string = \"\"\"This is\na multiline\nstring.\"\"\"\nprint(multi_line_string)\n\nThis is\na multiline\nstring.\n\n\nYou can find the length of a string using the len() funtion, just like with lists.\n\nmy_string = \"Hello, World!\"\nprint(len(my_string))\n\n13\n\n\nAccessing characters in a string\nStrings are indexed like lists, with the first character having index 0. You can access individual characters using their index.\n\nmy_string = \"Hello, World!\"\n\n# First character\nfirst_char = my_string[0]\n\n# Last character (using negative indexing)\nlast_char = my_string[-1]\n\n# Accessing a range of characters (slicing)\nsubstring = my_string[0:5]\n\nprint(first_char, last_char, substring)\n\nH ! Hello\n\n\nStrings are immutable!\nUnlike lists, strings cannot be changed after creation. If you try to change an individual character, you’ll get an error.\n\nmy_string = \"Hello\"\ntry:\n    my_string[0] = \"h\"  # This will raise an error\nexcept TypeError:\n    print(\"Strings are immutable!\")\n\nStrings are immutable!\n\n\nConcatenating Strings\nYou can concatenate (combine) strings using the + operator:\n\ngreeting = \"Hello\"\nname = \"Patrick\"\ncombined_string = greeting + \", \" + name + \"!\"\nprint(combined_string)\n\nHello, Patrick!\n\n\nString methods\nPython provides many built-in methods for manipulating strings. Some common ones are:\nupper() and lower() These methods convert a string to uppercase or lowercase.\n\nmy_string = \"Hello, World!\"\nprint(my_string.upper())\nprint(my_string.lower())\n\nHELLO, WORLD!\nhello, world!\n\n\nstrip() This method removes any leading or trailing whitespace from the string.\n\nmy_string = \"   Hello, World!   \"\nprint(my_string.strip())\n\nHello, World!\n\n\nreplace() You can replace parts of a string with another string.\n\nmy_string = \"Hello, World!\"\nnew_string = my_string.replace(\"World\", \"Patrick\")\nprint(new_string)\n\nHello, Patrick!\n\n\nThe split() method divides a string into a list of substrings based on a delimiter (default is whitespace).\n\nmy_string = \"Hello, World!\"\nwords = my_string.split()\nprint(words)\n\n\nanother_string=\"Hello-World!\"\nmore_words=another_string.split(\"-\")\nprint(more_words)\n\n['Hello,', 'World!']\n['Hello', 'World!']\n\n\nThe join() method takes an iterable (like a list) and concatenates its elements into a string with a specified separator between them.\n\nmy_list=['Hello,', 'my', 'name', 'is', 'Patrick']\n\nmy_string=' '.join(my_list)\n\nprint(my_string)\n\nHello, my name is Patrick\n\n\nf-strings (Python 3.6+) You can insert variables directly into strings using f-strings.\n\nname = \"Patrick\"\nage = 30\nformatted_string = f\"My name is {name} and I am {age} years old.\"\nprint(formatted_string)\n\nMy name is Patrick and I am 30 years old.\n\n\n\nmy_string = \"Hello, World!\"\n\n# Extract all vowels from the string\nvowels = str([char for char in my_string if char.lower() in \"aeiou\"])\nprint(vowels)\n\n['e', 'o', 'o']\n\n\nIn this last example, we will slice a string using different combinations of start, end, and step to extract different parts of the string.\n\n#To slice a string, follow the string[start:end:step] format\n\n# Original string\nmy_string = \"Python is awesome!\"\n\n# Slice from index 0 to 6 (not inclusive), stepping by 1 (default)\n# This will extract \"Python\"\nsubstring_1 = my_string[0:6]\n\n# Slice from index 7 to the end of the string, stepping by 1 (default)\n# This will extract \"is awesome!\"\nsubstring_2 = my_string[7:]\n\n# Slice the entire string but take every second character\n# This will extract \"Pto saeoe\"\nsubstring_3 = my_string[::2]\n\n# Slice from index 0 to 6, stepping by 2\n# This will extract \"Pto\"\nsubstring_4 = my_string[0:6:2]\n\n# Slice from index 11 to 6, stepping backward by -1\n# This will extract \"wa si\" (reverse slice)\nsubstring_5 = my_string[11:6:-1]\n\n# Print the results\nprint(f\"Original string: {my_string}\")\nprint(f\"Substring 1 (0:6): {substring_1}\")\nprint(f\"Substring 2 (7:): {substring_2}\")\nprint(f\"Substring 3 (every second character): {substring_3}\")\nprint(f\"Substring 4 (0:6:2): {substring_4}\")\nprint(f\"Substring 5 (11:6:-1): {substring_5}\")\n\nOriginal string: Python is awesome!\nSubstring 1 (0:6): Python\nSubstring 2 (7:): is awesome!\nSubstring 3 (every second character): Pto saeoe\nSubstring 4 (0:6:2): Pto\nSubstring 5 (11:6:-1): wa si"
  },
  {
    "objectID": "session2a/session2.html#topic-4-dictionaries",
    "href": "session2a/session2.html#topic-4-dictionaries",
    "title": "Python Machine Learning Demo",
    "section": "Topic 4: Dictionaries",
    "text": "Topic 4: Dictionaries\nA dictionary is a collection in Python that stores data as key-value pairs. It’s similar to a real-world dictionary where you look up a word (the key) to get its definition (the value). In Python, dictionaries are mutable, meaning you can add, remove, and change items.\nTo create a dictionary, use curly braces {}, with each key-value pair separated by a colon (:), and pairs separated by commas.\n\n# Creating a dictionary\nmy_dictionary = {\n    'name': 'Alice',\n    'age': 25,\n    'city': 'New York'\n}\nprint(my_dictionary)\n\n{'name': 'Alice', 'age': 25, 'city': 'New York'}\n\n\nAccessing dictionary values To access a specific value in a dictionary, use the key in square brackets. You can also use the .get() method, which returns None if the key does not exist, instead of raising an error.\n\nprint(my_dictionary['name'])      # Using key\nprint(my_dictionary.get('age'))   # Using .get() method\nprint(my_dictionary.get('gender', 'Not specified'))  # Providing a default value\n\nAlice\n25\nNot specified\n\n\nAdding and updating dictionary items\nDictionaries are mutable, so you can add new items or update existing ones using assignment.\n\nmy_dictionary['job'] = 'Engineer'        # Adding a new key-value pair\nmy_dictionary['age'] = 26                # Updating an existing value\nprint(my_dictionary)\n\n{'name': 'Alice', 'age': 26, 'city': 'New York', 'job': 'Engineer'}\n\n\nDictionary Methods\nPython dictionaries have several useful methods for managing data:\nkeys(): Returns a list of all the keys in the dictionary. values(): Returns a list of all values in the dictionary. items(): Returns a list of key-value pairs as tuples.\n\n# Getting all keys\nprint(my_dictionary.keys())\n\n# Getting all values\nprint(my_dictionary.values())\n\n# Getting all key-value pairs\nprint(my_dictionary.items())\n\ndict_keys(['name', 'age', 'city', 'job'])\ndict_values(['Alice', 26, 'New York', 'Engineer'])\ndict_items([('name', 'Alice'), ('age', 26), ('city', 'New York'), ('job', 'Engineer')])\n\n\nPython Dictionaries for Cancer Research Data\nIn cancer research, dictionaries can be used to store patient data, genetic mutations, and statistical results as key-value pairs. This allows for easy lookup, organization, and analysis of data.\n\n#Let’s create a dictionary to store basic patient information, where each patient has a unique ID, and each ID maps to a dictionary containing information about the patient’s age, cancer type, and stage.\n\n# Dictionary of patients with nested dictionaries\npatient_data = {\n    'P001': {'age': 50, 'cancer_type': 'Lung Cancer', 'stage': 'II'},\n    'P002': {'age': 60, 'cancer_type': 'Breast Cancer', 'stage': 'I'},\n    'P003': {'age': 45, 'cancer_type': 'Melanoma', 'stage': 'III'}\n}\n\nprint(patient_data)\n\n{'P001': {'age': 50, 'cancer_type': 'Lung Cancer', 'stage': 'II'}, 'P002': {'age': 60, 'cancer_type': 'Breast Cancer', 'stage': 'I'}, 'P003': {'age': 45, 'cancer_type': 'Melanoma', 'stage': 'III'}}\n\n\nYou can access a patient’s information using their unique ID. To access nested data, chain the keys. For example, to retrieve the cancer type of a specific patient, you’d use the following:\n\n# Accessing specific information\n\npatient_id = 'P002'\ncancer_type = patient_data[patient_id]['cancer_type']\nprint(f\"Cancer type for {patient_id}: {cancer_type}\")\n\n# Updating a patient’s stage\npatient_data['P003']['stage'] = 'IV'\nprint(f\"Updated stage for P003: {patient_data['P003']['stage']}\")\n\nCancer type for P002: Breast Cancer\nUpdated stage for P003: IV\n\n\nAdding and Removing Data New patient data can be added using assignment, and pop() or del can remove a patient’s data.\n\n# Adding a new patient\npatient_data['P004'] = {'age': 70, 'cancer_type': 'Prostate Cancer', 'stage': 'II'}\nprint(\"Added new patient:\", patient_data['P004'])\n\n# Removing a patient\nremoved_patient = patient_data.pop('P001')\nprint(\"Removed patient:\", removed_patient)\n\nAdded new patient: {'age': 70, 'cancer_type': 'Prostate Cancer', 'stage': 'II'}\nRemoved patient: {'age': 50, 'cancer_type': 'Lung Cancer', 'stage': 'II'}\n\n\nDictionary Methods: Data Summary Dictionaries allow you to retrieve keys, values, or entire key-value pairs. Here’s how to use these methods to get an overview of the data.\nkeys(): Retrieves all patient IDs. values(): Retrieves all patient records. items(): Retrieves patient records as key-value pairs\n\n# Getting all patient IDs\nprint(\"Patient IDs:\", patient_data.keys())\n\n# Getting all patient details\nprint(\"Patient Details:\", patient_data.values())\n\n# Looping through each patient's data\nfor patient_id, details in patient_data.items():\n    print(f\"Patient {patient_id} - Age: {details['age']}, Cancer Type: {details['cancer_type']}, Stage: {details['stage']}\")\n\nPatient IDs: dict_keys(['P002', 'P003', 'P004'])\nPatient Details: dict_values([{'age': 60, 'cancer_type': 'Breast Cancer', 'stage': 'I'}, {'age': 45, 'cancer_type': 'Melanoma', 'stage': 'IV'}, {'age': 70, 'cancer_type': 'Prostate Cancer', 'stage': 'II'}])\nPatient P002 - Age: 60, Cancer Type: Breast Cancer, Stage: I\nPatient P003 - Age: 45, Cancer Type: Melanoma, Stage: IV\nPatient P004 - Age: 70, Cancer Type: Prostate Cancer, Stage: II"
  }
]