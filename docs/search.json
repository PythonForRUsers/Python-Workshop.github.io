[
  {
    "objectID": "session2/DataStructuresDemo.html#links",
    "href": "session2/DataStructuresDemo.html#links",
    "title": "Session 2: Python Data Structures",
    "section": "Links",
    "text": "Links\n \n\nGuide to Python Data Structures\n\n  \n\nCancer Dataset"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#topic-1-lists",
    "href": "session2/DataStructuresDemo.html#topic-1-lists",
    "title": "Session 2: Python Data Structures",
    "section": "Topic 1: Lists",
    "text": "Topic 1: Lists\nThe most important thing about lists\nIn Python, lists are mutable, meaning their elements can be changed after the list is created, allowing for modification such as adding, removing, or updating items. This flexibility makes lists powerful for handling dynamic collections of data.\n\nWe will learn how to do things such as:\n\nCreate lists\nModify lists\nSort lists\nLoop over elements of a list with a for-loop or using list comprehension\nSlice a list\nAppend to a list"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#lists",
    "href": "session2/DataStructuresDemo.html#lists",
    "title": "Session 2: Python Data Structures",
    "section": "Lists",
    "text": "Lists\n\nTo create a list:\n\n\nmy_list = [1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\nprint(my_list)\n\n\n\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\n\n\n\nA list is simply a collection of objects. We can find the length of a list using the len() function:\n\n\nmy_list=[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\nlen(my_list)\n\n\n\n7"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#lists-continued",
    "href": "session2/DataStructuresDemo.html#lists-continued",
    "title": "Session 2: Python Data Structures",
    "section": "Lists (continued)",
    "text": "Lists (continued)\n\nIn Python, lists are objects like all other data types, and the class for lists is named ‘list’ with a lowercase ‘L’.\nTo transform another Python object into a list, you can use the list() function, which is essentially the constructor of the list class. This function accepts a single argument: an iterable. So, you can use it to turn any iterable, such as a range, set, or tuple, into a list of concrete values.\nPython indices begin at 0. In addition, certain built-in python functions such as range will terminate at n-1 in the second argument.\n\n\nfirst_range=range(5)\nfirst_range_list=list(first_range)\nprint(first_range_list)\n\n\n\n[0, 1, 2, 3, 4]\n\n\n\nsecond_range=range(5,10)\nsecond_range_list=list(second_range)\nprint(second_range_list)\n\n\n\n[5, 6, 7, 8, 9]"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#accessing-python-list-elements",
    "href": "session2/DataStructuresDemo.html#accessing-python-list-elements",
    "title": "Session 2: Python Data Structures",
    "section": "Accessing Python list elements",
    "text": "Accessing Python list elements\nTo access an individual list element, you need to know its position. Since python starts counting at 0, the first element is in position 0, and the second element is in position 1. You can also access nested elements within a list, or access the list in reverse.\n\nExamples using my_list from above\n\nprint(my_list)\n\n\n\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\n\n\n\nexample_1=my_list[0]\n\nexample_2=my_list[6][1]\n\nexample_3=my_list[-1]\n\nprint(example_1, example_2, example_3)\n\n\n\n1 z [4, 'z']"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#mutability-of-lists",
    "href": "session2/DataStructuresDemo.html#mutability-of-lists",
    "title": "Session 2: Python Data Structures",
    "section": "Mutability of lists",
    "text": "Mutability of lists\nSince lists are mutable objects, we can directly change their elements.\n\nsome_list=[1,2,3]\nprint(some_list)\n\nsome_list[0]=\"hello\"\nprint(some_list)\n\n\n\n[1, 2, 3]\n['hello', 2, 3]"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#appending-an-element-to-a-list",
    "href": "session2/DataStructuresDemo.html#appending-an-element-to-a-list",
    "title": "Session 2: Python Data Structures",
    "section": "Appending an element to a list",
    "text": "Appending an element to a list\nWhen calling append on a list, we append an object to the end of the list:\n\nprint(my_list)\n\nmy_list.append(5)\n\nprint(my_list)\n\n\n\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z'], 5]"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#combining-lists",
    "href": "session2/DataStructuresDemo.html#combining-lists",
    "title": "Session 2: Python Data Structures",
    "section": "Combining lists",
    "text": "Combining lists\nWe can combine lists with the “+” operator. This keeps the original lists intact\n\nlist_1=[1,2,3]\n\nlist_2=['a','b','c']\n\ncombined_lists=list_1+list_2\n\nprint(combined_lists)\n\n\n\n[1, 2, 3, 'a', 'b', 'c']\n\n\n\nAnother method is to extend one list onto another.\n\nlist_1=[1,2,3]\n\nlist_2=['a','b','c']\n\nlist_1.extend(list_2)\n\nprint(list_1)\n\n\n\n[1, 2, 3, 'a', 'b', 'c']"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#pop-method",
    "href": "session2/DataStructuresDemo.html#pop-method",
    "title": "Session 2: Python Data Structures",
    "section": ".pop() method",
    "text": ".pop() method\nThe .pop() method removes and returns the last item by default unless you give it an index argument. If you’re familiar with stacks, this method as well as .append() can be used to create one!\n\nlist_1=[1,2,3]\n\nelement_1=list_1.pop()\nelement_2=list_1.pop(1)\n\nprint(element_1, element_2)\n\n\n\n3 2"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#deleting-items-by-index",
    "href": "session2/DataStructuresDemo.html#deleting-items-by-index",
    "title": "Session 2: Python Data Structures",
    "section": "Deleting items by index",
    "text": "Deleting items by index\ndel removes an item without returning anything. In fact, you can delete any object, including the entire list, using del:\n\nlist_1=[1,2,3]\n\ndel list_1[0]\n\nprint(list_1)\n\n\n\n[2, 3]"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#deleting-items-by-value",
    "href": "session2/DataStructuresDemo.html#deleting-items-by-value",
    "title": "Session 2: Python Data Structures",
    "section": "Deleting items by value",
    "text": "Deleting items by value\nThe .remove() method deletes a specific value from the list. This method will remove the first occurrence of the given object in a list.\n\nlist_1=[1,2,3]\n\nlist_1.remove(1)\n\nprint(list_1)\n\n\n\n[2, 3]"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#lists-vs.-sets-and-deleting-duplicates-from-a-list",
    "href": "session2/DataStructuresDemo.html#lists-vs.-sets-and-deleting-duplicates-from-a-list",
    "title": "Session 2: Python Data Structures",
    "section": "Lists vs. sets, and deleting duplicates from a list",
    "text": "Lists vs. sets, and deleting duplicates from a list\nThe difference between a list and a set:\n\nA set is an unordered collection of distinct elements.\nA list is ordered and can contain repeats of an element.\nSets are denoted by curly brackets {}. We can use this knowledge to easily delete duplicates from a list, since there is no built-in method to do so.\n\n\nlist_1=[1,2,3,1,2]\nprint(list_1)\n\n\n\n[1, 2, 3, 1, 2]\n\n\n\nset_1=set(list_1)\nprint(set_1)\n\n\n\n{1, 2, 3}\n\n\n\nlist_2=list(set_1)\nprint (list_2)\n\n\n\n[1, 2, 3]"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#sorting-a-list",
    "href": "session2/DataStructuresDemo.html#sorting-a-list",
    "title": "Session 2: Python Data Structures",
    "section": "Sorting a list",
    "text": "Sorting a list\nThere are two ways to sort a list in Python\n\n.sort() modifies the original list itself. Nothing is returned.\n.sorted() returns a new list, which is a sorted version of the original list.\n.reverse=True: Use this parameter to sort the list in reverse order.\n\n\nnumber_list_1=[3,5,2,1,6,19]\nnumber_list_1.sort()\nprint(number_list_1)\n\n\n\n[1, 2, 3, 5, 6, 19]\n\n\n\nnumber_list_2=sorted(number_list_1, reverse=True)\nprint(number_list_2)\n\n\n\n[19, 6, 5, 3, 2, 1]\n\n\n\nalphabet_list_1=['a','z','e','b']\nalphabet_list_1.sort()\nprint(alphabet_list_1)\n\n\n\n['a', 'b', 'e', 'z']\n\n\n\nalphabet_list_2=sorted(alphabet_list_1, reverse=True)\nprint(alphabet_list_2)\n\n\n\n['z', 'e', 'b', 'a']\n\n\n\nmixed_list_1=[1,5,3,'a','c','b']\ntry:\n    mixed_list_1.sort()\n    print(mixed_list_1)\nexcept TypeError:\n    print(\"Can't sort a list of mixed elements\")\n\n\n\nCan't sort a list of mixed elements"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#list-comprehension",
    "href": "session2/DataStructuresDemo.html#list-comprehension",
    "title": "Session 2: Python Data Structures",
    "section": "List comprehension",
    "text": "List comprehension\nList comprehension offers a shorter syntax when you want to create a new list based on the values of an existing list (or other object)\n\n#Longer syntax with for loop\n\n\n#Example 1:\n\nsome_list=[1,2,3,'a', 'b', 'c']\nnew_list=[]\nfor item in some_list:\n    if type(item)==str:\n        new_list.append(item)\nprint(new_list)\n\n\n\n['a', 'b', 'c']\n\n\n\n#Example 2:\n\nlowercase_list=['joe', 'sarah', 'emily']\ncapital_list=[]\nfor item in lowercase_list:\n    capital_item=item.upper()\n    capital_list.append(capital_item)\nprint(capital_list)\n\n\n\n['JOE', 'SARAH', 'EMILY']\n\n\n\n#Example 3:\n\nsome_string=\"patrick\"\npatrick_list=[]\nfor letter in some_string:\n    if letter=='t' or letter=='a':\n        patrick_list.append(letter)\nprint(patrick_list)\n\n\n\n['a', 't']"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#shorter-syntax-with-list-comprehension",
    "href": "session2/DataStructuresDemo.html#shorter-syntax-with-list-comprehension",
    "title": "Session 2: Python Data Structures",
    "section": "Shorter syntax with list comprehension",
    "text": "Shorter syntax with list comprehension\n\n#Example 1:\n\nsome_list=[1,2,3,'a', 'b', 'c']\nnew_list=[x for x in some_list if type(x)==str]\nprint(new_list)\n\n\n\n['a', 'b', 'c']\n\n\n\n#Example 2:\n\nlowercase_list=['joe', 'sarah', 'emily']\ncapital_list=[name.upper() for name in lowercase_list]\nprint(capital_list)\n\n\n\n['JOE', 'SARAH', 'EMILY']\n\n\n\n#Example 3:\n\nsome_string=\"patrick\"\npatrick_list=[x for x in some_string if x=='t' or x=='a']\nprint(patrick_list)\n\n\n\n['a', 't']"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#topic-2-tuples",
    "href": "session2/DataStructuresDemo.html#topic-2-tuples",
    "title": "Session 2: Python Data Structures",
    "section": "Topic 2: Tuples",
    "text": "Topic 2: Tuples\n\nA tuple is similar to a list, but with one key difference. Tuples are immutable. This means that once you create a tuple, you cannot modify its elements.\nTuples are useful for storing data that should not be changed after creation, such as coordinates, days of the week, or fixed pairs.\nJust like lists, tuples are objects, and the class for tuples is tuple.\nTo transform another Python object into a tuple, you can use the tuple() constructor. It accepts a single iterable, such as a list, range, or string.\n\n\nTo create a tuple, you use parentheses () rather than square brackets [].\n\n# Creating a tuple\nmy_tuple = (10, 20, 30)\n\n# Accessing elements by index\nprint(\"First element:\", my_tuple[0])\nprint(\"Last element:\", my_tuple[-1])\n\n\n\nFirst element: 10\nLast element: 30"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#mutability-of-tuples",
    "href": "session2/DataStructuresDemo.html#mutability-of-tuples",
    "title": "Session 2: Python Data Structures",
    "section": "Mutability of Tuples",
    "text": "Mutability of Tuples\nTuples are immutable, so you can’t modify their elements. Attempting to change a tuple will result in an error.\n\n# Trying to modify a tuple element (this will raise an error)\ntry:\n    my_tuple[1] = 99\nexcept TypeError:\n    print(\"Tuples are immutable and cannot be changed!\")\n\n\n\nTuples are immutable and cannot be changed!"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#functions-and-tuples",
    "href": "session2/DataStructuresDemo.html#functions-and-tuples",
    "title": "Session 2: Python Data Structures",
    "section": "Functions and Tuples",
    "text": "Functions and Tuples\nFunctions can return multiple values as a tuple. This is useful for returning multiple results in a single function call.\n\n# Function that returns multiple values as a tuple\ndef min_max(nums):\n    return min(nums), max(nums)  # Returns a tuple of (min, max)\n\n# Calling the function and unpacking the tuple\n\nnumbers = [3, 7, 1, 5]\n\nour_tuple = min_max(numbers)\n\nmin_val, max_val = min_max(numbers) #Unpacking in the function call\n\nprint(our_tuple)\nprint(\"Min:\", min_val)\nprint(\"Max:\", max_val)\n\n\n\n(1, 7)\nMin: 1\nMax: 7"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#topic-3-strings",
    "href": "session2/DataStructuresDemo.html#topic-3-strings",
    "title": "Session 2: Python Data Structures",
    "section": "Topic 3: Strings",
    "text": "Topic 3: Strings\nYou can use single or double quotes to define a string (but keep it consistent!)\n\nmy_string = \"Hello, World!\"\nprint(my_string)\n\n\n\nHello, World!\n\n\n\nYou can also create a multiline string using triple quotes:\n\nmulti_line_string = \"\"\"This is\na multiline\nstring.\"\"\"\nprint(multi_line_string)\n\n\n\nThis is\na multiline\nstring."
  },
  {
    "objectID": "session2/DataStructuresDemo.html#string-operations",
    "href": "session2/DataStructuresDemo.html#string-operations",
    "title": "Session 2: Python Data Structures",
    "section": "String Operations",
    "text": "String Operations\nYou can find the length of a string using the len() funtion, just like with lists.\n\nmy_string = \"Hello, World!\"\nprint(len(my_string))\n\n\n\n13"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#accessing-characters-in-a-string",
    "href": "session2/DataStructuresDemo.html#accessing-characters-in-a-string",
    "title": "Session 2: Python Data Structures",
    "section": "Accessing characters in a string",
    "text": "Accessing characters in a string\nStrings are indexed like lists, with the first character having index 0. You can access individual characters using their index.\n\nmy_string = \"Hello, World!\"\n\n# First character\nfirst_char = my_string[0]\n\n# Last character (using negative indexing)\nlast_char = my_string[-1]\n\n# Accessing a range of characters (slicing)\nsubstring = my_string[0:5]\n\nprint(first_char, last_char, substring)\n\n\n\nH ! Hello"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#mutability-of-strings",
    "href": "session2/DataStructuresDemo.html#mutability-of-strings",
    "title": "Session 2: Python Data Structures",
    "section": "Mutability of Strings",
    "text": "Mutability of Strings\nStrings are immutable!\nUnlike lists, strings cannot be changed after creation. If you try to change an individual character, you’ll get an error.\n\nmy_string = \"Hello\"\ntry:\n    my_string[0] = \"h\"  # This will raise an error\nexcept TypeError:\n    print(\"Strings are immutable!\")\n\n\n\nStrings are immutable!"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#concatenating-strings",
    "href": "session2/DataStructuresDemo.html#concatenating-strings",
    "title": "Session 2: Python Data Structures",
    "section": "Concatenating Strings",
    "text": "Concatenating Strings\nYou can concatenate (combine) strings using the + operator:\n\ngreeting = \"Hello\"\nname = \"Patrick\"\ncombined_string = greeting + \", \" + name + \"!\"\nprint(combined_string)\n\n\n\nHello, Patrick!"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#string-methods",
    "href": "session2/DataStructuresDemo.html#string-methods",
    "title": "Session 2: Python Data Structures",
    "section": "String methods",
    "text": "String methods\nPython provides many built-in methods for manipulating strings. Some common ones are:\n\nupper() and lower() These methods convert a string to uppercase or lowercase.\n\nmy_string = \"Hello, World!\"\nprint(my_string.upper())\nprint(my_string.lower())\n\n\n\nHELLO, WORLD!\nhello, world!\n\n\n\n\nstrip() This method removes any leading or trailing whitespace from the string.\n\nmy_string = \"   Hello, World!   \"\nprint(my_string.strip())\n\n\n\nHello, World!\n\n\n\n\nreplace() You can replace parts of a string with another string.\n\nmy_string = \"Hello, World!\"\nnew_string = my_string.replace(\"World\", \"Patrick\")\nprint(new_string)\n\n\n\nHello, Patrick!\n\n\n\n\nThe split() method divides a string into a list of substrings based on a delimiter (default is whitespace).\n\nmy_string = \"Hello, World!\"\nwords = my_string.split()\nprint(words)\n\n\n\n['Hello,', 'World!']\n\n\n\nanother_string=\"Hello-World!\"\nmore_words=another_string.split(\"-\")\nprint(more_words)\n\n\n\n['Hello', 'World!']\n\n\n\n\nThe join() method takes an iterable (like a list) and concatenates its elements into a string with a specified separator between them.\n\nmy_list=['Hello,', 'my', 'name', 'is', 'Patrick']\n\nmy_string=' '.join(my_list)\n\nprint(my_string)\n\n\n\nHello, my name is Patrick"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#f-strings-python-3.6",
    "href": "session2/DataStructuresDemo.html#f-strings-python-3.6",
    "title": "Session 2: Python Data Structures",
    "section": "f-strings (Python 3.6+)",
    "text": "f-strings (Python 3.6+)\nYou can insert variables directly into strings using f-strings.\n\nname = \"Patrick\"\nage = 30\nformatted_string = f\"My name is {name} and I am {age} years old.\"\nprint(formatted_string)\n\n\n\nMy name is Patrick and I am 30 years old.\n\n\n\nmy_string = \"Hello, World!\"\n\n# Extract all vowels from the string\nvowels = str([char for char in my_string if char.lower() in \"aeiou\"])\nprint(vowels)\n\n\n\n['e', 'o', 'o']"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#string-slicing",
    "href": "session2/DataStructuresDemo.html#string-slicing",
    "title": "Session 2: Python Data Structures",
    "section": "String Slicing",
    "text": "String Slicing\nWe will slice a string using different combinations of start, end, and step to extract different parts of the string.\n\n#To slice a string, follow the string[start:end:step] format\n\n# Original string\nmy_string = \"Python is awesome!\"\nprint(f\"Original string: {my_string}\")\n\n\n\nOriginal string: Python is awesome!\n\n\n\n# Slice from index 0 to 6 (not inclusive), stepping by 1 (default)\n# This will extract \"Python\"\nsubstring_1 = my_string[0:6]\nprint(f\"Substring 1 (0:6): {substring_1}\")\n\n\n\nSubstring 1 (0:6): Python\n\n\n\n# Slice from index 7 to the end of the string, stepping by 1 (default)\n# This will extract \"is awesome!\"\nsubstring_2 = my_string[7:]\nprint(f\"Substring 2 (7:): {substring_2}\")\n\n\n\nSubstring 2 (7:): is awesome!\n\n\n\n# Slice the entire string but take every second character\n# This will extract \"Pto saeoe\"\nsubstring_3 = my_string[::2]\nprint(f\"Substring 3 (every second character): {substring_3}\")\n\n\n\nSubstring 3 (every second character): Pto saeoe\n\n\n\n# Slice from index 0 to 6, stepping by 2\n# This will extract \"Pto\"\nsubstring_4 = my_string[0:6:2]\nprint(f\"Substring 4 (0:6:2): {substring_4}\")\n\n\n\nSubstring 4 (0:6:2): Pto\n\n\n\n# Slice from index 11 to 6, stepping backward by -1\n# This will extract \"wa si\" (reverse slice)\nsubstring_5 = my_string[11:6:-1]\nprint(f\"Substring 5 (11:6:-1): {substring_5}\")\n\n\n\nSubstring 5 (11:6:-1): wa si"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#topic-4-dictionaries",
    "href": "session2/DataStructuresDemo.html#topic-4-dictionaries",
    "title": "Session 2: Python Data Structures",
    "section": "Topic 4: Dictionaries",
    "text": "Topic 4: Dictionaries\n\nA dictionary is a collection in Python that stores data as key-value pairs.\nIt’s similar to a real-world dictionary where you look up a word (the key) to get its definition (the value).\nIn Python, dictionaries are mutable, meaning you can add, remove, and change items.\n\n\nTo create a dictionary, use curly braces {}, with each key-value pair separated by a colon (:), and pairs separated by commas.\n\n# Creating a dictionary\nmy_dictionary = {\n    'name': 'Alice',\n    'age': 25,\n    'city': 'New York'\n}\nprint(my_dictionary)\n\n\n\n{'name': 'Alice', 'age': 25, 'city': 'New York'}"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#dictionary-operations",
    "href": "session2/DataStructuresDemo.html#dictionary-operations",
    "title": "Session 2: Python Data Structures",
    "section": "Dictionary Operations",
    "text": "Dictionary Operations\nAccessing dictionary values\n\nTo access a specific value in a dictionary, use the key in square brackets.\nYou can also use the .get() method, which returns None if the key does not exist, instead of raising an error.\n\n\nprint(my_dictionary['name'])      # Using key\nprint(my_dictionary.get('age'))   # Using .get() method\nprint(my_dictionary.get('gender', 'Not specified'))  # Providing a default value\n\n\n\nAlice\n25\nNot specified"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#adding-and-updating-dictionary-items",
    "href": "session2/DataStructuresDemo.html#adding-and-updating-dictionary-items",
    "title": "Session 2: Python Data Structures",
    "section": "Adding and updating dictionary items",
    "text": "Adding and updating dictionary items\nDictionaries are mutable, so you can add new items or update existing ones using assignment.\n\nmy_dictionary['job'] = 'Engineer'        # Adding a new key-value pair\nmy_dictionary['age'] = 26                # Updating an existing value\nprint(my_dictionary)\n\n\n\n{'name': 'Alice', 'age': 26, 'city': 'New York', 'job': 'Engineer'}"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#dictionary-methods",
    "href": "session2/DataStructuresDemo.html#dictionary-methods",
    "title": "Session 2: Python Data Structures",
    "section": "Dictionary Methods",
    "text": "Dictionary Methods\nPython dictionaries have several useful methods for managing data:\n\nkeys(): Returns a list of all the keys in the dictionary.\nvalues(): Returns a list of all values in the dictionary.\nitems(): Returns a list of key-value pairs as tuples.\n\n\n# Getting all keys\nprint(my_dictionary.keys())\n\n\n\ndict_keys(['name', 'age', 'city', 'job'])\n\n\n\n# Getting all values\nprint(my_dictionary.values())\n\n\n\ndict_values(['Alice', 26, 'New York', 'Engineer'])\n\n\n\n# Getting all key-value pairs\nprint(my_dictionary.items())\n\n\n\ndict_items([('name', 'Alice'), ('age', 26), ('city', 'New York'), ('job', 'Engineer')])"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#python-dictionaries-for-cancer-research-data",
    "href": "session2/DataStructuresDemo.html#python-dictionaries-for-cancer-research-data",
    "title": "Session 2: Python Data Structures",
    "section": "Python Dictionaries for Cancer Research Data",
    "text": "Python Dictionaries for Cancer Research Data\nIn cancer research, dictionaries can be used to store patient data, genetic mutations, and statistical results as key-value pairs. This allows for easy lookup, organization, and analysis of data.\n\n#Let’s create a dictionary to store basic patient information, where each patient has a unique ID, and each ID maps to a dictionary containing information about the patient’s age, cancer type, and stage.\n\n# Dictionary of patients with nested dictionaries\npatient_data = {\n    'P001': {'age': 50, 'cancer_type': 'Lung Cancer', 'stage': 'II'},\n    'P002': {'age': 60, 'cancer_type': 'Breast Cancer', 'stage': 'I'},\n    'P003': {'age': 45, 'cancer_type': 'Melanoma', 'stage': 'III'}\n}\n\nprint(patient_data)\n\n\n\n{'P001': {'age': 50, 'cancer_type': 'Lung Cancer', 'stage': 'II'}, 'P002': {'age': 60, 'cancer_type': 'Breast Cancer', 'stage': 'I'}, 'P003': {'age': 45, 'cancer_type': 'Melanoma', 'stage': 'III'}}\n\n\n\nYou can access a patient’s information using their unique ID. To access nested data, chain the keys. For example, to retrieve the cancer type of a specific patient, you’d use the following:\n\n# Accessing specific information\n\npatient_id = 'P002'\ncancer_type = patient_data[patient_id]['cancer_type']\nprint(f\"Cancer type for {patient_id}: {cancer_type}\")\n\n\n\nCancer type for P002: Breast Cancer\n\n\n\n# Updating a patient’s stage\npatient_data['P003']['stage'] = 'IV'\nprint(f\"Updated stage for P003: {patient_data['P003']['stage']}\")\n\n\n\nUpdated stage for P003: IV"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#adding-and-removing-data",
    "href": "session2/DataStructuresDemo.html#adding-and-removing-data",
    "title": "Session 2: Python Data Structures",
    "section": "Adding and Removing Data",
    "text": "Adding and Removing Data\nNew patient data can be added using assignment, and pop() or del can remove a patient’s data.\n\n# Adding a new patient\npatient_data['P004'] = {'age': 70, 'cancer_type': 'Prostate Cancer', 'stage': 'II'}\nprint(\"Added new patient:\", patient_data['P004'])\n\n\n\nAdded new patient: {'age': 70, 'cancer_type': 'Prostate Cancer', 'stage': 'II'}\n\n\n\n# Removing a patient\nremoved_patient = patient_data.pop('P001')\nprint(\"Removed patient:\", removed_patient)\n\n\n\nRemoved patient: {'age': 50, 'cancer_type': 'Lung Cancer', 'stage': 'II'}"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#dictionary-methods-1",
    "href": "session2/DataStructuresDemo.html#dictionary-methods-1",
    "title": "Session 2: Python Data Structures",
    "section": "Dictionary Methods",
    "text": "Dictionary Methods\nDictionaries allow you to retrieve keys, values, or entire key-value pairs. Here’s how to use these methods to get an overview of the data.\nkeys(): Retrieves all patient IDs. values(): Retrieves all patient records. items(): Retrieves patient records as key-value pairs\n\nFurther Example\n\n# Getting all patient IDs\nprint(\"Patient IDs:\", patient_data.keys())\n\n# Getting all patient details\nprint(\"Patient Details:\", patient_data.values())\n\n# Looping through each patient's data\nfor patient_id, details in patient_data.items():\n    print(f\"Patient {patient_id} - Age: {details['age']}, Cancer Type: {details['cancer_type']}, Stage: {details['stage']}\")\n\n\n\nPatient IDs: dict_keys(['P002', 'P003', 'P004'])\nPatient Details: dict_values([{'age': 60, 'cancer_type': 'Breast Cancer', 'stage': 'I'}, {'age': 45, 'cancer_type': 'Melanoma', 'stage': 'IV'}, {'age': 70, 'cancer_type': 'Prostate Cancer', 'stage': 'II'}])\nPatient P002 - Age: 60, Cancer Type: Breast Cancer, Stage: I\nPatient P003 - Age: 45, Cancer Type: Melanoma, Stage: IV\nPatient P004 - Age: 70, Cancer Type: Prostate Cancer, Stage: II"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#topic-5-functions-vs-methods-in-python",
    "href": "session2/DataStructuresDemo.html#topic-5-functions-vs-methods-in-python",
    "title": "Session 2: Python Data Structures",
    "section": "Topic 5: Functions vs Methods in Python",
    "text": "Topic 5: Functions vs Methods in Python\nWhat’s the difference?\n\n\n\n\n\n\n\n\nConcept\nFunction\nMethod\n\n\n\n\nDefinition\nA block of code that performs an action\nA function that is associated with an object\n\n\nCalled on\nStandalone / with parameters\nCalled on an object (e.g., a string or list)\n\n\nSyntax\nfunction(arg)\nobject.method()\n\n\nExample\nlen(\"hello\")\n\"hello\".upper()"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#function-example",
    "href": "session2/DataStructuresDemo.html#function-example",
    "title": "Session 2: Python Data Structures",
    "section": "Function Example",
    "text": "Function Example\n\n# Define your own function\ndef greet(name):\n    return f\"Hello, {name}!\"\n\nprint(greet(\"Alice\"))  # Hello, Alice!\n\n\n\nHello, Alice!\n\n\n\n# Use a built-in function\nwords = [\"Python\", \"Data\", \"Science\"]\nprint(len(words))  # 3\n\n\n\n3"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#method-example",
    "href": "session2/DataStructuresDemo.html#method-example",
    "title": "Session 2: Python Data Structures",
    "section": "Method Example",
    "text": "Method Example\n\n# 'upper()' is a method of string objects\nname = \"patrick\"\nprint(name.upper())  # Output: PATRICK\nprint(name) #Why doesn't name.upper() modify the original string? What would we have to do so that name becomes permanently uppercase?\n\n\n\nPATRICK\npatrick\n\n\n\n# 'append()' is a method of list objects\ncolors = [\"red\", \"blue\"]\ncolors.append(\"green\")\nprint(colors)  # ['red', 'blue', 'green']\n\n#Think back to the last example.  Why does colors permanently change when we use a method, but name did not?\n\n\n\n['red', 'blue', 'green']"
  },
  {
    "objectID": "session2/DataStructuresDemo.html#behind-the-scenes",
    "href": "session2/DataStructuresDemo.html#behind-the-scenes",
    "title": "Session 2: Python Data Structures",
    "section": "Behind the Scenes",
    "text": "Behind the Scenes\n\n# This also works\nstr.upper(\"patrick\")  # Output: 'PATRICK'\n\n\n\n'PATRICK'\n\n\n\n# But this is more common\n\"patrick\".upper()  # Output: 'PATRICK'\n\n\n\n'PATRICK'\n\n\n\nBoth work — but “str.upper()” is a method being called directly from the class."
  },
  {
    "objectID": "session2/DataStructuresDemo.html#you-try",
    "href": "session2/DataStructuresDemo.html#you-try",
    "title": "Session 2: Python Data Structures",
    "section": "You Try!",
    "text": "You Try!\nNavigate to the follow-along file and try the practice problems!"
  },
  {
    "objectID": "session4/session4v2_slides.html#session-overview",
    "href": "session4/session4v2_slides.html#session-overview",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Session Overview",
    "text": "Session Overview\n\nIn this session, we’ll explore how Python’s object-oriented nature affects our modeling workflows. \nTopics:\n\n\n\nIntro to OOP and how it makes modeling in Python different from R\n\n\nBuilding and extending classes using inheritance and mixins\n\n\nApplying OOP to machine learning through demos with scikit-learn\n\n\n\nCreating and using models\n\n\nPlotting data with plotnine and seaborn\n\n\n\n\n\nThis is the last session of the intro to python workshops. There will be a brief ‘homework’ that covers the topics in this session. I do not anticipate having much time left over at the end of this session so if you have any questions about that please feel free to reach out to us via teams!"
  },
  {
    "objectID": "session4/session4v2_slides.html#why-python",
    "href": "session4/session4v2_slides.html#why-python",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Why Python? 🐍",
    "text": "Why Python? 🐍\n\n\n\nR: Built by Statisticians for Statisticians\n\nExcels at:\n\nStatistical analysis and modeling\n\nClean outputs and tables from models\nBeautiful data visualizations with simple code\n\n\n\nPython: General-Purpose Language\n\nExcels at:\n\nMachine Learning, Neural Networks & Deep Learning (scikit-learn, PyTorch, TensorFlow)\n\nImage & Genomic Data Analysis (scikit-image, biopython, scanpy)\nSoftware & Command Line Interfaces, Web Scraping, Automation\n\n\n\nPython’s broader ecosystem makes it the go-to language in domains like AI, bioinformatics, data engineering, and computational biology.\n\nNote: Packages like rpy2 and reticulate make it possible to use both R and Python in the same project, but those are beyond the scope of this course.\nA primer on reticulate is available here: https://www.r-bloggers.com/2022/04/getting-started-with-python-using-r-and-reticulate/\n\n\n\nAs we talked about in session 1, R and Python have different strengths. R was designed for statistics and excels at statistical analysis & modeling, clean outputs and beautiful visualizations. Python is a general purpose programming language that excels at things like machine learning, image/genomic analysis and software.\nPython’s broader ecosystem makes it the go-to language for things like AI, bioinformatics, data engineering and computational biology.\nThere are packages like rpy2 (for python) and reticulate (for R) that make it possible to use both R and python in the same project, but those are beyond the scope of this course."
  },
  {
    "objectID": "session4/session4v2_slides.html#programming-styles-r-vs-python",
    "href": "session4/session4v2_slides.html#programming-styles-r-vs-python",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Programming Styles: R vs Python",
    "text": "Programming Styles: R vs Python\n\n In the first session, we talked briefly about functional vs object-oriented programming:\n\n\nFunctional programming: focuses on functions as the primary unit of code  Object-oriented programming: uses objects with attached attributes(data) and methods(behaviors) \n\n\nR leans heavily on the functional paradigm — you pass data into functions and get back results, in most cases without altering the original data. Functions and pipes (%&gt;%) dominate most workflows.\nIn Python, everything is an object, even basic things like lists, strings, and dataframes. A lot of ‘functions’ are instead written as object-associated methods. Some of these methods modify the objects in-place by altering their attributes. Understanding how this works is key to using Python effectively!\n\n\nYou’ve already seen this object-oriented style in Sessions 2 and 3 — you create objects like lists or dataframes, then call methods on them like .append() or .sort_values(). In python, instead of piping, we sometimes chain methods together.\n\n\n\nIn the first session we briefly mentioned functional vs object-oriented programming. Functional programming uses functions asa the primary unit of code while object-oriented programming uses objects with attached attributes (data) and methods (behaviors).\nWhen we’re programming in R, we typically use (and sometimes write) functions that we pass data into and get back results without altering the original data. Functions and pipes dominate most workflows.\nIn python, we use objects. Everything is an object (lists, strings, dataframes, etc) and a lot of ‘functions’ are written as object-associated methods. Some of these methods modify the objects in place by altering their attributes instead of returning a new object. Understanding this is key to using python effectively."
  },
  {
    "objectID": "session4/session4v2_slides.html#modeling-in-python",
    "href": "session4/session4v2_slides.html#modeling-in-python",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Modeling in Python",
    "text": "Modeling in Python\n\nPython absolutely uses functions—just like R! They’re helpful for data transformation, wrangling, and automation tasks like looping and parallelization. \nBut when it comes to modeling, libraries are designed around classes: blueprints for creating objects that store data (attributes) and define behaviors (methods). \n\nscikit-learn is great for getting started—everything follows a simple, consistent OOP interface. Its API is also consistant with other modeling packages, like xgboost and scvi-tools.\nscikit-survival is built on top of scikit-learn. https://scikit-survival.readthedocs.io/en/stable/user_guide/00-introduction.html is a good tutorial for it.\nPyTorch and TensorFlow are essential if you go deeper into neural networks or custom models—you’ll define your own model classes with attributes and methods, but the basic structure is similar to scikit-learn.\n\nstatsmodels is an alternative to scikit-learn for statistical analyses and has R-like syntax and outputs. It’s a bit more complex than scikit-learn and a bit less consistant with other packages in the python ecosystem. https://wesmckinney.com/book/modeling is a good tutorial for statsmodels.\n\n\n💡 To work effectively in Python, especially for tasks involving modeling or model training, it helps to think in terms of objects and classes, not just functions.\n\n\n\nEven though python is more object focused, it still uses functions! They are particualrly helpful for things like data transformation, data wrangling and automation tasks like looping and parallelization. However, when it comes to modeling, libraries are designed around classes, which are like blueprints for creating objects that store data and define behaviors.\nThese are some of the most commonly used modeling libraries in python. For this session, we will focus on scikit-learn, which is relatively simple and follows a consistent interface. Its API (Application Programming Interface) is also consistant with other modeling packages like xgboost and scvi-tools. scikit-survival is built off of scikit-learn and therefore has a similar API.\nPytorch and tensorflow are more complex but essential for neural networks/deep learning models. With these packages, you define your own model classes, but the basic class structure is similar to sklearn.\nFinally, there is statsmodels, which is an alternative to sklearn for statistical analyses. It has R-like syntax and outputs and is a bit more complicated to use than sklearn. Personally, if I wanted R-like outputs I’d use reticulate or just save my data and re-load in R, but statsmodels is available if needed."
  },
  {
    "objectID": "session4/session4v2_slides.html#why-does-oop-matter-in-python-modeling",
    "href": "session4/session4v2_slides.html#why-does-oop-matter-in-python-modeling",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Why Does OOP Matter in Python Modeling?",
    "text": "Why Does OOP Matter in Python Modeling?\n\nIn Python modeling frameworks:\n\n\n\nModels are instances of classes\n\n\nYou call methods like .fit(), .predict(), .score()\n\n\nInternal model details like coefficients or layers are stored as attributes\n\n\n\nThis makes model behavior consistent between model classes and even libraries. It also simplifies creating/using pre-trained models: both the architecture and learned weights are bundled into a single object with expected built-in methods like .predict() or .fine_tune().\n\n\nInstead of having a separate results object, like in R, you would retrieve your results by accessing an attribute or using a method that is attached to the model object itself.\n\n\n We’ll focus on scikit-learn in this session, but these ideas carry over to other libraries like xgboost, statsmodels, and PyTorch. \n\n\nSo why is OOP so important for python modeling? First, models in python are instances of classes that have attahced methods like fit, predict and score. The internal model details like coefficients or layers are stored as attributes.\nDefinining models as instances of classes is useful because it makes model behavior consistant between model classes (due to inheritance which i’ll explain more about shortly) and even libraries. It also simplifies creating/using pre-trained models because the model architecture and learned weights are bundled into a single object that can be loaded. Expected built-in methods are also included in this bundle.\nAdditionally, instead of having a separate results object like in R, you can retrieve your results by either accessing an attribute or using a method attached to the model object itself."
  },
  {
    "objectID": "session4/session4v2_slides.html#key-oop-principles-recap",
    "href": "session4/session4v2_slides.html#key-oop-principles-recap",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Key OOP Principles (Recap)",
    "text": "Key OOP Principles (Recap)\n\nIn OOP, code is structured around objects (as opposed to functions). This paradigm builds off the following principles:\n\n\nEncapsulation: Bundling data and methods together in a single unit.\n\nA StandardScaler object stores mean and variance data and has .fit() and .transform() methods\n\n\n\n\n\nInheritance: Creating new classes based on existing ones.\n\nsklearn.LinearRegression inherits attributes and methods from a general regression model class.\n\n\n\n\n\n\nAbstraction: Hiding implementation details and exposing only essential functionality.\n\ne.g., .fit() works the same way from the outside, regardless of model complexity\n\n\n\n\n\n\nPolymorphism: Objects of different types can be treated the same way if they implement the same methods.\n\nPython’s duck typing:\n\n🦆 “If it walks like a duck and quacks like a duck, then it must be a duck.” 🦆\n\nex: If different objects all have a .summarize() method, we can loop over them and call .summarize() without needing to check their class. As long as the method exists, Python will know what to do.\nThis lets us easily create pipelines that can work for many types of models.\n\n\n\n\nWe won’t cover pipelines here, but they are worth looking into!\n\n\n\n\nIf you read through the OOP pre-reading this will be a review. In OOP, code is structured around objects and this paradigm builds off of the following principles: 1: Encapsulation - bundling data and methods together in a single unit. For example, an object of the Standard Scalar class, which we’ll use later, stores mean and variance data and has fit and transform methods. 2: Inheritance - which allows us to create new classes that inherit attributes and methods from existing classes. For example, the lienar regression model class in sklearn inherits attributes and methods from a general regression model class 3: Abstraction - hiding implementation detals and exposing only essential functionality For example, the fit method works the same way from the outside regardless of model complexity 4: Polymorphism means we can treat objects of different types the same way—as long as they implement the same method.\nIn Python, this is done through something called duck typing, which comes from the phrase: “If it walks like a duck and quacks like a duck, it’s probably a duck.” In other words, Python doesn’t care what class an object is—it just checks whether it has the method you’re trying to call.\nSo if three different objects each have a .summarize() method, we can loop through them and call .summarize() without needing to check their type first. As long as the method exists and accepts the right arguments, Python will just run it. This flexibility is one of the biggest reasons OOP is so useful for machine learning. Most model classes in Python—whether it’s logistic regression, random forests, or a neural network—come with a standard set of methods like .fit(), .predict(), and .score(). That consistency means we can swap models in and out with very little change to our code. It also makes it easy to build reusable code—like pipelines—that can work across different model types without needing special handling for each one."
  },
  {
    "objectID": "session4/session4v2_slides.html#classes-and-objects",
    "href": "session4/session4v2_slides.html#classes-and-objects",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Classes and Objects",
    "text": "Classes and Objects\n\nClasses are blueprints for creating objects. Each object contains:\n\n\n\nAttributes (data): model coefficients, class labels\n\n\nMethods (behaviors): .fit(), .predict()\n\n\n👉 To Get the class of an object, use:\n\ntype(object) # Returns the type of the object\n\n👉 To check if an object is an instance of a particular class, use:\n\nisinstance(object, class)  # Returns True if `object` is an instance of `class`.\n\n\n\nKnowing what class an object belongs to helps us understand what methods and attributes it provides.\n\n\n\nLike I mentioned earlier, classes are blueprints for creating objects which contain attributes (data) like model coefficients or labels and methods like fit and predict.\nTo get the class of an object in python, we can use the ‘type’ function. To check if an object is an instance of a particular class, we can use the ‘isinstance’ function.\nKnowing what class an object belongs to can help us understand what methods and attibutes it should have, but we do not need to know class to use a method."
  },
  {
    "objectID": "session4/session4v2_slides.html#base-classes",
    "href": "session4/session4v2_slides.html#base-classes",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Base Classes",
    "text": "Base Classes\n\nA base class (or parent class) serves as a template for creating objects. Other classes can inherit from it to reuse its properties and methods.\nClasses are defined using the class keyword, and their structure is specified using an __init__() method for initialization.\n\nFor example, we can define a class called Dog and give it attributes that store data about a given dog and methods that represent behaviors an object of the Dog class can perform. We can also edit the special or “dunder” methods (short for double underscore) that define how objects behave in certain contexts.\n\nclass Dog: ## begin class definition\n    def __init__(self, name, breed): ## define init method\n        self.name = name ## add attributes\n        self.breed = breed\n\n    def speak(self): ## add methods\n        return f\"{self.name} says woof!\"\n\n    def __str__(self): # __str__(self) tells python what to display when an object is printed\n        return f\"Our dog {self.name}\"\n\n    def __repr__(self): # add representation to display when dog is called in console\n        return f\"Dog(name={self.name!r}, breed={self.breed!r})\"\n\n\n\n\nA base class serves as a template for creating objects and other classes can inherit its attributes and methods. Classes are defined using the ‘class’ keyword and their structure is specified using an init method for initialization.\nFor example, we can create a class called Dog and give it attributes and methods. We can also edit special or dunder methods that define how objects of this class behave in certain contexts.\nFirst we will define our init method, which sets up the structure for objects of the dog class. The parameters here (name and breed) can be stored as attributes. The speak method here defines the speech behavior for the dog and returns a string, that changes based on the value stored in the ‘name’ attribute.\nWe can also define our str and repr methods which tell python what to do when an object is printed (str) or called in console (repr). These methods are useful for displaying information about objects at a glance. We will see an interesting example of a repr method later on."
  },
  {
    "objectID": "session4/session4v2_slides.html#creating-a-dog",
    "href": "session4/session4v2_slides.html#creating-a-dog",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Creating a dog",
    "text": "Creating a dog\n\nCreating an instance of the Dog class lets us model a particular dog:\n\nbuddy = Dog(\"Buddy\", \"Golden Retriever\")\nprint(f\"Buddy is an object of class {type(buddy)}\")\n\n\n\nBuddy is an object of class &lt;class '__main__.Dog'&gt;\n\n\n\n\nWe set the value of the attributes [name and breed], which are then stored as part of the buddy object\n\n\nWe can use any methods defined in the Dog class on buddy\n\n\n\n## if we want to see what kind of dog our dog is\n## we can call buddy's attributes\nprint(f\"Our dog {buddy.name} is a {buddy.breed}.\")\n\n## we can also call any Dog methods\nprint(buddy.speak())  \n\n## including special methods\nbuddy ## displays what was in the __repr__() method\n\n\n\nOur dog Buddy is a Golden Retriever.\nBuddy says woof!\n\n\nDog(name='Buddy', breed='Golden Retriever')\n\n\nNote: For python methods, the self argument is assumed to be passed and therefore we do not put anything in the parentheses when calling .speak(). For attributes, we do not put () at all.\n\n\nIf we want to model a particular dog, we create an instance of the dog class. The arguments we pass to Dog set the name and breed of the dog.\nIf we want to prove that buddy is an object of class dog, we can use type(buddy).\nOnce we create buddy, we can use any attributes or methods defined in the dog class."
  },
  {
    "objectID": "session4/session4v2_slides.html#derived-child-classes",
    "href": "session4/session4v2_slides.html#derived-child-classes",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Derived (Child) Classes",
    "text": "Derived (Child) Classes\n\nDerived/child classes build on base classes using the principle of inheritence. \nNow that we have a Dog class, we can build on it to create a specialized GuardDog class.\n\nclass GuardDog(Dog):  # GuardDog inherits from Dog\n    def __init__(self, name, breed, training_level): ## in addition to name and breed, we can \n        # define a training level. \n        # Call the parent (Dog) class's __init__ method\n        super().__init__(name, breed)\n        self.training_level = training_level  # New attribute for GuardDog that stores the \n        # training level for the dog\n\n    def guard(self): ## checks if the training level is &gt; 5 and if not says train more\n        if self.training_level &gt; 5:\n            return f\"{self.name} is guarding the house!\"\n        else:\n            return f\"{self.name} needs more training before guarding.\"\n    \n    def train(self): # modifies the training_level attribute to increase the dog's training level\n        self.training_level = self.training_level + 1\n        return f\"Training {self.name}. {self.name}'s training level is now {self.training_level}\"\n\n# Creating an instance of GuardDog\nrex = GuardDog(\"Rex\", \"German Shepherd\", training_level= 5)\n\n\n\nDerived or child classes build on base classes using the principle of inheritence. This helps us build more specialized classes without having to start from scratch.\nIf we want to make a new guardDog class, we can start by inheriting the attributes and methods of the dog class by putting ‘Dog’ in the parentheses here.\nWhen we define our init method, we can first call the parent class’s init method to assign the dog’s name and breed. We can also add an additional attribute that stores the dog’s training level.\nBecause of inheritence, guarddogs also have the speak method, but we can also give them new methods like guard and train. The guard method, like the speak method earlier, just returns a value. However, the train method actually modifies the training_level attribute of the object in-place. This means that, if we want to train a guarddog, we do not have to assign the trained dog to a new variable to preserve the trained state.\nLet’s make an instance of guarddog called rex and we can see how this works."
  },
  {
    "objectID": "session4/session4v2_slides.html#mixins",
    "href": "session4/session4v2_slides.html#mixins",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Mixins",
    "text": "Mixins\n\nA mixin is a special kind of class designed to add functionality to another class. Unlike base classes, mixins aren’t used alone.\n\nFor example, scikit-learn uses mixins like:\n- sklearn.base.ClassifierMixin (adds classifier-specific methods)\n- sklearn.base.RegressorMixin (adds regression-specific methods)\nwhich it adds to the BaseEstimator class to add functionality.  \nTo finish up our dog example, we are going to define a mixin class that adds learning tricks to the base Dog class and use it to create a new class called SmartDog.\n\n\n\nMixins area special kind of class that take advantage of the inheritence property to add functionality to other classes without needing to re-define the initial class structure. Unlike base classes, mixins are not useful on their own.\nSome examples of mixin classes are ClassifierMixin and RegressorMixin from scikit learn, which add functionality to the BaseEstimator class. To finish up our dog example, we are going to define a mixin class that adds learning tricks to the base Dog class and use it to create a new class called SmartDog."
  },
  {
    "objectID": "session4/session4v2_slides.html#duck-typing",
    "href": "session4/session4v2_slides.html#duck-typing",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Duck Typing",
    "text": "Duck Typing\n\n\n🦆 “If it quacks like a duck and walks like a duck, it’s a duck.” 🦆\n\nPython’s duck typing makes our lives a lot easier, and is one of the main benefits of methods over functions:\n\n\nInheritence - objects inherit methods from base classes\n\n\nRepurposing old code - methods by the same name work the same for different model types\n\n\nUse methods without checking types - methods are assumed to work on the object they’re attached to\n\n\n\nWe can demonstrate duck typing by defining two new base classes that are different than Dog but also have a speak() method.\n\n\nclass Human:\n    def __init__(self, name):\n        self.name = name\n\n    def speak(self):\n        return f\"{self.name} says hello!\"\n\nclass Parrot:\n    def __init__(self, name):\n        self.name = name\n\n    def speak(self):\n        return f\"{self.name} says squawk!\"\n\n\nIn my opinion python’s ducktyping makes our lives a lot easier and is a good reason to use methods when possible. Duck typing makes it easy to repurpose old code with few changes - because methods by the same name work the same for different model types, even if the underlying method code is very different. Models from the same package likely share a base class, or at least can be expected to have a similar API. Duck typing also stops us from having to check object types before performing operations as methods are assumed to work on the object they’re attached to.\nWe can demonstrate this by defining two new base classes that are different than dog but also have a speak method. -adv-\nThe human and parrot class here both have a speak method and a name attribute like the dog class."
  },
  {
    "objectID": "session4/session4v2_slides.html#duck-typing-in-action",
    "href": "session4/session4v2_slides.html#duck-typing-in-action",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Duck Typing in Action",
    "text": "Duck Typing in Action\n\nEven though Dog, Human and Parrot are entirely different classes…\n\n\ndef call_speaker(obj):\n    print(obj.speak())\n\ncall_speaker(Dog(\"Fido\", \"Labrador\"))\ncall_speaker(Human(\"Alice\"))\ncall_speaker(Parrot(\"Polly\"))\n\n\n\nFido says woof!\nAlice says hello!\nPolly says squawk!\n\n\n\nThey all implement .speak(), so Python treats them the same!\nIn the context of our work, this would allow us to make a pipeline using models from different libraries that have the same methods.\nWhile our dog example was very simple, this is the same way that model classes work in python!\n\n\n\n\n\n\nWarning\n\n\nWith duck typing, Python lets us use methods without breaking. It does not mean that any given method is correct to use in all cases, or that all similar objects will have the same methods.\n\n\n\n\nEven though the three classes are different, we can still write a function that performs ‘obj.speak()’ and pass in any object with this method without having to worry about the object’s class."
  },
  {
    "objectID": "session4/session4v2_slides.html#example-oop-in-machine-learning-and-modeling",
    "href": "session4/session4v2_slides.html#example-oop-in-machine-learning-and-modeling",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Example: OOP in Machine Learning and Modeling",
    "text": "Example: OOP in Machine Learning and Modeling\n\nMachine learning models in Python are implemented as classes.\n\n\n\nWhen you create a model, you’re instantiating an object of a predefined class (e.g., LogisticRegression()).\n\n\nThat model has attributes (parameters, coefficients) and methods (like .fit() and .predict()).\n\n\nFor example LogisticRegression is a model class that inherits from SparseCoefMixin and BaseEstimator.\nclass LogisticRegression(LinearClassifierMixin, SparseCoefMixin, BaseEstimator):\n  \nTo perform logistic regression, we create an instance of the LogisticRegression class.\n## Example: \nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()  # Creating an instance of the LogisticRegression class\nmodel.fit(X_train, y_train)   # Calling a method to train the model\npredictions = model.predict(X_test)  # Calling a method to make predictions\ncoefs = model.coef_ # Access model coefficients using attribute\n\n\nBecause models in python are defined by classes, when you create a model you are instantiating an object of a predefined class. When you fit a model and use it to make predictions, you are using methods! You can also access attributes like .coef_ to get coefficients."
  },
  {
    "objectID": "session4/session4v2_slides.html#key-benefits-of-oop-in-machine-learning",
    "href": "session4/session4v2_slides.html#key-benefits-of-oop-in-machine-learning",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Key Benefits of OOP in Machine Learning",
    "text": "Key Benefits of OOP in Machine Learning\n\n\nEncapsulation – Models store parameters and methods inside a single object.\n\nInheritance – New models can build on base models, reusing existing functionality.\n\nAbstraction – .fit() should work as expected, regardless of complexity of underlying implimentation.\nPolymorphism (Duck Typing) – Different models share the same method names (.fit(), .predict()), making them easy to use interchangeably, particularly in analysis pipelines.\n\nUnderstanding base classes and mixins is especially important when working with deep learning frameworks like PyTorch and TensorFlow, which require us to create our own model classes.\n\n\nTo recap: python’s oject oriented nature has some notable benefits for modeling and machine learning."
  },
  {
    "objectID": "session4/session4v2_slides.html#mini-project-classifying-penguins-with-scikit-learn",
    "href": "session4/session4v2_slides.html#mini-project-classifying-penguins-with-scikit-learn",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "🐧 Mini Project: Classifying Penguins with scikit-learn",
    "text": "🐧 Mini Project: Classifying Penguins with scikit-learn\n\nNow that you understand classes and data structures in Python, let’s apply that knowledge to classify penguin species using two features:\n\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nWe’ll explore:\n\n\nUnsupervised learning with K-Means clustering (model doesn’t ‘know’ y)\n\n\nSupervised learning with a k-NN classifier (model trained w/ y information)\n\n\nAll scikit-learn models are designed to have\n\n\nCommon Methods:\n\n\n\n.fit() — Train the model\n\n\n.predict() — Make predictions\n\n\n\nCommon Attributes:\n\n\n.classes_, .n_clusters_, etc.\n\n\n\n\nThis is true of the scikit-survival package too!\n\n\n\nFor the rest of this session, we’ll be going through 2 mini projects to classify penguin species using bill length and bill depth using kmeans and knn models from scikit-learn. In terms of API, all sklearn models are designed to have common methods like fit and predict and have some attributes like .classes_ or .n_clusters_ that store information needed to specify the model."
  },
  {
    "objectID": "session4/session4v2_slides.html#import-libraries",
    "href": "session4/session4v2_slides.html#import-libraries",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Import Libraries",
    "text": "Import Libraries\n\nBefore any analysis, we must import the necessary libraries.\nFor large libraries like scikit-learn, PyTorch, or TensorFlow, we usually do not import the entire package. Instead, we selectively import the classes and functions we need.\n\n\nClasses\n- StandardScaler — for feature scaling\n- KNeighborsClassifier — for supervised k-NN classification\n- KMeans — for unsupervised clustering\n\n\n🔤 Naming Tip:\n- CamelCase = Classes\n- snake_case = Functions\n\n\nFunctions\n- train_test_split() — to split data into training and test sets\n- accuracy_score() — to evaluate classification accuracy\n- classification_report() — to print precision, recall, F1 (balance of precision and recall), Support (number of true instances per class) - adjusted_rand_score() — to evaluate clustering performance\n\n\n\nOur first step is always to import any necessary libraries - but in this case, we will import specific classes and functions we plan to use. For our classes, we’ll need standardscaler as well as our two model classes. We’ll also import some useful functions for splitting our dataset into training/test sets and computing classification scores.\nA quick tip: for most python libraries, classes use CamelCase and functions use snake_case."
  },
  {
    "objectID": "session4/session4v2_slides.html#import-libraries-1",
    "href": "session4/session4v2_slides.html#import-libraries-1",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Import Libraries",
    "text": "Import Libraries\n\n## imports\nimport pandas as pd\nimport numpy as np\n\nfrom plotnine import *\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom great_tables import GT\n\n## sklearn imports\n\n## import classes\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\n\n## import functions\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, adjusted_rand_score"
  },
  {
    "objectID": "session4/session4v2_slides.html#data-preparation",
    "href": "session4/session4v2_slides.html#data-preparation",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Data Preparation",
    "text": "Data Preparation\n\n# Load the Penguins dataset\npenguins = sns.load_dataset(\"penguins\").dropna()\n\n# Make a summary table for the penguins dataset, grouping by species. \nsummary_table = penguins.groupby(\"species\").agg({\n    \"bill_length_mm\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"bill_depth_mm\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"sex\": lambda x: x.value_counts().to_dict()  # Count of males and females\n})\n\n# Round numeric values to 1 decimal place (excluding the 'sex' column)\nfor col in summary_table.columns:\n    if summary_table[col].dtype in [float, int]:\n        summary_table[col] = summary_table[col].round(1)\n\n# Display the result\ndisplay(summary_table)\n\n\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nsex\n\n\n\nmean\nstd\nmin\nmax\nmean\nstd\nmin\nmax\n&lt;lambda&gt;\n\n\nspecies\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n38.8\n2.7\n32.1\n46.0\n18.3\n1.2\n15.5\n21.5\n{'Male': 73, 'Female': 73}\n\n\nChinstrap\n48.8\n3.3\n40.9\n58.0\n18.4\n1.1\n16.4\n20.8\n{'Female': 34, 'Male': 34}\n\n\nGentoo\n47.6\n3.1\n40.9\n59.6\n15.0\n1.0\n13.1\n17.3\n{'Male': 61, 'Female': 58}\n\n\n\n\n\n\n\n\nWe’re using the penguins dataset from the seaborn package, so we can use the load_dataset function to get that. We can also drop any na values with the .dropna() method.\nWe can then make a quick summary table for the dataset using some of the methods covered last session."
  },
  {
    "objectID": "session4/session4v2_slides.html#data-visualization",
    "href": "session4/session4v2_slides.html#data-visualization",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nTo do visualization, we can use either seaborn or plotnine. plotnine mirrors ggplot2 syntax from R and is great for layered grammar-of-graphics plots, while seaborn is more convienient if you want to put multiple plots on the same figure. \nPlotting with Plotnine vs Seaborn\n\n\nPlotnine (like ggplot2 in R) The biggest differences between plotnine and ggplot2 syntax are:\n\n\nWith plotnine the whole call is wrapped in () parentheses\n\n\nVariables are called with strings (\"\" are needed!)\n\n\nIf you don’t use from plotnine import *, you will need to import each individual function you plan to use!\n\n\n\nSeaborn (base matplotlib + enhancements)\n\n\nDesigned for quick, polished plots\n\n\nWorks well with pandas DataFrames or NumPy arrays\n\n\nIntegrates with matplotlib for customization\n\n\nGood for things like decision boundaries or heatmaps\n\n\nHarder to customize than plotnine plots\n\n\n\n\n\nTo visually check out our data, we can use either plotnine (which is designed to work like ggplot) or seaborn (base matplotlib + enhancements). For single plots, plotnine can be more convenient but seaborn has better support for putting multiple plots on the same figure."
  },
  {
    "objectID": "session4/session4v2_slides.html#scatterplot-with-plotnine",
    "href": "session4/session4v2_slides.html#scatterplot-with-plotnine",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Scatterplot with plotnine",
    "text": "Scatterplot with plotnine\n\nTo take a look at the distribution of our species by bill length and bill depth before clustering…\n\n\nplot1 = (ggplot(penguins, aes(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"species\"))\n + geom_point()\n + ggtitle(\"Penguin Species\")\n + theme_bw())\n\ndisplay(plot1)\n\n\n\n\n\n\n\n\n\n\n\nTo plot our bill length and depth with plotnine, we can pretty much use ggplot syntax, but wrap the whole statement in parentheses. However, our variable names need to be entered as strings."
  },
  {
    "objectID": "session4/session4v2_slides.html#scatterplot-with-seaborn",
    "href": "session4/session4v2_slides.html#scatterplot-with-seaborn",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Scatterplot with seaborn",
    "text": "Scatterplot with seaborn\nWe can make a similar plot in seaborn. This time, let’s include sex by setting the point style\n\n# Create the figure and axes obects\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Create a plot \nsns.scatterplot(\n    data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\",\n    hue=\"species\", ## hue = fill\n    style=\"sex\",  ## style = style of dots\n    palette=\"Set2\", ## sets color pallet\n    edgecolor=\"black\", s=300, ## line color and point size \n    ax=ax              ## Draw plot on ax      \n)\n\n# Use methods on ax to set title, labels\nax.set_title(\"Penguin Bill Length vs Depth by Species\")\nax.set_xlabel(\"Bill Length (mm)\")\nax.set_ylabel(\"Bill Depth (mm)\")\nax.legend(title=\"Species\")\n\n# Plot the figure\nfig.tight_layout() \n#fig.show() -&gt; if not in interactive\n\n\n\nIf we want to do the same thing in seaborn, we can use sns.scatterplot() directly (which doesn’t save the figure to a variable), or we can create figure and axis objects first and then draw the plot on the created axis. We then use methods on the axis object to set the titles and labels.\nBecause we are working in an interactive notebook, we don’t have to explicitly use plt.plot() to display the figure."
  },
  {
    "objectID": "session4/session4v2_slides.html#scatterplot-with-seaborn-output",
    "href": "session4/session4v2_slides.html#scatterplot-with-seaborn-output",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Scatterplot with seaborn",
    "text": "Scatterplot with seaborn"
  },
  {
    "objectID": "session4/session4v2_slides.html#scaling-the-data---understanding-the-standard-scaler-class",
    "href": "session4/session4v2_slides.html#scaling-the-data---understanding-the-standard-scaler-class",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Scaling the data - Understanding the Standard Scaler class",
    "text": "Scaling the data - Understanding the Standard Scaler class\n\nFor our clustering to work well, the predictors should be on the same scale. To achieve this, we use an instance of the StandardScaler class.\nclass sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)\n\n\nParameters are supplied by user\n- copy, with_mean, with_std \nAttributes contain the data of the object\n- scale_: scaling factor\n- mean_: mean value for each feature\n- var_: variance for each feature\n- n_features_in_: number of features seen during fit\n- n_samples_seen: number of samples processed for each feature \nMethods describe the behaviors of the object and/or modify its attributes\n- fit(X): computes mean and std used for scaling and ‘fits’ scaler to data X\n- transform(X): performs standardization by centering and scaling X with fitted scaler\n- fit_transform(X): does both\n\nFor our clustering to work well, we typically want to put all of the predictors on the same scale. We can do this using an instance of the StandardScaler class.\nWe supply the parameters that dictate the behavior of the scaler at instantiation. The new scaler object has attributes that contain the information the object needs to perform its functions, like the scaling factor, feature means and variances, etc.\nIt also has methods that can modify its attributes, like the fit method, which calculates the mean and variance used for scaling based on data X, and the tranform method which uses the calculated values to scale X.\nFor convenience there is also the fit_transform() method that does both to the same data. When we look at our model classes, we’ll see that they also have fit methods."
  },
  {
    "objectID": "session4/session4v2_slides.html#scaling-data",
    "href": "session4/session4v2_slides.html#scaling-data",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Scaling Data",
    "text": "Scaling Data\n\n\n# Selecting features for clustering -&gt; let's just use bill length and bill depth.\nX = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]]\ny = penguins[\"species\"]\n\n# Standardizing the features for better clustering performance\nscaler = StandardScaler() ## create instance of StandardScaler\nX_scaled = scaler.fit_transform(X) \n\n\n\n\n\n\n\n\n\n\nOriginal vs Scaled Features\n\n\nFeature\nOriginal\nScaled\n\n\nBill Length\nBill Depth\nBill Length\nBill Depth\n\n\n\n\nmean\n44\n17\n0\n0\n\n\nstd\n5\n2\n1\n1\n\n\n\n\n\n\n        \n\n\n\n\nShow table code\n## Make X_scaled a pandas df\nX_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n\n# Compute summary statistics and round to 2 sig figs\noriginal_stats = X.agg([\"mean\", \"std\"])\nscaled_stats = X_scaled_df.agg([\"mean\", \"std\"])\n\n# Combine into a single table with renamed columns\nsummary_table = pd.concat([original_stats, scaled_stats], axis=1)\nsummary_table.columns = [\"Bill_Length_o\", \"Bill_Depth_o\", \"Bill_Length_s\", \"Bill_Depth_s\"]\nsummary_table.index.name = \"Feature\"\n\n# Display nicely with great_tables\n(\n    GT(summary_table.reset_index()).tab_header(\"Original vs Scaled Features\")\n    .fmt_number(columns =  [\"Bill_Length_o\", \"Bill_Depth_o\", \"Bill_Length_s\", \"Bill_Depth_s\"], decimals=0)\n    .tab_spanner(label=\"Original\", columns=[\"Bill_Length_o\", \"Bill_Depth_o\"])\n    .tab_spanner(label=\"Scaled\", columns=[\"Bill_Length_s\", \"Bill_Depth_s\"])\n    .cols_label(Bill_Length_o = \"Bill Length\", Bill_Depth_o = \"Bill Depth\", Bill_Length_s = \"Bill Length\", Bill_Depth_s = \"Bill Depth\")\n    .tab_options(table_font_size = 16)\n)\n\n\n\n\n\nTo keep things simple, we will just subset the penguins dataset and keep only bill length and depth for our X dataframe. For y, we’ll grab the species as a pandas series.\nThe next step is to create the scaler object and use fit_transform to both fit the scaler to X and return the transformed X matrix. One of the nice things about using a scaler object rather than a function is that the object saves the scaling parameters so we can reuse them later.\nThis table here just shows how the mean and standard deviation change between the original and scaled features. It was made with great_tables and the code is included below."
  },
  {
    "objectID": "session4/session4v2_slides.html#understanding-the-kmeans-model-class",
    "href": "session4/session4v2_slides.html#understanding-the-kmeans-model-class",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Understanding the KMeans model class",
    "text": "Understanding the KMeans model class\n\nclass sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, \ntol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')\nParameters: Set by user at time of instantiation\n- n_clusters, max_iter, algorithm \nAttributes: Store object data\n- cluster_centers_: stores coordinates of cluster centers\n- labels_: stores labels of each point - n_iter_: number of iterations run (will be changed during method run)\n- n_features_in and feature_names_in_: store info about features seen during fit \nMethods: Define object behaviors\n- fit(X): fits model to data X - predict(X): predicts closest cluster each sample in X belongs to\n- transform(X): transforms X to cluster-distance space\n\n\nNow that we have our scaled data, the next step is to create and fit the kmeans model. Like the standardScaler class earlier, we have parameters set by the user. This is where we would specify the number of clusters we want, etc.\nThere are also attributes that store information like the location of cluster centers and the cluster labels associated with each point. These attributes are set by the fit method and wont exist for a model that has not been fit yet.\nAnd we have the fit and tranasform methods like the scaler class. We also get a predict method that lets us predict cluster labels for any new input data in the same cluster-space."
  },
  {
    "objectID": "session4/session4v2_slides.html#use-function-to-calculate-ari",
    "href": "session4/session4v2_slides.html#use-function-to-calculate-ari",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Use function to calculate ARI",
    "text": "Use function to calculate ARI\n\nTo check how good our model is, we can use one of the functions included in the sklearn library.\nThe adjusted_rand_score() function evaluates how well the cluster groupings agree with the species groupings while adjusting for chance.\n\n# Calculate clustering performance using Adjusted Rand Index (ARI)\nkmeans_ari = adjusted_rand_score(penguins['species'], penguins[\"kmeans_cluster\"])\nprint(f\"k-Means Adjusted Rand Index: {kmeans_ari:.2f}\")\n\n\n\nk-Means Adjusted Rand Index: 0.82\n\n\n\n\nSo far, we’ve mostly used methods in our analysis, but for things that are not model-specific, functions can be useful! We can use the adjusted_rand_score() function to evaluate how well the cluster groupings agree with the species groupings.\nSince this function just needs the ‘true’ labels and the cluster labels, with no actual model information, it makes sense for it to be a function and not a method."
  },
  {
    "objectID": "session4/session4v2_slides.html#scatter-data-heatmap-data",
    "href": "session4/session4v2_slides.html#scatter-data-heatmap-data",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Scatter data & Heatmap Data",
    "text": "Scatter data & Heatmap Data\n\ndisplay(scatter_data.head(3))\n\n\n\n\n\n\n\n\nspecies\nkmeans_cluster\nsex\ncount\nx_jittered\n\n\n\n\n0\nAdelie\n0\nFemale\n73\n0.1\n\n\n1\nAdelie\n0\nMale\n69\n-0.1\n\n\n2\nAdelie\n2\nMale\n4\n-0.1\n\n\n\n\n\n\n\n\ndisplay(heatmap_data)\n\n\n\n\n\n\n\nspecies\nAdelie\nChinstrap\nGentoo\n\n\nkmeans_cluster\n\n\n\n\n\n\n\n0\n142\n5\n1\n\n\n1\n0\n9\n112\n\n\n2\n4\n54\n6\n\n\n\n\n\n\n\n\nAfter the transformations we performed, this is what our scatter data and heatmap data look like."
  },
  {
    "objectID": "session4/session4v2_slides.html#creating-plots",
    "href": "session4/session4v2_slides.html#creating-plots",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Creating Plots",
    "text": "Creating Plots\n\n# Prepare the figure with 2 subplots; the axes object will contain both plots\nfig2, axes = plt.subplots(1, 2, figsize=(16, 7)) ## 1 row 2 columns\n\n# Plot heatmap on the first axis\nsns.heatmap(data = heatmap_data, cmap=\"Blues\", linewidths=0.5, linecolor='white', annot=True, \nfmt='d', ax=axes[0]) ## fmt='d' = decimal (base10) integer, use fmt='f' for floats \naxes[0].set_title(\"Heatmap of KMeans Clustering by Species\")\naxes[0].set_xlabel(\"Species\")\naxes[0].set_ylabel(\"KMeans Cluster\")\n\n# Scatterplot with jitter\nsns.scatterplot(data=scatter_data, x=\"x_jittered\", y=\"kmeans_cluster\",\n    hue=\"species\", style=\"sex\", size=\"count\", sizes=(100, 500),\n    alpha=0.8, ax=axes[1], legend=\"brief\")\naxes[1].set_xticks(range(len(species_order)))\naxes[1].set_xticklabels(species_order)\naxes[1].set_title(\"Cluster Assignment by Species and Sex (Jittered)\")\naxes[1].set_ylabel(\"KMeans Cluster\")\naxes[1].set_xlabel(\"Species\")\naxes[1].set_yticks([0, 1, 2])\naxes[1].legend(bbox_to_anchor=(1.05, 0.5), loc='center left', borderaxespad=0.0, title=\"Legend\")\n\nfig2.tight_layout()\n#fig2.show()\n\n\n\nNow we can create our plots on this new figure, fig2. Because we want to create more than 1 plot, we have additional arguments in plt.subplots. 1, 2 means 1 row 2 columns.\nThis time, our ‘axis’ is actually a 1d array of axis objects. If we had more than 1 row, it would be a 2d array (like a matrix). -adv- If we want to plot the heatmap on the first ‘ax’, we can set ax=axes[0]. Here, we don’t have to explicitly set X and Y, we can just pass the data into the heatmap function.\n-adv- For our scatterplot, we need to set x and y explicitly, using the x_jittered column so our points don’t overlap. To make things clean here, we can set the xticks using the species_order we defined previously and put our legend to the left of the plot.\nNote: bbox_to_anchor -&gt; x coord, y coord in terms of plot size. Aka a little more than 1 plot to the left and half a plot up."
  },
  {
    "objectID": "session4/session4v2_slides.html#creating-plots-output",
    "href": "session4/session4v2_slides.html#creating-plots-output",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Creating Plots",
    "text": "Creating Plots"
  },
  {
    "objectID": "session4/session4v2_slides.html#project-2-knn-classification",
    "href": "session4/session4v2_slides.html#project-2-knn-classification",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Project 2: KNN classification",
    "text": "Project 2: KNN classification\n\nFor our KNN classification, the model is supervised (meaning it is dependent on the outcome ‘y’ data). This time, we need to split our data into a training and test set. \n\n\nThe function train_test_split() from scikit-learn is helpful here!\n\n# Splitting dataset into training and testing sets (still using scaled X!)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n\n\n\n\nUnlike R functions, which return a single object (often a list when multiple outputs are needed), Python functions can return multiple values as a tuple—letting you unpack them directly into separate variables.\n\n\nNow we can move on to our KNN classification. Because this model is supervised, it is dependent on the species data. To have a meaningful model, we need to split our data into a training and test set, which we can do using the function train_test_split(). We are still using the scaled X from before, but we want to reserve 30% of our data for testing.\nWe can assign the outputs directly to different variables.\n(help(train_test_split) will show what the output order is)"
  },
  {
    "objectID": "session4/session4v2_slides.html#understanding-kneighborsclassifier-class",
    "href": "session4/session4v2_slides.html#understanding-kneighborsclassifier-class",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Understanding KNeighborsClassifier class",
    "text": "Understanding KNeighborsClassifier class\nclass sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', \nalgorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n\nParameters: Set by user at time of instantiation\n- n_neigbors, weights, algorithm, etc. \nAttributes: Store object data\n- classes_: class labels known to the classifier\n- effective_metric_: distance metric used\n- effective_metric_params_: parameters for the metric function\n- n_features_in and feature_names_in_: store info about features seen during fit\n- n_samples_fit_: number of samples in fitted data \nMethods: Define object behaviors\n- .fit(X, y): fit knn classifier from training dataset (X and y)\n- .predict(X): predict class labels for provided data X\n- .predict_proba(X): return probability estimates for test data X\n- .score(X, y): return mean accuracy on given test data X and labels y\n\nThe KNeighbors classifier class is similar to the kmeans class, with a notable difference. The fit method here requires both X and y. Technically, the fit method in kmeans can accept a y value, it just won’t use it. This is done to allow for interchangable models in pipelines.\nThe neighbors classifier also lacks the transform method because it doesn’t create a ‘space’ like the kmeans cluster-distance space. Instead, it has the .predict_proba method, which gives class probability estimates for input data."
  },
  {
    "objectID": "session4/session4v2_slides.html#making-an-instance-of-kneighborsclassifier-and-fitting-to-training-data",
    "href": "session4/session4v2_slides.html#making-an-instance-of-kneighborsclassifier-and-fitting-to-training-data",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Making an instance of KNeighborsClassifier and fitting to training data",
    "text": "Making an instance of KNeighborsClassifier and fitting to training data\n\nFor a supervised model, y_train is included in .fit()!\n\n\n## perform knn classification\n# Applying k-NN classification with 5 neighbors\nknn = KNeighborsClassifier(n_neighbors=5) ## make an instance of the KNeighborsClassifier class\n# and set the n_neighbors parameter to be 5. \n\n# Use the fit method to fit the model to the training data\nknn.fit(X_train, y_train)\nknn\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KNeighborsClassifier?Documentation for KNeighborsClassifieriFittedKNeighborsClassifier() \n\n\n\nFitting the knn model works the same as fitting the kmeans model, except we have to supply the ‘y’ values."
  },
  {
    "objectID": "session4/session4v2_slides.html#once-the-model-is-fit",
    "href": "session4/session4v2_slides.html#once-the-model-is-fit",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Once the model is fit…",
    "text": "Once the model is fit…\n-We can look at its attributes (ex: .classes_) which gives the class labels as known to the classifier\n\nprint(knn.classes_)\n\n\n\n['Adelie' 'Chinstrap' 'Gentoo']\n\n\n\n-And use fitted model to predict species for test data\n\n# Use the predict method on the test data to get the predictions for the test data\ny_pred = knn.predict(X_test)\n\n# Also can take a look at the prediction probabilities, \n# and use the .classes_ attribute to put the column labels in the right order\nprobs = pd.DataFrame(\n    knn.predict_proba(X_test),\n    columns = knn.classes_)\nprobs['y_pred'] = y_pred\n\nprint(\"Predicted probabilities: \\n\", probs.head())\n\n\n\nPredicted probabilities: \n    Adelie  Chinstrap  Gentoo     y_pred\n0     1.0        0.0     0.0     Adelie\n1     0.0        0.0     1.0     Gentoo\n2     1.0        0.0     0.0     Adelie\n3     0.0        0.6     0.4  Chinstrap\n4     1.0        0.0     0.0     Adelie\n\n\n\nAfter fitting, we can see that the classifier has learned the labels from the dataset, and we are able to use it to predict species labels for the test data.\nWe use the .predict method to get the predicted classes and the .predict proba method to get the actual probabilities. We can store both of these in a pandas dataframe, which we can use to make a scatterplot to compare the actual and predicted classes."
  },
  {
    "objectID": "session4/session4v2_slides.html#scatterplot-for-k-nn-classification-of-test-data",
    "href": "session4/session4v2_slides.html#scatterplot-for-k-nn-classification-of-test-data",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Scatterplot for k-NN classification of test data",
    "text": "Scatterplot for k-NN classification of test data\n\n\nCreate dataframe of unscaled X_test, bill_length_mm, and bill_depth_mm.\nAdd to it the actual and predicted species labels\n\n\n## First unscale the test data\nX_test_unscaled = scaler.inverse_transform(X_test)\n\n## create dataframe \npenguins_test = pd.DataFrame(\n    X_test_unscaled,\n    columns=['bill_length_mm', 'bill_depth_mm']\n)\n\n## add actual and predicted species \npenguins_test['y_actual'] = y_test.values\npenguins_test['y_pred'] = y_pred\npenguins_test['correct'] = penguins_test['y_actual'] == penguins_test['y_pred']\n\nprint(\"Results: \\n\", penguins_test.head())\n\n\n\nResults: \n    bill_length_mm  bill_depth_mm   y_actual     y_pred  correct\n0            39.5           16.7     Adelie     Adelie     True\n1            46.9           14.6     Gentoo     Gentoo     True\n2            42.1           19.1     Adelie     Adelie     True\n3            49.8           17.3  Chinstrap  Chinstrap     True\n4            41.1           18.2     Adelie     Adelie     True\n\n\n\nUsing the scaler object we created previously, we can un-scale the test data and create a new dataframe that contains the unscaled predictors as well as the true and predicted species.\nLine 13 here creates a boolean (true/false) column for agreement between the actual and predicted species."
  },
  {
    "objectID": "session4/session4v2_slides.html#plotnine-scatterplot-for-k-nn-classification-of-test-data",
    "href": "session4/session4v2_slides.html#plotnine-scatterplot-for-k-nn-classification-of-test-data",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Plotnine scatterplot for k-NN classification of test data",
    "text": "Plotnine scatterplot for k-NN classification of test data\nTo see how well our model did at classifying the remaining penguins…\n\n## Build the plot\nplot3 = (ggplot(penguins_test, aes(x=\"bill_length_mm\", y=\"bill_depth_mm\", \ncolor=\"y_actual\", fill = 'y_pred', shape = 'correct'))\n + geom_point(size=4, stroke=1.1)  # Stroke controls outline thickness\n + scale_shape_manual(values={True: 'o', False: '^'})  # Circle and triangle\n + ggtitle(\"k-NN Classification Results\")\n + theme_bw())\n\ndisplay(plot3)\n\n\n\nNow we can put all of this together into a plot. This plot is overely complicated, but I wanted to use it to show some more of the plot options in plotnine."
  },
  {
    "objectID": "session4/session4v2_slides.html#plotnine-scatterplot-for-k-nn-classification-of-test-data-output",
    "href": "session4/session4v2_slides.html#plotnine-scatterplot-for-k-nn-classification-of-test-data-output",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Plotnine scatterplot for k-NN classification of test data",
    "text": "Plotnine scatterplot for k-NN classification of test data"
  },
  {
    "objectID": "session4/session4v2_slides.html#visualizing-decision-boundary-with-seaborn-and-matplotlib",
    "href": "session4/session4v2_slides.html#visualizing-decision-boundary-with-seaborn-and-matplotlib",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Visualizing Decision Boundary with seaborn and matplotlib",
    "text": "Visualizing Decision Boundary with seaborn and matplotlib\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create and fit label encoder for y (just makes y numeric because it makes the scatter plot happy)\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Create the plot objects\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Create display object\ndisp = DecisionBoundaryDisplay.from_estimator(\n    knn,\n    X_test,\n    response_method = 'predict',\n    plot_method = 'pcolormesh',\n    xlabel = \"bill_length_scaled\",\n    ylabel = \"bill_depth_scaled\",\n    shading = 'auto',\n    alpha = 0.5,\n    ax = ax\n)\n\n# Use method from display object to create scatter plot\nscatter = disp.ax_.scatter(X_scaled[:,0], X_scaled[:,1], c=y_encoded, edgecolors = 'k')\ndisp.ax_.legend(scatter.legend_elements()[0], knn.classes_, loc = 'lower left', title = 'Species')\n_ = disp.ax_.set_title(\"Penguin Classification\")\n\nfig.show()\n\n\n\nTo visualize the decision boundary of the modelm we can use another sklearn class called DecisionBoundaryDisplay, which creates a display object that has matplotlib-style plotting methods. We assign our axis in the creation of our display object and use the attached .ax_. methods to create the scatter plot and legend as well as set the title.\nBecause we’re using this display object, we have to explicitly call fig.show() here to display the figure."
  },
  {
    "objectID": "session4/session4v2_slides.html#visualizing-decision-boundary-with-seaborn-and-matplotlib-output",
    "href": "session4/session4v2_slides.html#visualizing-decision-boundary-with-seaborn-and-matplotlib-output",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Visualizing Decision Boundary with seaborn and matplotlib",
    "text": "Visualizing Decision Boundary with seaborn and matplotlib"
  },
  {
    "objectID": "session4/session4v2_slides.html#evaluate-knn-performance",
    "href": "session4/session4v2_slides.html#evaluate-knn-performance",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Evaluate KNN performance",
    "text": "Evaluate KNN performance\n\nTo check the performance of our KNN classifier, we can check the accuracy score and print a classification report.\n- accuracy_score and classification_report are both functions!\n- They are not unique to scikit-learn classes so it makes sense for them to be functions not methods\n\n\n## eval knn performance\nknn_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"k-NN Accuracy: {knn_accuracy:.2f}\")\nprint(\"Classification Report: \\n\", classification_report(y_test, y_pred))\n\n\n\nk-NN Accuracy: 0.94\nClassification Report: \n               precision    recall  f1-score   support\n\n      Adelie       0.98      0.98      0.98        48\n   Chinstrap       0.80      0.89      0.84        18\n      Gentoo       0.97      0.91      0.94        34\n\n    accuracy                           0.94       100\n   macro avg       0.92      0.93      0.92       100\nweighted avg       0.94      0.94      0.94       100\n\n\n\n\nFinally, we can use the ‘accuracy_score’ and classification_report functions to check the performance of our classifier."
  },
  {
    "objectID": "session4/session4v2_slides.html#make-a-summary-table-of-metrics-for-both-models",
    "href": "session4/session4v2_slides.html#make-a-summary-table-of-metrics-for-both-models",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Make a Summary Table of Metrics for Both Models",
    "text": "Make a Summary Table of Metrics for Both Models\n\nsummary_table = pd.DataFrame({\n    \"Metric\": [\"k-Means Adjusted Rand Index\", \"k-NN Accuracy\"],\n    \"Value\": [kmeans_ari, knn_accuracy]\n})\n(\n    GT(summary_table)\n    .tab_header(title = \"Model Results Summary\")\n    .fmt_number(columns = \"Value\", n_sigfig = 2)\n    .tab_options(table_font_size = 20)\n)\n\n\n\n\n\n\n\n\n\nModel Results Summary\n\n\nMetric\nValue\n\n\n\n\nk-Means Adjusted Rand Index\n0.82\n\n\nk-NN Accuracy\n0.94\n\n\n\n\n\n\n        \n\n\n\nUsing the great_tables package, we can make a nice little summary table of our results from both mini-projects."
  },
  {
    "objectID": "session4/session4v2_slides.html#key-takeaways-from-this-session",
    "href": "session4/session4v2_slides.html#key-takeaways-from-this-session",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Key Takeaways from This Session",
    "text": "Key Takeaways from This Session\n\n\n\n\n\nPython workflows rely on object-oriented structures in addition to functions: Understanding the OOP paradigm makes Python a lot easier!\n\n\nEverything is an object!\n\n\nDuck Typing: If an object has a method, that method can be called regardless of the object type. Caveat being, make sure the arguments (if any) in the method are specified correctly for all objects!\n\n\nPython packages use common methods that make it easy to change between model types without changing a lot of code."
  },
  {
    "objectID": "session4/session4v2_slides.html#additional-insights",
    "href": "session4/session4v2_slides.html#additional-insights",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Additional Insights",
    "text": "Additional Insights\n\n\n\nPredictable APIs enable seamless model switching: Swapping models like LogisticRegression → RandomForestClassifier usually requires minimal code changes.\n\n\nscikit-learn prioritizes interoperability: Its consistent class design integrates with tools like Pipeline, GridSearchCV, and cross_val_score.\n\n\nClass attributes improve model transparency: Access attributes like .coef_, .classes_, and .feature_importances_ for model interpretation and debugging.\n\n\nCustom classes are central to deep learning: Frameworks like PyTorch and TensorFlow require you to define your own model classes by subclassing base models.\n\n\nMixins support modular design: Mixins (e.g., ClassifierMixin) let you add specific functionality without duplicating code."
  },
  {
    "objectID": "session4/session4v2_slides.html#homework",
    "href": "session4/session4v2_slides.html#homework",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "‘Homework’",
    "text": "‘Homework’\n\nThe homework for this session can be found at “H:Exercises_homework_blank.ipynb”  Please copy the file! Do not modify the file on the H drive!\nAlso on the H drive, under the ‘Solutions’ subfolder, are an html file and a jupyter notebook with sample solutions for the homework."
  },
  {
    "objectID": "session4/session4v2_slides.html#pre-reading-for-this-session",
    "href": "session4/session4v2_slides.html#pre-reading-for-this-session",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Pre-Reading for This Session",
    "text": "Pre-Reading for This Session\n\n\nScikit-learn Documentation\n\nIntroduction to OOP in Python (Real Python)\n\nPlotnine Reference\nSeaborn Reference"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Python Workshops",
    "section": "",
    "text": "This workshop series is geared toward R and SAS users who are interested in exploring Python–a powerful, versatile programming language that excels in many areas such as machine learning, large-scale data processing, and broader data science applications.\nOur goal is to help you build a solid foundation in Python and gain skills that can be integrated into your own work. This series will guide you through installing and setting up Python, understanding basic Python data structures, data manipulation through pandas, and, finally, applying machine learning methods using libraries such as scikit-learn.\nAll workshop materials, including pre-session handouts, coding demos, assignments, session slides, and video recordings, will be uploaded here. Be sure to check out the FAQ tab for common questions. If you have any questions, suggestions, or ideas for future sessions, please feel free to share them on our GitHub Discussions page. We value your feedback and aim to continually improve the workshops. We look forward to learning with you!"
  },
  {
    "objectID": "index.html#recordings",
    "href": "index.html#recordings",
    "title": "Introduction to Python Workshops",
    "section": "▶️Recordings",
    "text": "▶️Recordings\nRecordings will be uploaded after each session.\n\nSession 1 recording: Python Workshop Monday, April 14, 2025\nSession 2 recording: Python Workshop Wednesday, April 16, 2025\nSession 3 recording: Python Workshop Monday, April 21, 2025"
  },
  {
    "objectID": "index.html#useful-links",
    "href": "index.html#useful-links",
    "title": "Introduction to Python Workshops",
    "section": "🔗Useful Links",
    "text": "🔗Useful Links\n\nTutorials and Handouts\n\nSession 1 Tutorial: Installing Python and Essential Tools\nSession 2: Intro to Python Data Structures \nSession 3: Intro to Pandas \nSession 4: Object-Oriented Programming and Intro to ML Libraries - Extended tutorial\n\n\n\nDownloads\n \n\nInstall Miniconda\n\n \n\nInstall VS Code\n\n \n\nDownload environment.yml file"
  },
  {
    "objectID": "sessions.html",
    "href": "sessions.html",
    "title": "Sessions",
    "section": "",
    "text": "Session links\nTutorial: Get Started with Python \nGo to Session2a: Intro to Python Data Structures \nGo to Session2b: Intro to Pandas \nGo to Object-oriented Programming Demo"
  },
  {
    "objectID": "session1/session1.html",
    "href": "session1/session1.html",
    "title": "Intro to Python",
    "section": "",
    "text": "The overall goal of the is to learn how to program in Python using modern, reproducible tools.\n\nSession 1 will help you set up a flexible, interactive, working environment for Python programming (Miniconda, VS Code IDE, Jupyter Notebook, etc.)\nSession 2 and 3 will focus on the Python basics such as data structure, list comprehensive, and functions. We will also learn data frame manipulations with pandas.\nSession 4 will introduce you to object-oriented programming (OOP) with examples from the machine learning library scikit-learn."
  },
  {
    "objectID": "session1/session1.html#workshop-at-a-glance",
    "href": "session1/session1.html#workshop-at-a-glance",
    "title": "Intro to Python",
    "section": "",
    "text": "The overall goal of the is to learn how to program in Python using modern, reproducible tools.\n\nSession 1 will help you set up a flexible, interactive, working environment for Python programming (Miniconda, VS Code IDE, Jupyter Notebook, etc.)\nSession 2 and 3 will focus on the Python basics such as data structure, list comprehensive, and functions. We will also learn data frame manipulations with pandas.\nSession 4 will introduce you to object-oriented programming (OOP) with examples from the machine learning library scikit-learn."
  },
  {
    "objectID": "session1/session1.html#what-youll-need-for-the-workshop",
    "href": "session1/session1.html#what-youll-need-for-the-workshop",
    "title": "Intro to Python",
    "section": "❗What You’ll Need for the Workshop",
    "text": "❗What You’ll Need for the Workshop\n\nBring your laptop (Windows/MacOS)\nBasic GitHub knowledge & MSK GitHub Enterprise account–The workshop website is hosted on MSK Enterprise GitHub, which might require you logging in with MSK credentials to access links/download files. Check out Biostatistics Resource Guide for GitHub-related training.\nInstall Miniconda and Visual Studio Code before session 1 (see this guide)\n\n\n\n\n\n\nImportant\n\n\n\nWhen following the installation instructions in this article, we recommend that you install software on your MSK laptop or workstation. Downloading/installing software files on VDI is extremely slow and might not install at all if the software is too large."
  },
  {
    "objectID": "session1/session1.html#about-this-guide",
    "href": "session1/session1.html#about-this-guide",
    "title": "Intro to Python",
    "section": "📖 About this Guide",
    "text": "📖 About this Guide\nThis handout is a reading for the first session of the Introduction to Python workshop. It serves as a follow-along guide to help you install Python, set up essential tools like Miniconda and VS Code, and prepare for coding in Python for the upcoming sessions.\nAlong the way, we will also discuss some important questions–What makes Python useful? Why would we, as biostatisticians, want to learn it? You will get an overview of the key features of Python, what it can do in relation to biostatistics/bioinformatics research, and how it compared to R. In the upcoming sessions, we will dive deeper into some of Python’s features and functions through hands-on programming practices.\n\n\n\n\n\n\n📝Learning objectives of the article\n\n\n\nThis handout walks you through installing the necessary tools and setting up your Python environment. Please follow each step before the first workshop session.\n💡Aim 1: know key features of Python and its applications.\n💡Aim 2: know what Miniconda is and the steps to install Python through it.\n💡Aim 3: know what Python virtual environments are and how to create them with conda.\n💡Aim 4: know what integrated development environments (IDE) are and steps to set up an interactive Python coding environment in Visual Studio Code."
  },
  {
    "objectID": "session1/session1.html#what-is-python",
    "href": "session1/session1.html#what-is-python",
    "title": "Intro to Python",
    "section": "What is Python?",
    "text": "What is Python?\n\n\nPython is a high-level, interpreted, and general-purpose programming language first developed by Guido van Rossum in 1991.\n\nPython has gained much popularity in the past 20 years. Its user group has expanded into a large and active scientific computing and developer community that spans numerous academic and industrial fields. Nowadays, Python has a powerful ecosystem of external packages (libraries) for data science, artificial intelligence, and software development.\nPython is cross-platform and open-source. In Python, you can easily install packages with the built-in installer pip or package manager conda, just as you do with install.packages() in R."
  },
  {
    "objectID": "session1/session1.html#what-can-python-do",
    "href": "session1/session1.html#what-can-python-do",
    "title": "Intro to Python",
    "section": "What can Python do?",
    "text": "What can Python do?\nJust like R, Python is an open source and versatile programming language that allows users to perform a wide range of data analysis and computational tasks. While R is particularly useful in statistical analysis and visualizations, Python has been used in many distinct areas, such as:\n\nMachine Learning and Deep learning\nWeb Development\nScripting & Automation\nCloud Computing\nGame Development\nCybersecurity"
  },
  {
    "objectID": "session1/session1.html#why-learn-pythonas-biostatisticians",
    "href": "session1/session1.html#why-learn-pythonas-biostatisticians",
    "title": "Intro to Python",
    "section": "Why learn Python–as Biostatisticians?",
    "text": "Why learn Python–as Biostatisticians?\nWithin the field of biostatistics/ bioinformatics, Python has become a core tool for biomedical data analysis due to its versatility, reproducibility, and strong ecosystem of scientific libraries. Below are some areas where Python can be useful and some essential libraries.\n\nStatistical analysis\nWhile R is the go-to tool for statistical analysis, Python has caught up with many equivalent libraries and functions:\n\nstatsmodels/ scipy.stats provide regression modeling and hypothesis testing.\nlifelines/ scikit-survival support survival analysis and plotting.\n\nML/DL ecosystem\nPython dominates in machine learning and AI development:\n\nscikit-learn is a rich machine learning library that supports both supervised regression an dclassification (e.g., random forests, gradient boosting) and unsupervised clustering (e.g., K-means).\nTensorFlow, PyTorch are deep learning libraries widely used for computer vision and natural language processing.\noptuna, Ray can be integrated into ML/DL workflows for easy and efficient model training, hyperparameter tuning, fine-tuning, etc.\n\nOmics data analysis\nEmerging packages that provide standard omics data preprocessing and analysis pipelines allow Python to become increasingly popular in the field of bioinformatics:\n\nscanpy, anndata are libraries for single-cell RNA-seq data loading, preprocessing, and analysis.\nBiopython is a set of tools for biological computation that performs file parsering (BLAST, FASTA, GenBank, etc.), sequence analysis, clustering algorithms, etc.\npysam works with BAM/SAM/VCF files."
  },
  {
    "objectID": "session1/session1.html#python-vs.-r-differences",
    "href": "session1/session1.html#python-vs.-r-differences",
    "title": "Intro to Python",
    "section": "Python vs. R: Differences",
    "text": "Python vs. R: Differences\nWhile both programming languages are popular for data analysis and computation, Python and R differ in their underlying code structure, the scope of functionality, and the extensibility of tasks they can perform. Here is a non-exhaustive summary of some key differences:\n\n\n\n\n\n\n\n\n\nFeature/Task\nR\nPython\n\n\n\n\nProgramming logic\nMostly function-oriented\nFunction and object-oriented – structured around classes\n\n\nGeneral-purpose programming\n⚠️ Less ideal – designed mainly for working with data\n✅ Strong – ML & AI, software development, scripting, etc.\n\n\nComputational power\n✅ Vectorization allows operating on all elements of a vector at once  ✅ Best for statistical analysis  ⚠️ Memory-intensive; often slow for reading large data and performing large computations\n✅ Generally faster for loops  ✅ Strong support for GPU computing  ✅ Memory-efficient for handling large objects and complex computations\n\n\nPackage availability\n✅ Excellent for statistical analysis (glm, survival, ggplot2)  ⚠️ Good options for ML (caret, mlr3) but few DL packages  ✅ Great for omics-focused analysis (Bioconductor, ComplexHeatmap, Seurat)\n☑️ Improving on statistical packages (statsmodels, lifelines)  ✅ Best for ML/DL (scikit-learn, pytorch, keras)  ✅ Established packages specialized in processing large omics datasets (scanpy, scvi-tools)\n\n\nIDE & Reproducible environments (notebooks etc.)\n✅ RStudio  ✅ RMarkdown, Quarto\n✅ Visual Studio Code, JupyterLab, PyCharm, Spyder, etc.  ✅ Jupyter Notebooks, Quarto"
  },
  {
    "objectID": "session1/session1.html#essential-tools-for-python-programming",
    "href": "session1/session1.html#essential-tools-for-python-programming",
    "title": "Intro to Python",
    "section": "Essential Tools for Python Programming",
    "text": "Essential Tools for Python Programming\nTo get the most out of Python–especially for data science and reproducible research–it’s important to set up an integrated, flexible programming environment.\nHere is a list of tools we recommend using:\n\nConda: a powerful package and environment manager for Python.\nVisual Studio Code: a lightweight code editor that integrates programming + plots + terminal + etc.\nJupyter Notebook: an interactive computing tool that combines code execution, text documentation, and visualizations.\nGit (GitHub): for version control, collaboration, and publishing code.\n\n\nConda (via Miniconda)\nConda is a package manager, much like CRAN + Bioconductor, and can be utilized across languages (Python, R, C/C++ etc.). It also simplifies Python environment management, similar to renv in R but more powerful and flexible, ensuring dependency isolation without cluttering the global system.\nConda can be installed via either Miniconda (lightweight version) or the Anaconda Distribution (full version). We will use the former for the purpose of this workshop series.\n\n\nVisual Studio Code\nAkin to RStudio, Visual Studio Code (often called VS Code) is an IDE for multi-language coding (Python, R, Java, etc.). It has many features integrated within it, including interactive coding via Jupyter Notebook or Quarto, version control with Git, and built-in terminal and debugging tools.\nNext, we will walk through installing and setting up these tools."
  },
  {
    "objectID": "session1/session1.html#follow-along-install-miniconda-python-conda",
    "href": "session1/session1.html#follow-along-install-miniconda-python-conda",
    "title": "Intro to Python",
    "section": "▶️Follow-Along: Install Miniconda (Python + conda)",
    "text": "▶️Follow-Along: Install Miniconda (Python + conda)\nLet’s walk through steps to install Minconda.\n\nFor the latest Miniconda installers for Python 3.12, navigate to the Anaconda website.\nDownload the 64-bit graphical installer according to your system (Windows or MacOS):\nNote: Make sure you are downloading from the Miniconda Installers section, not Anaconda!\n\nAlternatively, check out the Quick command line installation guide to install Miniconda through command line interface.\n\nRun the installer (.exe for Windows / .pkg for MacOS)\n\nselect Just Me for installation type – recommended; doesn’t require admin rights.\n\n\n\n\n\n\n\nInstalling for current user only\n\n\n\nYou don’t need to install for all users most of the time. This option requires admin privileges which you might not have on your MSK laptop.\n\n\nKeep the default for installation location. E.g.,\n\nWindows: C:\\Users\\&lt;user_name&gt;\\AppData\\Local\\miniconda3\nMacOS: /Users/yourname/miniconda3\n\nCustomize the advanced installation options:\n\n❌ Add Miniconda to my PATH environment variable – NOT recommended\n✅ Register Miniconda3 as my default Python 3.12\n\n\n\n\n\n\n\n\n\nDo not add Miniconda to PATH⚠️\n\n\n\nIt is recommended that you do not add Miniconda to system’s PATH environment variable, as it might lead to conflicts with your other Python installations or accidentally break software using the system Python.\nInstead, you could later run conda init in Anaconda Prompt to configure the terminal shells (like PowerShell or Command Prompt) to recognize the conda command.\n\n\nComplete installation. This might take a few minutes to complete.\nCheck installation–verify that Python and Conda are successfully installed.\n\nWindows:\n\nOpen the Start Menu and run Anaconda Prompt.\nType the following command.\nconda --version\npython --version\nYou should see the current versions of your Python and Conda being returned–such as conda 24.9.2 and Python 3.12.4 (the exact numbers might differ). This means that Miniconda is properly installed and initialized.\n\nMacOS:\n\nOpen Terminal. Configure your shell to make the conda command available.\nsource ~/miniconda3/bin/activate\nconda init zsh # or conda init bash if you are using bash\nThen restart your Terminal and type:\nconda --version\npython --version\nYou should see the current versions of your Python and Conda being returned, which means everything has been correctly installed."
  },
  {
    "objectID": "session1/session1.html#what-is-conda",
    "href": "session1/session1.html#what-is-conda",
    "title": "Intro to Python",
    "section": "What is Conda?",
    "text": "What is Conda?\nConda is a cross-platform command line tool for managing packages and environment via the conda command line interface (CLI). It can handle both Python and non-Python dependencies (R, C, system binaries, etc.), making it particularly powerful.\nYou can install conda via installers such as Miniconda or the Anaconda Distribution."
  },
  {
    "objectID": "session1/session1.html#conda-for-managing-packages",
    "href": "session1/session1.html#conda-for-managing-packages",
    "title": "Intro to Python",
    "section": "Conda for Managing Packages",
    "text": "Conda for Managing Packages\nConda installs packages from channels(repositories), such as the default Anaconda channel or community-maintained channels like conda-forge.\nTo install packages from the default Anaconda repository:\n# Install a single package\nconda install scipy\n\n# Install a specific version of a package\nconda install scipy=0.15.0 \n\n# Install multiple packages\nconda install scipy=0.15.0 pandas matplotlib\nIf the package is located in another channel, such as conda-forge, you can manually specify the channel when installing the package. For example:\nconda install conda-forge::pytorch \n# or \nconda install pytorch --channel conda-forge\nTo update a package:\nconda update scipy\nNote that this automatically updates the package to the highest version supported by the current Python series. For example, Python 3.9 updates to the highest available in the 3.x series.\nTo remove a package (or multiple packages at once):\nconda remove scipy pandas matplotlib\n\nConda vs. Pip\nIf a Python package is not available through any conda channel, consider using the pip package manager:\npip install\n\n\n\n\n\n\nDifference Between conda install and pip install\n\n\n\nLong story short: Pip installs Python libraries only, while conda can install both Python and non-Python packages (e.g., R, C/C++, system binaries).\nIt is generally recommended that you only use conda install within a conda environment, as anything installed via pip won’t be recognized by conda and vice versa. Using the two interchangeably might overwrite or break packages and mess up the environment.\nWhat if the Python package is unavailable through conda?\nThe best practice is to install everything with conda first, then use pip only when the package is not available in conda.\nCheck out this blog for more information on using pip in a conda environment."
  },
  {
    "objectID": "session1/session1.html#what-is-a-virtual-environment",
    "href": "session1/session1.html#what-is-a-virtual-environment",
    "title": "Intro to Python",
    "section": "What is a Virtual Environment?",
    "text": "What is a Virtual Environment?\nA virtual environment is an isolated, self-contained workspace that includes its own language interpreter and package dependencies. Each environment operates independently, ensuring that projects are isolated from one another and from the system’s global setup.\nIn the previous section, we installed Python 3.12 via Miniconda and set it as the default (global) Python. However, you might need a different version of Python–say, Python 3.8–or a different set of packages for a particular project. In this case, creating a virtual environment allows you to maintain a completely separate Python setup, including its own Python version and /site-packages folder.\nYou can create as many environments as needed — ideal for managing multiple projects with different requirements."
  },
  {
    "objectID": "session1/session1.html#why-use-virtual-environments",
    "href": "session1/session1.html#why-use-virtual-environments",
    "title": "Intro to Python",
    "section": "Why Use Virtual Environments?",
    "text": "Why Use Virtual Environments?\nYou may find the flexibility of environments beneficial in many cases.\n\nAvoid Conflicts. Creating virtual environments can help resolve potential conflicts between different projects that might require different Python version or conflicting dependencies. Changes made to one environment won’t affect other projects that use different environments.\nEasy Management. When your work is temporary or that you simply want to experiment things without having to worry about breaking things, you can work within a virtual environment and later delete it when needed.\nSharing Environment. You can share your Python environment and whole list of dependencies with other people through a copy of the .yml file.\nReproducibility. They work as time capsules, allowing you to come back to an older project at any time later by recreating the virtual environment."
  },
  {
    "objectID": "session1/session1.html#follow-along-create-a-conda-virtual-environment",
    "href": "session1/session1.html#follow-along-create-a-conda-virtual-environment",
    "title": "Intro to Python",
    "section": "▶️Follow-Along: Create a Conda Virtual Environment",
    "text": "▶️Follow-Along: Create a Conda Virtual Environment\nUsing conda, we can create, activate, update, export, and remove virtual environments, each with its own Python version and set of packages. Let’s practice creating a virtual environment and installing packages into it using the command line.\n\n\n\n\n\n\n📌Prerequisite\n\n\n\nBe sure you have installed Miniconda by following the previous tutorial to access the conda command line interface.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Anaconda Navigator is an alternative to creating conda environments without the terminal skills. However, for users comfortable using the command line tool, we recommend the conda approach for better speed, control, and stability,\n\n\n\nOpen Anaconda Prompt (Windows) or Terminal (MacOS)\nCreate the virtual environment. Replace &lt;env-name&gt; with the name you want to give your environment.\nconda create --name &lt;env-name&gt;\nNote: you can use -n (shorthand) and --name interchangeably.\nTo create an environment with a specific Python version:\nconda create -n &lt;env-name&gt; python=3.10\nTo create an environment with a specific package(s):\nconda create -n &lt;env-name&gt; python=3.10 scipy pandas matplotlib\nor install packages later with separate commands:\nconda create -n &lt;env-name&gt; python=3.10\nconda install -n &lt;env-name&gt; scipy pandas matplotlib\nYou can also install packages from channels other than the defaults (your can pass multiple channels for the package search):\nconda install -n &lt;env-name&gt; scipy --channel conda-forge --channel bioconda\nor explicitly specify the channel from which you want the package to be installed:\n conda install -n &lt;env-name&gt; conda-forge::scipy\nNow, activate your environment.\nconda activate &lt;env-name&gt;\nYou can verify that your installation was successful by looking up the list of all current environments on your computer.\nconda env list\nThe default location for the installed conda environments (except for the base conda environment) is ..\\miniconda3\\envs\\&lt;env-name&gt;\nDeactivate the conda environment.\nconda deactivate\n\n\n\n\n\n\nAvoid activating on top of another virtual environment!\n\n\n\nAlways conda deactivate first before activating another one because environments can be stacked. This can lead to chaos in the packages in both environments.\n💡Tip: make sure you see (base) at the beginning of the terminal prompt when you are about to activate an environment.\n\n\nRemoving an environment.\n\nRemove by environment name:\nconda env remove -n &lt;env-name&gt;\nRemove by environment folder path:\nconda env remove --prefix &lt;/path/to/your/env&gt;\n\n\n\n👉Create an Environment from an environment.yml File\n\nYou can also create a virtual environment from a .yml configuration file.\nconda env create -f environment.yml\nExample: An environment file contains information about the environment name, channels, and dependencies:\nname: python310\nchannels:\n  - defaults\ndependencies:\n  - python=3.10\n  - pandas\n  - numpy\n\n\n\n\n\n\nNote\n\n\n\nDownload the .yml file for this Python workshop series here. This file includes the Python version and required channels and dependencies for completing the workshop sessions.\n\n\nThen activate the new environment:\nconda activate python310\nThis way, we can skip the cumbersome steps to set up an environment from scratch and easily recreate an environment shared by others or share our environment settings with others.\n\n\nSummary: A list of useful conda commands for managing environments.\n\n\n\n\n\n\nTask\nCommand\n\n\n\n\nCreate an environment\nconda create --name &lt;env-name&gt;\n\n\nList all environments\nconda env list\n\n\nRemove an environment\nconda remove --name &lt;env-name&gt; --all\n\n\nList packages in current environment\nconda list\n\n\nExport environment to .yml\nconda env export &gt; environment.yml\n\n\nRecreate environment from .yml file\nconda env create -f environment.yml"
  },
  {
    "objectID": "session1/session1.html#visual-studio-code-1",
    "href": "session1/session1.html#visual-studio-code-1",
    "title": "Intro to Python",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\n\n\nVisual Studio Code (VS Code) is one of the most popular open-source code editors with many features.\n\n\nMulti-Language Programming. VS Code supports multiple programming languages including Python, R, C/C++, JavaScript, etc.\nIntegrated Git Source Control. VS Code automatically recognizes and uses the computer’s Git installation. You can easily track changes, stage, and commit changes to your working branch.\nVariety of Project Development Support. You can add extra features such as language packs, debugging tools, Git/Github features, and remote server connector by installing extensions from the Extension Marketplace."
  },
  {
    "objectID": "session1/session1.html#follow-along-set-up-jupyter-notebook-in-vs-code",
    "href": "session1/session1.html#follow-along-set-up-jupyter-notebook-in-vs-code",
    "title": "Intro to Python",
    "section": "▶️Follow-Along: Set up Jupyter Notebook in VS Code",
    "text": "▶️Follow-Along: Set up Jupyter Notebook in VS Code\nWe’ll now walk through setting up your Python coding environment in VS Code with full support for virtual environments and Jupyter notebooks.\n\n\n\n\n\n\n💡Prerequisites\n\n\n\nEnsure you have the following:\n\nMiniconda: see ▶️Follow-Along: Install Miniconda (Python + conda)\nVS Code: Download and install VS Code\n\n\n\n\nStep 1: Install Required VS Code Extensions\n\nLaunch VS Code.\nOpen the Extensions panel from the left toolbar (or Ctrl+Shift+X on Windows/ Cmd+Shift+X on Mac).\n\nInstall the following extensions:\n\nPython: To support Python language, debugging, documentations, etc.\nJupyter: To support rendering Python documents from Jupyter Notebooks or Quarto files.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou still need to install Python (Miniconda) to your computer for these extensions to fully function.\n\n\n\n\n\nStep 2: Create a Conda Virtual Environment\nYou can create an environment in one step using the shared environment.yml file.\nconda env create -f environment.yml\nIf you want to create your own environment or use existing ones, install these necessary packages to allow your Python to work with Jupyter Notebooks in VS Code:\n\njupyter\nipykernel\npyyaml\n\nconda install jupyter ipykernel pyyaml -c conda-forge\n\n\nStep 3: Configure the Environment in VS Code.\n\nOpen the Command Palette (Ctrl+Shift+P on Windows / Cmd+Shift+X on Mac).\nSearch for and select “Python: Select Interpreter”.\nChoose your conda environment (e.g., python-intro-env).\n\nIf it doesn’t pop up, click Enter interpreter path…\nManually enter the path to your conda virtual environment Python executable:\n\nWindows: C:\\Users\\&lt;username&gt;\\AppData\\Local\\miniconda3\\envs\\&lt;env-name&gt;\\python.exe\nmacOS: Users/&lt;username&gt;/miniconda/envs/&lt;env-name&gt;/python\n\n\n\n\n\n\n\n\n🔎Find your conda Python executable path\n\n\n\nTo search for the conda Python interpreter location on your computer, open the Anaconda Prompt or terminal:\nconda activate &lt;env-name&gt;\nThen, locate your Python executable by typing the following:\n\nWindows: where python\nmacOS: which python\n\n\n\n\n\n\nStep 4: Open a Python Project and Create a Jupyter Notebook File\n\nIf you have an existing Python project you wish to work on in VS Code, you may open the project folder in VS Code.\n\nOpen from the VS Code Welcome page:\n\nOr by selecting File &gt; Open Folder (Ctrl+K Ctrl+O)\n\nTo create a new Jupyter Notebook file, go to File &gt; New File and select Jupyter Notebook (.ipynb).\nSelect the correct kernel (conda environment):\n\nIn the top-right corner of the notebook, you will see a Select Kernel button\nChoose the kernel that matches your conda environment (e.g., python-intro-env (Python 3.10.0))\n\nIf you don’t see the environment showing up:\n\nMake sure your conda env has the dependencies installed:\nconda install ipykernel jupyter pyyaml\nThen restart VS Code or reload the window (Ctrl+Shift+P &gt; Reload Window).\nIf all packages are installed but the issue persists, manually specify the Python path (see 🔎Find your conda Python executable path)\n\n\nYou are all set!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "FAQ.html",
    "href": "FAQ.html",
    "title": "FAQ",
    "section": "",
    "text": "Question: Is it possible (or recommended) to create a virtual environment in a specific project folder? Are there advantages or disadvantages to keeping the virtual environment in the default location versus a network drive (e.g., H drive)?\nAnswer: It is definitely possible to create an environment in a specific project folder/on the H drive (conda create --prefix /path/to/your/project_folder/env_name)! Note that, to activate the environment, you need to activate it by the full path, not by a name–e.g., conda activate /path/to/your/project_folder/env_name.\n\nAdvantages:\n\nBackup and Portability: Keeping the environment with the project makes it easier to back up as a single unit and enables access on multiple devices without recreating the environment each time.\n\nDisadvantages:\n\nPerformance: Access speed may be slower on a network drive, and network or permissions issues could make the environment temporarily inaccessible. If you typically work off a network drive, storing your environments within the project folder can be beneficial for organization and consistency.\n\n\n\n\n\n\n\n\nActivating environments with custom folder locations\n\n\n\nYou need to activate the conda environment stored outside of the default location by the path, not by a name.\nconda activate /path/to/your/project_folder/env_name\nYou may also create a symbolic link in the default envs folder to point to your custom environment location.\n\nOn macOS:\nln -s /path/to/your/project_folder/env_name /path/to/conda/envs/&lt;custom_env_name&gt;\nOn Windows:\nmklink /J \"C:\\path\\to\\conda\\envs\\&lt;custom_env_name&gt;\" \"H:\\path\\to\\your\\project_folder\\env_name\"\n\nNow, you will be able to activate the environment by name only.\nconda activate &lt;custom_env_name&gt;\n\n\n\nQuestion: Will creating a virtual environment for each project lead to memory or storage issues? What is the best practice for closing out a project environment?\nAnswer: In general, having a virtual environment for each active project shouldn’t cause memory issues. However, it’s good practice to clean up environments when a project concludes.\n\nHere’s a recommended process:\n\nActivating the environment you want to export.\nExporting the environment to a .yaml file with conda env export &gt; environment.yaml.\nAfter the .yaml file is created (make sure to check before deleting the environment!), you can delete the environment with conda env remove --name env_name (or conda env remove --prefix path/to/env_name if using a specific path).\n\nTip: By exporting the environment before deletion, you can always recreate the environment later if needed by using:\nconda env create --prefix /path/to/your/project_folder/env_name -f environment.yaml\nconda activate /path/to/your/project_folder/env_name"
  },
  {
    "objectID": "FAQ.html#managing-virtual-environments-with-conda",
    "href": "FAQ.html#managing-virtual-environments-with-conda",
    "title": "FAQ",
    "section": "",
    "text": "Question: Is it possible (or recommended) to create a virtual environment in a specific project folder? Are there advantages or disadvantages to keeping the virtual environment in the default location versus a network drive (e.g., H drive)?\nAnswer: It is definitely possible to create an environment in a specific project folder/on the H drive (conda create --prefix /path/to/your/project_folder/env_name)! Note that, to activate the environment, you need to activate it by the full path, not by a name–e.g., conda activate /path/to/your/project_folder/env_name.\n\nAdvantages:\n\nBackup and Portability: Keeping the environment with the project makes it easier to back up as a single unit and enables access on multiple devices without recreating the environment each time.\n\nDisadvantages:\n\nPerformance: Access speed may be slower on a network drive, and network or permissions issues could make the environment temporarily inaccessible. If you typically work off a network drive, storing your environments within the project folder can be beneficial for organization and consistency.\n\n\n\n\n\n\n\n\nActivating environments with custom folder locations\n\n\n\nYou need to activate the conda environment stored outside of the default location by the path, not by a name.\nconda activate /path/to/your/project_folder/env_name\nYou may also create a symbolic link in the default envs folder to point to your custom environment location.\n\nOn macOS:\nln -s /path/to/your/project_folder/env_name /path/to/conda/envs/&lt;custom_env_name&gt;\nOn Windows:\nmklink /J \"C:\\path\\to\\conda\\envs\\&lt;custom_env_name&gt;\" \"H:\\path\\to\\your\\project_folder\\env_name\"\n\nNow, you will be able to activate the environment by name only.\nconda activate &lt;custom_env_name&gt;\n\n\n\nQuestion: Will creating a virtual environment for each project lead to memory or storage issues? What is the best practice for closing out a project environment?\nAnswer: In general, having a virtual environment for each active project shouldn’t cause memory issues. However, it’s good practice to clean up environments when a project concludes.\n\nHere’s a recommended process:\n\nActivating the environment you want to export.\nExporting the environment to a .yaml file with conda env export &gt; environment.yaml.\nAfter the .yaml file is created (make sure to check before deleting the environment!), you can delete the environment with conda env remove --name env_name (or conda env remove --prefix path/to/env_name if using a specific path).\n\nTip: By exporting the environment before deletion, you can always recreate the environment later if needed by using:\nconda env create --prefix /path/to/your/project_folder/env_name -f environment.yaml\nconda activate /path/to/your/project_folder/env_name"
  },
  {
    "objectID": "link.html",
    "href": "link.html",
    "title": "Links",
    "section": "",
    "text": "Links for downloading and installing mentioned software/files.\n \n\nInstall Anaconda\n\n\n \n\nDownload environment YML file\n\n\n \n\nDownload Follow Along File\n\n\n \n\nDownload Dataset for Intro to Pandas"
  },
  {
    "objectID": "session4/sess4preread.html",
    "href": "session4/sess4preread.html",
    "title": "Session 4 – Pre-read",
    "section": "",
    "text": "We will be using sci-kit learn implementations of both of these algorithms during the session 4 tutorial.\n\n\nKNN is a supervised learning algorithm used for classification (and sometimes regression):\n\nYou train the model on labeled data (i.e., you know the “answer” or class).\nWhen predicting a new sample, the model finds the k training samples closest to it (its “neighbors”) and uses them to assign a label.\nCloseness is usually based on Euclidean distance.\n\n\nExample: Given a penguin with known bill length and depth, predict its species by looking at its 5 nearest neighbors in the training data.\n\n\n\n\n\nK-Means is an unsupervised learning algorithm used for clustering:\n\nYou do not provide the true labels.\nThe algorithm tries to split your data into k groups based on similarity.\nIt randomly initializes cluster centers, assigns points to the nearest one, then updates the centers iteratively.\n\n\nExample: Given penguin data without species labels, group them into 3 clusters based on bill length and depth.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nKNN\nK-Means\n\n\n\n\nLearning Type\nSupervised\nUnsupervised\n\n\nGoal\nClassification (or Regression)\nClustering\n\n\nInput Labels\nRequired\nNot used\n\n\nOutput\nPredicted class\nCluster assignment\n\n\nModel Type\nLazy (no training phase)\nIterative center updates"
  },
  {
    "objectID": "session4/sess4preread.html#knn-vs-k-means-supervised-vs-unsupervised-learning",
    "href": "session4/sess4preread.html#knn-vs-k-means-supervised-vs-unsupervised-learning",
    "title": "Session 4 – Pre-read",
    "section": "",
    "text": "We will be using sci-kit learn implementations of both of these algorithms during the session 4 tutorial.\n\n\nKNN is a supervised learning algorithm used for classification (and sometimes regression):\n\nYou train the model on labeled data (i.e., you know the “answer” or class).\nWhen predicting a new sample, the model finds the k training samples closest to it (its “neighbors”) and uses them to assign a label.\nCloseness is usually based on Euclidean distance.\n\n\nExample: Given a penguin with known bill length and depth, predict its species by looking at its 5 nearest neighbors in the training data.\n\n\n\n\n\nK-Means is an unsupervised learning algorithm used for clustering:\n\nYou do not provide the true labels.\nThe algorithm tries to split your data into k groups based on similarity.\nIt randomly initializes cluster centers, assigns points to the nearest one, then updates the centers iteratively.\n\n\nExample: Given penguin data without species labels, group them into 3 clusters based on bill length and depth.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nKNN\nK-Means\n\n\n\n\nLearning Type\nSupervised\nUnsupervised\n\n\nGoal\nClassification (or Regression)\nClustering\n\n\nInput Labels\nRequired\nNot used\n\n\nOutput\nPredicted class\nCluster assignment\n\n\nModel Type\nLazy (no training phase)\nIterative center updates"
  },
  {
    "objectID": "session4/sess4preread.html#plotting-in-python",
    "href": "session4/sess4preread.html#plotting-in-python",
    "title": "Session 4 – Pre-read",
    "section": "Plotting in Python",
    "text": "Plotting in Python\nPlease read the ‘Parts of a Figure’ and ‘Coding Styles’ sections of Quick Start Guide (Matplotlib). We will briefly cover plotting with Seaborn (which is built on top of the Matplotlib package), but will not spend much time talking about base Matplotlib."
  },
  {
    "objectID": "session1/session1-python-setup.html#session-1-learning-goals",
    "href": "session1/session1-python-setup.html#session-1-learning-goals",
    "title": "Python Installation, Environments, and VS Code Setup",
    "section": "Session 1 Learning Goals",
    "text": "Session 1 Learning Goals\nIn this session, we will familiarize with Python and essential tools for building reproducible Python projects. \n💡 Python overview: What is Python? How is it useful compared to R? 💡 Conda: conda for installing packages and creating reproducible Python virtual environments 💡 Familiarize with integrated development environments (Visual Studio Code) and interactive computing tools (Jupyter Notebook) 💡 Python reproducible workflow: Create a GitHub Python repository and work in Visual Studio Code within a project-specific conda environment"
  },
  {
    "objectID": "session1/session1-python-setup.html#what-is-python",
    "href": "session1/session1-python-setup.html#what-is-python",
    "title": "Python Installation, Environments, and VS Code Setup",
    "section": "What is Python?",
    "text": "What is Python?\n\n\n\nPython is an open-source, high-level, interpreted, general-purpose programming language\n\nKey Features\n\n\nEasy to use: Python syntax is designed to be readable and user-friendly\nGeneral-purpose: widely applied in data science, ML/AL, software development, automation, etc.\nInterpreted: run code without compiling (unlike C/C++)\nOpen-source: free to use and supported by a mass user community, who contributes to many high-quality packages open to public"
  },
  {
    "objectID": "session1/session1-python-setup.html#python-vs.-r",
    "href": "session1/session1-python-setup.html#python-vs.-r",
    "title": "Python Installation, Environments, and VS Code Setup",
    "section": "Python vs. R",
    "text": "Python vs. R\nPython and R differ in programming philosophy, computational capacity, and extensibility.\n\n\n\n\nFeature / Task\n\n\nR\n\n\nPython\n\n\n\n\n\n\nProgramming style\n\n\nFunction-oriented\n\n\nMostly object-oriented\n\n\n\n\nScope of functionality\n\n\nPrimarily for statistical computing and data analysis\n\n\nGeneral-purpose: ML & AI, software development, scripting, etc.\n\n\n\n\nComputational power\n\n\n✅ Vectorized operations  ⚠️ Memory-intensive for large data and loops\n\n\n✅ Faster loop performance  ✅ Strong GPU support  ✅ Efficient memory use\n\n\n\n\nPackage ecosystem\n\n\n✅ Rich ecosystem of statistical tools ⚠️ Fewer ML/AI tools\n\n\n☑️ Emerging statistical packages  ✅ Extensive ML/AI tools"
  },
  {
    "objectID": "session1/session1-python-setup.html#python-package-ecosystem",
    "href": "session1/session1-python-setup.html#python-package-ecosystem",
    "title": "Python Installation, Environments, and VS Code Setup",
    "section": "Python Package Ecosystem",
    "text": "Python Package Ecosystem\n\n\nStatistical analysis\nWhile R is the go-to tool for statistical analysis, Python has caught up with many equivalent libraries and functions:\n\nstatsmodels / scipy.stats provide regression modeling and hypothesis testing.\nscikit-survival /lifelines support survival analysis and plotting.\n\n\n\n\nML/DL ecosystem\nPython dominates in machine learning and AI development:\n\nscikit-learn is a comprehensive machine learning library that supports supervised regression and classification (e.g., random forests, gradient boosting) and unsupervised clustering (e.g., K-means) analyses\nTensorFlow / PyTorch are deep learning libraries widely used for computer vision and natural language processing (NLP).\noptuna / Ray can be integrated into ML/DL workflows for easy and efficient model training, hyperparameter tuning, fine-tuning, etc.\n\n\n\n\nOmics data analysis\nEmerging bioinformatics packages that provide standard omics data preprocessing and analysis pipeliness:\n\nscanpy, anndata are libraries for single-cell RNA-seq data loading, preprocessing, and analysis\nBiopython is a set of tools for biological computation that performs file parsering, sequence analysis, clustering algorithms, etc.\npysam works with raw input files (e.g., BAM/SAM/VCF)"
  },
  {
    "objectID": "session1/session1-python-setup.html#essential-tools",
    "href": "session1/session1-python-setup.html#essential-tools",
    "title": "Python Installation, Environments, and VS Code Setup",
    "section": "Essential Tools",
    "text": "Essential Tools\nLet’s familiarize with the essential software tools for Python programming and project building.\n\n\nMiniconda: provides Python installation and conda, a powerful package and environment manager\n\n\n\n\nVisual Studio Code: a lightweight code editor that integrates programming + plots + terminal + git + …\n\n\n\n\nJupyter Notebook: an interactive computing tool that combines code execution + markdown + visualizations\n\n\n\n\nGit (GitHub): for version control, collaboration, and publishing code"
  },
  {
    "objectID": "session1/session1-python-setup.html#miniconda-vs.-anaconda",
    "href": "session1/session1-python-setup.html#miniconda-vs.-anaconda",
    "title": "Python Installation, Environments, and VS Code Setup",
    "section": "Miniconda vs. Anaconda",
    "text": "Miniconda vs. Anaconda\n\nYou can install conda via both installers:\n\nMiniconda (minimal version)\n\n\nAnaconda Distribution (full version)\n\n The Latter provides the Anaconda Navigator application which allows you to manage packages and environments without having to use the command line."
  },
  {
    "objectID": "session1/session1-python-setup.html#conda-virtual-environments",
    "href": "session1/session1-python-setup.html#conda-virtual-environments",
    "title": "Python Installation, Environments, and VS Code Setup",
    "section": "Conda Virtual Environments",
    "text": "Conda Virtual Environments\n\nEnvironments are isolated, self-contained workspace that includes its own language interpreter and package dependencies \nExample:\n\nGlobal environment (system): Miniconda installation of Python 3.12 + dependencies\n\n\nVirtual environment1: Python 3.8 + dependencies\n\n\nVirtual environment2: R 4.4.2 + dependencies\n\n  In this case, creating virtual environments allows these Python installations to operate fully independently and not interfere with each other."
  },
  {
    "objectID": "session1/session1-python-setup.html#why-use-virtual-environments",
    "href": "session1/session1-python-setup.html#why-use-virtual-environments",
    "title": "Python Installation, Environments, and VS Code Setup",
    "section": "Why Use Virtual Environments?",
    "text": "Why Use Virtual Environments?\n You may find the flexibility of environments beneficial in many cases.\n\n\nAvoid Conflicts. Help resolve potential conflicts between different projects concerning conflicting dependencies. Changes made to one environment won’t affect other projects that use different environments.\n\n\n\n\nEasy Management. Reduce the risk of breaking system Python and globally installed packages. You can easily delete a virtual environment if issues occur and recreate it.\n\n\n\n\nReproducibility. Work as time capsules, allowing you to replicate the requirement of a project at later time points or on new machines.\n\n\n\n\nSharing Environments. Allow sharing the Python version and entire list of dependencies with other people through a copy of the .yml file."
  },
  {
    "objectID": "session1/session1-python-setup.html#overview-of-conda-commands",
    "href": "session1/session1-python-setup.html#overview-of-conda-commands",
    "title": "Python Installation, Environments, and VS Code Setup",
    "section": "Overview of conda commands",
    "text": "Overview of conda commands\nLet’s walk through some useful commands.  💡Open Anaconda Prompt (Windows) or Terminal (macOS) to execute commands"
  },
  {
    "objectID": "session1/session1-python-setup.html#visual-studio-code",
    "href": "session1/session1-python-setup.html#visual-studio-code",
    "title": "Python Installation, Environments, and VS Code Setup",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\n\n  Visual Studio Code (VS Code) is one of the most popular open-source code editors with many features.\n\nLightweight\nMulti-language support for Python, R, C++, etc.\nIntegrated Git version control\nExtensible features like Jupyter notebooks, Quarto, remote connection\n\n\n\nJupyter Notebook\n\nJupyter is an interactive computing tool that lets you combine executable code, Markdown text, and visual components in one file called a notebook (.ipynb).\n\n\n\n VS Code + Jupyter notebooks for Python = RStudio + RMarkdowns for R."
  },
  {
    "objectID": "session1/session1-python-setup.html#create-a-github-project---example",
    "href": "session1/session1-python-setup.html#create-a-github-project---example",
    "title": "Python Installation, Environments, and VS Code Setup",
    "section": "Create a GitHub Project - Example",
    "text": "Create a GitHub Project - Example\n\n\nCreate new repository from GitHub (public) or GitHub Enterprise (MSK Secured)\nInitialize with README.md and .gitignore (choose Python template)\n\n\n\nAlternatively…\n\nGo to practice repository: https://github.mskcc.org/Python-Workshop/workshop_proj\nFork the repo to create a copy on your GitHub account \nNow, you should have a forked copy of the repository on your own GitHub account (e.g. username/workshop_proj) \n\n\n\n\nNow, clone the repository to your local computer by clicking  \nOption 1 (CLI): Copy the HTTPS URL and clone with git command\ngit clone https://github.mskcc.org/&lt;username&gt;/workshop_proj.git\nOption 2 (GUI): Open in GitHub Desktop"
  },
  {
    "objectID": "session1/session1-python-setup.html#organize-the-project-directory",
    "href": "session1/session1-python-setup.html#organize-the-project-directory",
    "title": "Python Installation, Environments, and VS Code Setup",
    "section": "Organize the Project Directory",
    "text": "Organize the Project Directory\n\nOrganize the project with separate data/ and notebooks/ folders and environment.yml file to save virtual environment information.\n\n\n\nHere is an example project structure.\nworkshop_proj/\n   ├── data/            # Raw & processed data folder\n   ├── notebooks/       # Jupyter notebooks and scripts\n   ├── environment.yml  # Conda environment YML file\n   ├── .gitignore       # Files that git should not track\n   └── README.md        # Project description \n👉Downlaod workshop environment.yml file and add to the parent directory.\n\n\n\nNote: If you do not use GitHub – that is fine! You can skip the previous step of cloning from GitHub and directly create a local folder with subfolders and files."
  },
  {
    "objectID": "session1/session1-python-setup.html#create-a-project-specific-conda-environment",
    "href": "session1/session1-python-setup.html#create-a-project-specific-conda-environment",
    "title": "Python Installation, Environments, and VS Code Setup",
    "section": "Create a Project-Specific Conda Environment",
    "text": "Create a Project-Specific Conda Environment\n\n\n\n\n\n\nNote: Ensure that Miniconda is installed. For MacOs, ensure conda is correctly initialized with conda init (see installing Miniconda)\n\n\n\n\n\nLaunch Anaconda Prompt (Windows) or Terminal (macOS).\n\nChagne directory to your project folder with cd\nCreate the environment with:\nconda env create -f environment.yml\nconda activate python-intro-env"
  },
  {
    "objectID": "session1/session1-python-setup.html#set-up-vs-code-for-python-jupyter",
    "href": "session1/session1-python-setup.html#set-up-vs-code-for-python-jupyter",
    "title": "Python Installation, Environments, and VS Code Setup",
    "section": "Set up VS Code for Python & Jupyter",
    "text": "Set up VS Code for Python & Jupyter\n\n\nLaunch VS Code\nOpen the Extensions Marketplace (Ctrl+Shift+X or Cmd+Shift+X) \nInstall the following extensions:\n\n✅ Python (auto-, debugging, and interpreter selection)\n✅ Jupyter (notebook interface and interactive execution)\nNote: You still need local installations of Python to enable the extension."
  },
  {
    "objectID": "session1/session1-python-setup.html#work-in-jupyter-notebooks-in-vs-code",
    "href": "session1/session1-python-setup.html#work-in-jupyter-notebooks-in-vs-code",
    "title": "Python Installation, Environments, and VS Code Setup",
    "section": "Work in Jupyter Notebooks in VS Code",
    "text": "Work in Jupyter Notebooks in VS Code\n\n\nOpen your project folder in VS Code:\n\nSelect Open Folder… from the VS Code Welcome page: \nOr by selecting File &gt; Open Folder (Ctrl+K Ctrl+O)"
  },
  {
    "objectID": "session4/session4v2_webpage.html",
    "href": "session4/session4v2_webpage.html",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "",
    "text": "KNN vs K-means\n‘Parts of a Figure’ and ‘Coding Styles’ sections of Quick Start Guide (Matplotlib) ### Optional:\nScikit-learn Documentation\n\nIntroduction to OOP in Python (Real Python)\n\nPlotnine Reference\nSeaborn Reference"
  },
  {
    "objectID": "session4/session4v2_webpage.html#pre-reading-for-this-session",
    "href": "session4/session4v2_webpage.html#pre-reading-for-this-session",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "",
    "text": "KNN vs K-means\n‘Parts of a Figure’ and ‘Coding Styles’ sections of Quick Start Guide (Matplotlib) ### Optional:\nScikit-learn Documentation\n\nIntroduction to OOP in Python (Real Python)\n\nPlotnine Reference\nSeaborn Reference"
  },
  {
    "objectID": "session4/session4v2_webpage.html#note-the-tutorial-below-is-the-same-information-well-be-covering-in-session-4",
    "href": "session4/session4v2_webpage.html#note-the-tutorial-below-is-the-same-information-well-be-covering-in-session-4",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Note: The tutorial below is the same information we’ll be covering in session 4!",
    "text": "Note: The tutorial below is the same information we’ll be covering in session 4!"
  },
  {
    "objectID": "session4/session4v2_webpage.html#session-overview",
    "href": "session4/session4v2_webpage.html#session-overview",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Session Overview",
    "text": "Session Overview\nIn this session, we’ll explore how Python’s object-oriented nature affects our modeling workflows. \nTopics:\n\n\n\nIntro to OOP and how it makes modeling in Python different from R\n\n\nBuilding and extending classes using inheritance and mixins\n\n\nApplying OOP to machine learning through demos with scikit-learn\n\n\n\nCreating and using models\n\n\nPlotting data with plotnine and seaborn"
  },
  {
    "objectID": "session4/session4v2_webpage.html#why-python",
    "href": "session4/session4v2_webpage.html#why-python",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Why Python? 🐍",
    "text": "Why Python? 🐍\n\n\n\nR: Built by Statisticians for Statisticians\n\nExcels at:\n\nStatistical analysis and modeling\n\nClean outputs and tables from models\nBeautiful data visualizations with simple code\n\n\n\n\n\nPython: General-Purpose Language\n\nExcels at:\n\nMachine Learning, Neural Networks & Deep Learning (scikit-learn, PyTorch, TensorFlow)\n\nImage & Genomic Data Analysis (scikit-image, biopython, scanpy)\nSoftware & Command Line Interfaces, Web Scraping, Automation\n\n\n\n\n\nPython’s broader ecosystem makes it the go-to language in domains like AI, bioinformatics, data engineering, and computational biology.\n\nNote: Packages like rpy2 and reticulate make it possible to use both R and Python in the same project, but those are beyond the scope of this course.\nA primer on reticulate is available here: https://www.r-bloggers.com/2022/04/getting-started-with-python-using-r-and-reticulate/"
  },
  {
    "objectID": "session4/session4v2_webpage.html#programming-styles-r-vs-python",
    "href": "session4/session4v2_webpage.html#programming-styles-r-vs-python",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Programming Styles: R vs Python",
    "text": "Programming Styles: R vs Python\n In the first session, we talked briefly about functional vs object-oriented programming:\n\n\nFunctional programming: focuses on functions as the primary unit of code  Object-oriented programming: uses objects with attached attributes(data) and methods(behaviors) \n\n\nR leans heavily on the functional paradigm — you pass data into functions and get back results, in most cases without altering the original data. Functions and pipes (%&gt;%) dominate most workflows.\nIn Python, everything is an object, even basic things like lists, strings, and dataframes. A lot of ‘functions’ are instead written as object-associated methods. Some of these methods modify the objects in-place by altering their attributes. Understanding how this works is key to using Python effectively!\n\n\nYou’ve already seen this object-oriented style in Sessions 2 and 3 — you create objects like lists or dataframes, then call methods on them like .append() or .sort_values(). In python, instead of piping, we sometimes chain methods together."
  },
  {
    "objectID": "session4/session4v2_webpage.html#modeling-in-python",
    "href": "session4/session4v2_webpage.html#modeling-in-python",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Modeling in Python",
    "text": "Modeling in Python\nPython absolutely uses functions—just like R! They’re helpful for data transformation, wrangling, and automation tasks like looping and parallelization. \nBut when it comes to modeling, libraries are designed around classes: blueprints for creating objects that store data (attributes) and define behaviors (methods). \n\nscikit-learn is great for getting started—everything follows a simple, consistent OOP interface. Its API is also consistant with other modeling packages, like xgboost and scvi-tools.\nscikit-survival is built on top of scikit-learn. https://scikit-survival.readthedocs.io/en/stable/user_guide/00-introduction.html is a good tutorial for it.\nPyTorch and TensorFlow are essential if you go deeper into neural networks or custom models—you’ll define your own model classes with attributes and methods, but the basic structure is similar to scikit-learn.\n\nstatsmodels is an alternative to scikit-learn for statistical analyses and has R-like syntax and outputs. It’s a bit more complex than scikit-learn and a bit less consistant with other packages in the python ecosystem. https://wesmckinney.com/book/modeling is a good tutorial for statsmodels.\n\n\n💡 To work effectively in Python, especially for tasks involving modeling or model training, it helps to think in terms of objects and classes, not just functions."
  },
  {
    "objectID": "session4/session4v2_webpage.html#why-does-oop-matter-in-python-modeling",
    "href": "session4/session4v2_webpage.html#why-does-oop-matter-in-python-modeling",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Why Does OOP Matter in Python Modeling?",
    "text": "Why Does OOP Matter in Python Modeling?\nIn Python modeling frameworks:\n\n\n\nModels are instances of classes\n\n\nYou call methods like .fit(), .predict(), .score()\n\n\nInternal model details like coefficients or layers are stored as attributes\n\n\n\nThis makes model behavior consistent between model classes and even libraries. It also simplifies creating/using pre-trained models: both the architecture and learned weights are bundled into a single object with expected built-in methods like .predict() or .fine_tune().\n\n\nInstead of having a separate results object, like in R, you would retrieve your results by accessing an attribute or using a method that is attached to the model object itself.\n\n\n We’ll focus on scikit-learn in this session, but these ideas carry over to other libraries like xgboost, statsmodels, and PyTorch."
  },
  {
    "objectID": "session4/session4v2_webpage.html#key-oop-principles-recap",
    "href": "session4/session4v2_webpage.html#key-oop-principles-recap",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Key OOP Principles (Recap)",
    "text": "Key OOP Principles (Recap)\nIn OOP, code is structured around objects (as opposed to functions). This paradigm builds off the following principles:\n\n\nEncapsulation: Bundling data and methods together in a single unit.\n\nA StandardScaler object stores mean and variance data and has .fit() and .transform() methods\n\n\n\n\n\nInheritance: Creating new classes based on existing ones.\n\nsklearn.LinearRegression inherits attributes and methods from a general regression model class.\n\n\n\n\n\n\nAbstraction: Hiding implementation details and exposing only essential functionality.\n\ne.g., .fit() works the same way from the outside, regardless of model complexity\n\n\n\n\n\n\nPolymorphism: Objects of different types can be treated the same way if they implement the same methods.\n\nPython’s duck typing:\n\n🦆 “If it walks like a duck and quacks like a duck, then it must be a duck.” 🦆\n\nex: If different objects all have a .summarize() method, we can loop over them and call .summarize() without needing to check their class. As long as the method exists, Python will know what to do.\nThis lets us easily create pipelines that can work for many types of models.\n\n\n\n\nWe won’t cover pipelines here, but they are worth looking into!"
  },
  {
    "objectID": "session4/session4v2_webpage.html#classes-and-objects",
    "href": "session4/session4v2_webpage.html#classes-and-objects",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Classes and Objects",
    "text": "Classes and Objects\nClasses are blueprints for creating objects. Each object contains:\n\n\n\nAttributes (data): model coefficients, class labels\n\n\nMethods (behaviors): .fit(), .predict()\n\n\n👉 To Get the class of an object, use:\n\ntype(object) # Returns the type of the object\n\n👉 To check if an object is an instance of a particular class, use:\n\nisinstance(object, class)  # Returns True if `object` is an instance of `class`.\n\n\n\nKnowing what class an object belongs to helps us understand what methods and attributes it provides."
  },
  {
    "objectID": "session4/session4v2_webpage.html#base-classes",
    "href": "session4/session4v2_webpage.html#base-classes",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Base Classes",
    "text": "Base Classes\nA base class (or parent class) serves as a template for creating objects. Other classes can inherit from it to reuse its properties and methods.\nClasses are defined using the class keyword, and their structure is specified using an __init__() method for initialization.\n\nFor example, we can define a class called Dog and give it attributes that store data about a given dog and methods that represent behaviors an object of the Dog class can perform. We can also edit the special or “dunder” methods (short for double underscore) that define how objects behave in certain contexts.\n\nclass Dog: ## begin class definition\n    def __init__(self, name, breed): ## define init method\n        self.name = name ## add attributes\n        self.breed = breed\n\n    def speak(self): ## add methods\n        return f\"{self.name} says woof!\"\n\n    def __str__(self): # __str__(self) tells python what to display when an object is printed\n        return f\"Our dog {self.name}\"\n\n    def __repr__(self): # add representation to display when dog is called in console\n        return f\"Dog(name={self.name!r}, breed={self.breed!r})\""
  },
  {
    "objectID": "session4/session4v2_webpage.html#creating-a-dog",
    "href": "session4/session4v2_webpage.html#creating-a-dog",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Creating a dog",
    "text": "Creating a dog\nCreating an instance of the Dog class lets us model a particular dog:\n\nbuddy = Dog(\"Buddy\", \"Golden Retriever\")\nprint(f\"Buddy is an object of class {type(buddy)}\")\n\nBuddy is an object of class &lt;class '__main__.Dog'&gt;\n\n\n\n\nWe set the value of the attributes [name and breed], which are then stored as part of the buddy object\n\n\nWe can use any methods defined in the Dog class on buddy\n\n\n\n## if we want to see what kind of dog our dog is\n## we can call buddy's attributes\nprint(f\"Our dog {buddy.name} is a {buddy.breed}.\")\n\n## we can also call any Dog methods\nprint(buddy.speak())  \n\n## including special methods\nbuddy ## displays what was in the __repr__() method\n\nOur dog Buddy is a Golden Retriever.\nBuddy says woof!\n\n\nDog(name='Buddy', breed='Golden Retriever')\n\n\nNote: For python methods, the self argument is assumed to be passed and therefore we do not put anything in the parentheses when calling .speak(). For attributes, we do not put () at all."
  },
  {
    "objectID": "session4/session4v2_webpage.html#derived-child-classes",
    "href": "session4/session4v2_webpage.html#derived-child-classes",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Derived (Child) Classes",
    "text": "Derived (Child) Classes\nDerived/child classes build on base classes using the principle of inheritence. \nNow that we have a Dog class, we can build on it to create a specialized GuardDog class.\n\nclass GuardDog(Dog):  # GuardDog inherits from Dog\n    def __init__(self, name, breed, training_level): ## in addition to name and breed, we can \n        # define a training level. \n        # Call the parent (Dog) class's __init__ method\n        super().__init__(name, breed)\n        self.training_level = training_level  # New attribute for GuardDog that stores the \n        # training level for the dog\n\n    def guard(self): ## checks if the training level is &gt; 5 and if not says train more\n        if self.training_level &gt; 5:\n            return f\"{self.name} is guarding the house!\"\n        else:\n            return f\"{self.name} needs more training before guarding.\"\n    \n    def train(self): # modifies the training_level attribute to increase the dog's training level\n        self.training_level = self.training_level + 1\n        return f\"Training {self.name}. {self.name}'s training level is now {self.training_level}\"\n\n# Creating an instance of GuardDog\nrex = GuardDog(\"Rex\", \"German Shepherd\", training_level= 5)\n\n\nNow that we have a dog (rex), we can call on any of the methods/attributes introduced in the Dog class as well as the new GuardDog class.\nUsing methods from the base class:\n\nprint(rex.speak())\nrex\n\nRex says woof!\n\n\nDog(name='Rex', breed='German Shepherd')\n\n\n\nUsing a method from the child class:\n\nprint(f\"{rex.name}'s training level is {rex.training_level}.\")\nprint(rex.guard()) \n\nRex's training level is 5.\nRex needs more training before guarding.\n\n\n. . .\nThis is the power of inheritance—we don’t have to rewrite everything from scratch!\n\nUnlike standalone functions, methods in Python often update objects in-place—meaning they modify the object itself rather than returning a new one.\nWe can use the .train() method to increase rex’s training level.\n\nprint(rex.train())\n\nTraining Rex. Rex's training level is now 6\n\n\n\n\n\n\n\n\nBe Careful!!!\n\n\n\nCalling rex.train() within a print statement still updates rex’s training level. If we were to do this instead:\nrex.train()\nprint(rex.train())\nit would train rex twice!\n\n\n. . .\n\nNow if we check,\n\nprint(f\"{rex.name}'s training level is {rex.training_level}.\")\nprint(rex.guard()) \n\nRex's training level is 6.\nRex is guarding the house!\n\n\n\n. . .\nAs with Rex, child classes inherit all attributes (.name and .breed) and methods (.speak() __repr__()) from parent classes. They can also have new methods (.train())."
  },
  {
    "objectID": "session4/session4v2_webpage.html#mixins",
    "href": "session4/session4v2_webpage.html#mixins",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Mixins",
    "text": "Mixins\nA mixin is a special kind of class designed to add functionality to another class. Unlike base classes, mixins aren’t used alone.\n\nFor example, scikit-learn uses mixins like:\n- sklearn.base.ClassifierMixin (adds classifier-specific methods)\n- sklearn.base.RegressorMixin (adds regression-specific methods)\nwhich it adds to the BaseEstimator class to add functionality.  \nTo finish up our dog example, we are going to define a mixin class that adds learning tricks to the base Dog class and use it to create a new class called SmartDog.\n\n\nWhen creating a mixin class, we let the other base classes carry most of the initialization\n\nclass TrickMixin: ## mixin that will let us teach a dog tricks\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)  # Ensures proper initialization in multi inheritance\n        self.tricks = []  # Add attribute to store tricks\n\n## add trick methods\n    def learn_trick(self, trick):\n        \"\"\"Teaches the dog a new trick.\"\"\"\n        if trick not in self.tricks:\n            self.tricks.append(trick)\n            return f\"{self.name} learned a new trick: {trick}!\"\n        return f\"{self.name} already knows {trick}!\"\n\n    def perform_tricks(self):\n        \"\"\"Returns a list of tricks the dog knows.\"\"\"\n        if self.tricks:\n            return f\"{self.name} can perform: {', '.join(self.tricks)}.\"\n        return f\"{self.name} hasn't learned any tricks yet.\"\n\n## note: the TrickMixin class is not a standalone class!\n\n\nBy including both Dog and TrickMixin as base classes, we give objects of class SmartDog the ability to speak and learn tricks!\n\nclass SmartDog(Dog, TrickMixin):\n    def __init__(self, name, breed):\n        super().__init__(name, breed)  # Initialize Dog class\n        TrickMixin.__init__(self)  # Initialize TrickMixin separately\n\n# a SmartDog object can use methods from both parent object `Dog` and mixin `TrickMixin`.\nmy_smart_dog = SmartDog(\"Buddy\", \"Border Collie\")\nprint(my_smart_dog.speak()) \n\nBuddy says woof!\n\n\n\n\nprint(my_smart_dog.learn_trick(\"Sit\"))  \nprint(my_smart_dog.learn_trick(\"Roll Over\")) \nprint(my_smart_dog.learn_trick(\"Sit\"))  \n\nBuddy learned a new trick: Sit!\nBuddy learned a new trick: Roll Over!\nBuddy already knows Sit!\n\n\n\n\nprint(my_smart_dog.perform_tricks()) \n\nBuddy can perform: Sit, Roll Over."
  },
  {
    "objectID": "session4/session4v2_webpage.html#duck-typing",
    "href": "session4/session4v2_webpage.html#duck-typing",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Duck Typing",
    "text": "Duck Typing\n\n🦆 “If it quacks like a duck and walks like a duck, it’s a duck.” 🦆\n\nPython’s duck typing makes our lives a lot easier, and is one of the main benefits of methods over functions:\n\n\nRepurposing old code - methods by the same name work the same for different model types\n\n\nNot necessary to check types before using methods - methods are assumed to work on the object they’re attached to\n\n\nWe can demonstrate this by defining two new base classes that are different than Dog but also have a speak() method.\n. . .\n\nclass Human:\n    def __init__(self, name):\n        self.name = name\n\n    def speak(self):\n        return f\"{self.name} says hello!\"\n\nclass Parrot:\n    def __init__(self, name):\n        self.name = name\n\n    def speak(self):\n        return f\"{self.name} says squawk!\""
  },
  {
    "objectID": "session4/session4v2_webpage.html#duck-typing-in-action",
    "href": "session4/session4v2_webpage.html#duck-typing-in-action",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Duck Typing in Action",
    "text": "Duck Typing in Action\nEven though Dog, Human and Parrot are entirely different classes…\n\ndef call_speaker(obj):\n    print(obj.speak())\n\ncall_speaker(Dog(\"Fido\", \"Labrador\"))\ncall_speaker(Human(\"Alice\"))\ncall_speaker(Parrot(\"Polly\"))\n\nFido says woof!\nAlice says hello!\nPolly says squawk!\n\n\n. . .\nThey all implement .speak(), so Python treats them the same!\nIn the context of our work, this would allow us to make a pipeline using models from different libraries that have the same methods.\n\nWhile our dog example was very simple, this is the same way that model classes work in python!"
  },
  {
    "objectID": "session4/session4v2_webpage.html#example-oop-in-machine-learning-and-modeling",
    "href": "session4/session4v2_webpage.html#example-oop-in-machine-learning-and-modeling",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Example: OOP in Machine Learning and Modeling",
    "text": "Example: OOP in Machine Learning and Modeling\nMachine learning models in Python are implemented as classes.\n\n\n\nWhen you create a model, you’re instantiating an object of a predefined class (e.g., LogisticRegression()).\n\n\nThat model has attributes (parameters, coefficients) and methods (like .fit() and .predict()).\n\n\nFor example LogisticRegression is a model class that inherits from SparseCoefMixin and BaseEstimator.\nclass LogisticRegression(LinearClassifierMixin, SparseCoefMixin, BaseEstimator):\nTo perform logistic regression, we create an instance of the LogisticRegression class.\n## Example: \nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()  # Creating an instance of the LogisticRegression class\nmodel.fit(X_train, y_train)   # Calling a method to train the model\npredictions = model.predict(X_test)  # Calling a method to make predictions\ncoefs = model.coef_ # Access model coefficients using attribute"
  },
  {
    "objectID": "session4/session4v2_webpage.html#key-benefits-of-oop-in-machine-learning",
    "href": "session4/session4v2_webpage.html#key-benefits-of-oop-in-machine-learning",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Key Benefits of OOP in Machine Learning",
    "text": "Key Benefits of OOP in Machine Learning\n\nEncapsulation – Models store parameters and methods inside a single object.\n\nInheritance – New models can build on base models, reusing existing functionality.\n\nAbstraction – .fit() should work as expected, regardless of complexity of underlying implimentation.\nPolymorphism (Duck Typing) – Different models share the same method names (.fit(), .predict()), making them easy to use interchangeably, particularly in analysis pipelines.\n\nUnderstanding base classes and mixins is especially important when working with deep learning frameworks like PyTorch and TensorFlow, which require us to create our own model classes."
  },
  {
    "objectID": "session4/session4v2_webpage.html#mini-project-classifying-penguins-with-scikit-learn",
    "href": "session4/session4v2_webpage.html#mini-project-classifying-penguins-with-scikit-learn",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "🐧 Mini Project: Classifying Penguins with scikit-learn",
    "text": "🐧 Mini Project: Classifying Penguins with scikit-learn\nNow that you understand classes and data structures in Python, let’s apply that knowledge to classify penguin species using two features:\n\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nWe’ll explore:\n\n\nUnsupervised learning with K-Means clustering (model doesn’t ‘know’ y)\n\n\nSupervised learning with a k-NN classifier (model trained w/ y information)\n\n\nAll scikit-learn models are designed to have\n\n\nCommon Methods:\n\n\n\n.fit() — Train the model\n\n\n.predict() — Make predictions\n\n\n\nCommon Attributes:\n\n\n.classes_, .n_clusters_, etc.\n\n\n\n\n\nThis is true of the scikit-survival package too!"
  },
  {
    "objectID": "session4/session4v2_webpage.html#import-libraries",
    "href": "session4/session4v2_webpage.html#import-libraries",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Import Libraries",
    "text": "Import Libraries\nBefore any analysis, we must import the necessary libraries.\nFor large libraries like scikit-learn, PyTorch, or TensorFlow, we usually do not import the entire package. Instead, we selectively import the classes and functions we need.\n\n\nClasses\n- StandardScaler — for feature scaling\n- KNeighborsClassifier — for supervised k-NN classification\n- KMeans — for unsupervised clustering\n\n\n🔤 Naming Tip:\n- CamelCase = Classes\n- snake_case = Functions\n\n\nFunctions\n- train_test_split() — to split data into training and test sets\n- accuracy_score() — to evaluate classification accuracy\n- classification_report() — to print precision, recall, F1 (balance of precision and recall), Support (number of true instances per class) - adjusted_rand_score() — to evaluate clustering performance"
  },
  {
    "objectID": "session4/session4v2_webpage.html#import-libraries-1",
    "href": "session4/session4v2_webpage.html#import-libraries-1",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Import Libraries",
    "text": "Import Libraries\n\n## imports\nimport pandas as pd\nimport numpy as np\n\nfrom plotnine import *\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom great_tables import GT\n\n## sklearn imports\n\n## import classes\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\n\n## import functions\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, adjusted_rand_score"
  },
  {
    "objectID": "session4/session4v2_webpage.html#data-preparation",
    "href": "session4/session4v2_webpage.html#data-preparation",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Data Preparation",
    "text": "Data Preparation\n\n# Load the Penguins dataset\npenguins = sns.load_dataset(\"penguins\").dropna()\n\n# Make a summary table for the penguins dataset, grouping by species. \nsummary_table = penguins.groupby(\"species\").agg({\n    \"bill_length_mm\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"bill_depth_mm\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"sex\": lambda x: x.value_counts().to_dict()  # Count of males and females\n})\n\n# Round numeric values to 1 decimal place (excluding the 'sex' column)\nfor col in summary_table.columns:\n    if summary_table[col].dtype in [float, int]:\n        summary_table[col] = summary_table[col].round(1)\n\n# Display the result\ndisplay(summary_table)\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nsex\n\n\n\nmean\nstd\nmin\nmax\nmean\nstd\nmin\nmax\n&lt;lambda&gt;\n\n\nspecies\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n38.8\n2.7\n32.1\n46.0\n18.3\n1.2\n15.5\n21.5\n{'Male': 73, 'Female': 73}\n\n\nChinstrap\n48.8\n3.3\n40.9\n58.0\n18.4\n1.1\n16.4\n20.8\n{'Female': 34, 'Male': 34}\n\n\nGentoo\n47.6\n3.1\n40.9\n59.6\n15.0\n1.0\n13.1\n17.3\n{'Male': 61, 'Female': 58}"
  },
  {
    "objectID": "session4/session4v2_webpage.html#data-visualization",
    "href": "session4/session4v2_webpage.html#data-visualization",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Data Visualization",
    "text": "Data Visualization\nTo do visualization, we can use either seaborn or plotnine. plotnine mirrors ggplot2 syntax from R and is great for layered grammar-of-graphics plots, while seaborn seaborn is more convienient if you want to put multiple plots on the same figure. \n\nPlotting with Plotnine vs Seaborn\n\n\nPlotnine (like ggplot2 in R) The biggest differences between plotnine and ggplot2 syntax are:\n\n\nWith plotnine the whole call is wrapped in () parentheses\n\n\nVariables are called with strings (\"\" are needed!)\n\n\nIf you don’t use from plotnine import *, you will need to import each individual function you plan to use!\n\n\n\nSeaborn (base matplotlib + enhancements)\n\n\nDesigned for quick, polished plots\n\n\nWorks well with pandas DataFrames or NumPy arrays\n\n\nIntegrates with matplotlib for customization\n\n\nGood for things like decision boundaries or heatmaps\n\n\nHarder to customize than plotnine plots"
  },
  {
    "objectID": "session4/session4v2_webpage.html#scatterplot-with-plotnine",
    "href": "session4/session4v2_webpage.html#scatterplot-with-plotnine",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Scatterplot with plotnine",
    "text": "Scatterplot with plotnine\nTo take a look at the distribution of our species by bill length and bill depth before clustering…\n\nplot1 = (ggplot(penguins, aes(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"species\"))\n + geom_point()\n + ggtitle(\"Penguin Species\")\n + theme_bw())\n\ndisplay(plot1)"
  },
  {
    "objectID": "session4/session4v2_webpage.html#scatterplot-with-seaborn",
    "href": "session4/session4v2_webpage.html#scatterplot-with-seaborn",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Scatterplot with seaborn",
    "text": "Scatterplot with seaborn\nWe can make a similar plot in seaborn. This time, let’s include sex by setting the point style\n\n# Create the figure and axes obects\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Create a plot \nsns.scatterplot(\n    data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\",\n    hue=\"species\", ## hue = fill\n    style=\"sex\",  ## style = style of dots\n    palette=\"Set2\", ## sets color pallet\n    edgecolor=\"black\", s=300, ## line color and point size \n    ax=ax              ## Draw plot on ax      \n)\n\n# Use methods on ax to set title, labels\nax.set_title(\"Penguin Bill Length vs Depth by Species\")\nax.set_xlabel(\"Bill Length (mm)\")\nax.set_ylabel(\"Bill Depth (mm)\")\nax.legend(title=\"Species\")\n\n# Plot the figure\nfig.tight_layout() \n#fig.show() -&gt; if not in interactive"
  },
  {
    "objectID": "session4/session4v2_webpage.html#scaling-the-data---understanding-the-standard-scaler-class",
    "href": "session4/session4v2_webpage.html#scaling-the-data---understanding-the-standard-scaler-class",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Scaling the data - Understanding the Standard Scaler class",
    "text": "Scaling the data - Understanding the Standard Scaler class\nFor our clustering to work well, the predictors should be on the same scale. To achieve this, we use an instance of the StandardScaler class.\nclass sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)\n. . .\nParameters are supplied by user\n- copy, with_mean, with_std \nAttributes contain the data of the object\n- scale_: scaling factor\n- mean_: mean value for each feature\n- var_: variance for each feature\n- n_features_in_: number of features seen during fit\n- n_samples_seen: number of samples processed for each feature \nMethods describe the behaviors of the object and/or modify its attributes\n- fit(X): computes mean and std used for scaling and ‘fits’ scaler to data X\n- transform(X): performs standardization by centering and scaling X with fitted scaler\n- fit_transform(X): does both"
  },
  {
    "objectID": "session4/session4v2_webpage.html#scaling-data",
    "href": "session4/session4v2_webpage.html#scaling-data",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Scaling Data",
    "text": "Scaling Data\n\n# Selecting features for clustering -&gt; let's just use bill length and bill depth.\nX = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]]\ny = penguins[\"species\"]\n\n# Standardizing the features for better clustering performance\nscaler = StandardScaler() ## create instance of StandardScaler\nX_scaled = scaler.fit_transform(X) \n\n\n\n\n\n\n\n\n\n\nOriginal vs Scaled Features\n\n\nFeature\nOriginal\nScaled\n\n\nBill Length\nBill Depth\nBill Length\nBill Depth\n\n\n\n\nmean\n44\n17\n0\n0\n\n\nstd\n5\n2\n1\n1\n\n\n\n\n\n\n        \n\n\n\n\nShow table code\n## Make X_scaled a pandas df\nX_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n\n# Compute summary statistics and round to 2 sig figs\noriginal_stats = X.agg([\"mean\", \"std\"])\nscaled_stats = X_scaled_df.agg([\"mean\", \"std\"])\n\n# Combine into a single table with renamed columns\nsummary_table = pd.concat([original_stats, scaled_stats], axis=1)\nsummary_table.columns = [\"Bill_Length_o\", \"Bill_Depth_o\", \"Bill_Length_s\", \"Bill_Depth_s\"]\nsummary_table.index.name = \"Feature\"\n\n# Display nicely with great_tables\n(\n    GT(summary_table.reset_index()).tab_header(\"Original vs Scaled Features\")\n    .fmt_number(columns =  [\"Bill_Length_o\", \"Bill_Depth_o\", \"Bill_Length_s\", \"Bill_Depth_s\"], decimals=0)\n    .tab_spanner(label=\"Original\", columns=[\"Bill_Length_o\", \"Bill_Depth_o\"])\n    .tab_spanner(label=\"Scaled\", columns=[\"Bill_Length_s\", \"Bill_Depth_s\"])\n    .cols_label(Bill_Length_o = \"Bill Length\", Bill_Depth_o = \"Bill Depth\", Bill_Length_s = \"Bill Length\", Bill_Depth_s = \"Bill Depth\")\n    .tab_options(table_font_size = 16)\n)"
  },
  {
    "objectID": "session4/session4v2_webpage.html#understanding-the-kmeans-model-class",
    "href": "session4/session4v2_webpage.html#understanding-the-kmeans-model-class",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Understanding the KMeans model class",
    "text": "Understanding the KMeans model class\nclass sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, \ntol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')\nParameters: Set by user at time of instantiation\n- n_clusters, max_iter, algorithm \nAttributes: Store object data\n- cluster_centers_: stores coordinates of cluster centers\n- labels_: stores labels of each point - n_iter_: number of iterations run (will be changed during method run)\n- n_features_in and feature_names_in_: store info about features seen during fit \nMethods: Define object behaviors\n- fit(X): fits model to data X - predict(X): predicts closest cluster each sample in X belongs to\n- transform(X): transforms X to cluster-distance space\n\n\nCreate model\n\n## Choosing 3 clusters b/c we have 3 species\nkmeans = KMeans(n_clusters=3, random_state=42) ## make an instance of the K means class\nkmeans\n\nKMeans(n_clusters=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiNot fittedKMeans(n_clusters=3, random_state=42) \n\n\n\n\n\nFit model to data\n\n## the fit\npenguins[\"kmeans_cluster\"] = kmeans.fit_predict(X_scaled)\n\n## now that we fit the model, we should have cluster centers\nprint(\"Coordinates of cluster centers:\", kmeans.cluster_centers_)\n\n## shows that model is fitted\nkmeans\n\nCoordinates of cluster centers: [[-0.95023997  0.55393493]\n [ 0.58644397 -1.09805504]\n [ 1.0886843   0.79503579]]\n\n\nKMeans(n_clusters=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3, random_state=42)"
  },
  {
    "objectID": "session4/session4v2_webpage.html#use-function-to-calculate-ari",
    "href": "session4/session4v2_webpage.html#use-function-to-calculate-ari",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Use function to calculate ARI",
    "text": "Use function to calculate ARI\nTo check how good our model is, we can use one of the functions included in the sklearn library.\nThe adjusted_rand_score() function evaluates how well the cluster groupings agree with the species groupings while adjusting for chance.\n\n# Calculate clustering performance using Adjusted Rand Index (ARI)\nkmeans_ari = adjusted_rand_score(penguins['species'], penguins[\"kmeans_cluster\"])\nprint(f\"k-Means Adjusted Rand Index: {kmeans_ari:.2f}\")\n\nk-Means Adjusted Rand Index: 0.82\n\n\n\n\nWe can also use methods on our data structure to create new data\n\nWe can use the .groupby() method to help us plot cluster agreement with species label as a heatmap\nIf we want to add sex as a variable to see if that is why our clusters don’t agree with our species, we can use a scatterplot\nUsing seaborn and matplotlib, we can easily put both of these plots on the same figure. \n\n\n# Count occurrences of each species-cluster-sex combination\n# (.size gives the count as index, use reset_index to get count column.)\nscatter_data = (penguins.groupby([\"species\", \"kmeans_cluster\", \"sex\"])\n                .size()\n                .reset_index(name=\"count\"))\nspecies_order = list(scatter_data['species'].unique()) ## defining this for later\n\n# Create a mapping to add horizontal jitter for each sex for scatterplot\nsex_jitter = {'Male': -0.1, 'Female': 0.1}\nscatter_data['x_jittered'] = scatter_data.apply(\n    lambda row: scatter_data['species'].unique().tolist().index(row['species']) +\n     sex_jitter.get(row['sex'], 0),\n    axis=1\n)\n\nheatmap_data = scatter_data.pivot_table(index=\"kmeans_cluster\", columns=\"species\", \nvalues=\"count\", aggfunc=\"sum\", fill_value=0)"
  },
  {
    "objectID": "session4/session4v2_webpage.html#creating-plots",
    "href": "session4/session4v2_webpage.html#creating-plots",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Creating Plots",
    "text": "Creating Plots\n\n# Prepare the figure with 2 subplots; the axes object will contain both plots\nfig2, axes = plt.subplots(1, 2, figsize=(16, 7)) ## 1 row 2 columns\n\n# Plot heatmap on the first axis\nsns.heatmap(data = heatmap_data, cmap=\"Blues\", linewidths=0.5, linecolor='white', annot=True, \nfmt='d', ax=axes[0])\naxes[0].set_title(\"Heatmap of KMeans Clustering by Species\")\naxes[0].set_xlabel(\"Species\")\naxes[0].set_ylabel(\"KMeans Cluster\")\n\n# Scatterplot with jitter\nsns.scatterplot(data=scatter_data, x=\"x_jittered\", y=\"kmeans_cluster\",\n    hue=\"species\", style=\"sex\", size=\"count\", sizes=(100, 500),\n    alpha=0.8, ax=axes[1], legend=\"brief\")\naxes[1].set_xticks(range(len(species_order)))\naxes[1].set_xticklabels(species_order)\naxes[1].set_title(\"Cluster Assignment by Species and Sex (Jittered)\")\naxes[1].set_ylabel(\"KMeans Cluster\")\naxes[1].set_xlabel(\"Species\")\naxes[1].set_yticks([0, 1, 2])\naxes[1].legend(bbox_to_anchor=(1.05, 0.5), loc='center left', borderaxespad=0.0, title=\"Legend\")\n\nfig2.tight_layout()\n#fig2.show()"
  },
  {
    "objectID": "session4/session4v2_webpage.html#project-2-knn-classification",
    "href": "session4/session4v2_webpage.html#project-2-knn-classification",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Project 2: KNN classification",
    "text": "Project 2: KNN classification\nFor our KNN classification, the model is supervised (meaning it is dependent on the outcome ‘y’ data). This time, we need to split our data into a training and test set. \n. . .\nThe function train_test_split() from scikit-learn is helpful here!\n\n# Splitting dataset into training and testing sets (still using scaled X!)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n\n. . .\n\nUnlike R functions, which return a single object (often a list when multiple outputs are needed), Python functions can return multiple values as a tuple—letting you unpack them directly into separate variables."
  },
  {
    "objectID": "session4/session4v2_webpage.html#understanding-kneighborsclassifier-class",
    "href": "session4/session4v2_webpage.html#understanding-kneighborsclassifier-class",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Understanding KNeighborsClassifier class",
    "text": "Understanding KNeighborsClassifier class\nclass sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', \nalgorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n. . .\nParameters: Set by user at time of instantiation\n- n_neigbors, weights, algorithm, etc. \nAttributes: Store object data\n- classes_: class labels known to the classifier\n- effective_metric_: distance metric used\n- effective_metric_params_: parameters for the metric function\n- n_features_in and feature_names_in_: store info about features seen during fit\n- n_samples_fit_: number of samples in fitted data \nMethods: Define object behaviors\n- .fit(X, y): fit knn classifier from training dataset (X and y)\n- .predict(X): predict class labels for provided data X\n- .predict_proba(X): return probability estimates for test data X\n- .score(X, y): return mean accuracy on given test data X and labels y"
  },
  {
    "objectID": "session4/session4v2_webpage.html#making-an-instance-of-kneighborsclassifier-and-fitting-to-training-data",
    "href": "session4/session4v2_webpage.html#making-an-instance-of-kneighborsclassifier-and-fitting-to-training-data",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Making an instance of KNeighborsClassifier and fitting to training data",
    "text": "Making an instance of KNeighborsClassifier and fitting to training data\n\nFor a supervised model, y_train is included in .fit()!\n\n\n## perform knn classification\n# Applying k-NN classification with 5 neighbors\nknn = KNeighborsClassifier(n_neighbors=5) ## make an instance of the KNeighborsClassifier class\n# and set the n_neighbors parameter to be 5. \n\n# Use the fit method to fit the model to the training data\nknn.fit(X_train, y_train)\nknn"
  },
  {
    "objectID": "session4/session4v2_webpage.html#once-the-model-is-fit",
    "href": "session4/session4v2_webpage.html#once-the-model-is-fit",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Once the model is fit…",
    "text": "Once the model is fit…\n-We can look at its attributes (ex: .classes_) which gives the class labels as known to the classifier\n\nprint(knn.classes_)\n\n['Adelie' 'Chinstrap' 'Gentoo']\n\n\n. . .\n-And use fitted model to predict species for test data\n\n# Use the predict method on the test data to get the predictions for the test data\ny_pred = knn.predict(X_test)\n\n# Also can take a look at the prediction probabilities, \n# and use the .classes_ attribute to put the column labels in the right order\nprobs = pd.DataFrame(\n    knn.predict_proba(X_test),\n    columns = knn.classes_)\nprobs['y_pred'] = y_pred\n\nprint(\"Predicted probabilities: \\n\", probs.head())\n\nPredicted probabilities: \n    Adelie  Chinstrap  Gentoo     y_pred\n0     1.0        0.0     0.0     Adelie\n1     0.0        0.0     1.0     Gentoo\n2     1.0        0.0     0.0     Adelie\n3     0.0        0.6     0.4  Chinstrap\n4     1.0        0.0     0.0     Adelie"
  },
  {
    "objectID": "session4/session4v2_webpage.html#scatterplot-for-k-nn-classification-of-test-data",
    "href": "session4/session4v2_webpage.html#scatterplot-for-k-nn-classification-of-test-data",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Scatterplot for k-NN classification of test data",
    "text": "Scatterplot for k-NN classification of test data\n\nCreate dataframe of unscaled X_test, bill_length_mm, and bill_depth_mm.\nAdd to it the actual and predicted species labels\n\n\n## First unscale the test data\nX_test_unscaled = scaler.inverse_transform(X_test)\n\n## create dataframe \npenguins_test = pd.DataFrame(\n    X_test_unscaled,\n    columns=['bill_length_mm', 'bill_depth_mm']\n)\n\n## add actual and predicted species \npenguins_test['y_actual'] = y_test.values\npenguins_test['y_pred'] = y_pred\npenguins_test['correct'] = penguins_test['y_actual'] == penguins_test['y_pred']\n\nprint(\"Results: \\n\", penguins_test.head())\n\nResults: \n    bill_length_mm  bill_depth_mm   y_actual     y_pred  correct\n0            39.5           16.7     Adelie     Adelie     True\n1            46.9           14.6     Gentoo     Gentoo     True\n2            42.1           19.1     Adelie     Adelie     True\n3            49.8           17.3  Chinstrap  Chinstrap     True\n4            41.1           18.2     Adelie     Adelie     True"
  },
  {
    "objectID": "session4/session4v2_webpage.html#plotnine-scatterplot-for-k-nn-classification-of-test-data",
    "href": "session4/session4v2_webpage.html#plotnine-scatterplot-for-k-nn-classification-of-test-data",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Plotnine scatterplot for k-NN classification of test data",
    "text": "Plotnine scatterplot for k-NN classification of test data\nTo see how well our model did at classifying the remaining penguins…\n\n## Build the plot\nplot3 = (ggplot(penguins_test, aes(x=\"bill_length_mm\", y=\"bill_depth_mm\", \ncolor=\"y_actual\", fill = 'y_pred', shape = 'correct'))\n + geom_point(size=4, stroke=1.1)  # Stroke controls outline thickness\n + scale_shape_manual(values={True: 'o', False: '^'})  # Circle and triangle\n + ggtitle(\"k-NN Classification Results\")\n + theme_bw())\n\ndisplay(plot3)"
  },
  {
    "objectID": "session4/session4v2_webpage.html#visualizing-decision-boundary-with-seaborn-and-matplotlib",
    "href": "session4/session4v2_webpage.html#visualizing-decision-boundary-with-seaborn-and-matplotlib",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Visualizing Decision Boundary with seaborn and matplotlib",
    "text": "Visualizing Decision Boundary with seaborn and matplotlib\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create and fit label encoder for y (just makes y numeric because it makes the scatter plot happy)\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Create the plot objects\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Create display object\ndisp = DecisionBoundaryDisplay.from_estimator(\n    knn,\n    X_test,\n    response_method = 'predict',\n    plot_method = 'pcolormesh',\n    xlabel = \"bill_length_scaled\",\n    ylabel = \"bill_depth_scaled\",\n    shading = 'auto',\n    alpha = 0.5,\n    ax = ax\n)\n\n# Use method from display object to create scatter plot\nscatter = disp.ax_.scatter(X_scaled[:,0], X_scaled[:,1], c=y_encoded, edgecolors = 'k')\ndisp.ax_.legend(scatter.legend_elements()[0], knn.classes_, loc = 'lower left', title = 'Species')\n_ = disp.ax_.set_title(\"Penguin Classification\")\n\nfig.show()"
  },
  {
    "objectID": "session4/session4v2_webpage.html#evaluate-knn-performance",
    "href": "session4/session4v2_webpage.html#evaluate-knn-performance",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Evaluate KNN performance",
    "text": "Evaluate KNN performance\nTo check the performance of our KNN classifier, we can check the accuracy score and print a classification report.\n- accuracy_score and classification_report are both functions!\n- They are not unique to scikit-learn classes so it makes sense for them to be functions not methods\n\n## eval knn performance\nknn_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"k-NN Accuracy: {knn_accuracy:.2f}\")\nprint(\"Classification Report: \\n\", classification_report(y_test, y_pred))\n\nk-NN Accuracy: 0.94\nClassification Report: \n               precision    recall  f1-score   support\n\n      Adelie       0.98      0.98      0.98        48\n   Chinstrap       0.80      0.89      0.84        18\n      Gentoo       0.97      0.91      0.94        34\n\n    accuracy                           0.94       100\n   macro avg       0.92      0.93      0.92       100\nweighted avg       0.94      0.94      0.94       100"
  },
  {
    "objectID": "session4/session4v2_webpage.html#make-a-summary-table-of-metrics-for-both-models",
    "href": "session4/session4v2_webpage.html#make-a-summary-table-of-metrics-for-both-models",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Make a Summary Table of Metrics for Both Models",
    "text": "Make a Summary Table of Metrics for Both Models\n\nsummary_table = pd.DataFrame({\n    \"Metric\": [\"k-Means Adjusted Rand Index\", \"k-NN Accuracy\"],\n    \"Value\": [kmeans_ari, knn_accuracy]\n})\n(\n    GT(summary_table)\n    .tab_header(title = \"Model Results Summary\")\n    .fmt_number(columns = \"Value\", n_sigfig = 2)\n    .tab_options(table_font_size = 20)\n)\n\n\n\n\n\n\n\nModel Results Summary\n\n\nMetric\nValue\n\n\n\n\nk-Means Adjusted Rand Index\n0.82\n\n\nk-NN Accuracy\n0.94"
  },
  {
    "objectID": "session4/session4v2_webpage.html#key-takeaways-from-this-session",
    "href": "session4/session4v2_webpage.html#key-takeaways-from-this-session",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Key Takeaways from This Session",
    "text": "Key Takeaways from This Session\n\n\n\n\nPython workflows rely on object-oriented structures in addition to functions: Understanding the OOP paradigm makes Python a lot easier!\n\n\nEverything is an object!\n\n\nDuck Typing: If an object has a method, that method can be called regardless of the object type. Caveat being, make sure the arguments (if any) in the method are specified correctly for all objects!\n\n\nPython packages use common methods that make it easy to change between model types without changing a lot of code."
  },
  {
    "objectID": "session4/session4v2_webpage.html#additional-insights",
    "href": "session4/session4v2_webpage.html#additional-insights",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Additional Insights",
    "text": "Additional Insights\n\n\nPredictable APIs enable seamless model switching: Swapping models like LogisticRegression → RandomForestClassifier usually requires minimal code changes.\n\n\nscikit-learn prioritizes interoperability: Its consistent class design integrates with tools like Pipeline, GridSearchCV, and cross_val_score.\n\n\nClass attributes improve model transparency: Access attributes like .coef_, .classes_, and .feature_importances_ for model interpretation and debugging.\n\n\nCustom classes are central to deep learning: Frameworks like PyTorch and TensorFlow require you to define your own model classes by subclassing base models.\n\n\nMixins support modular design: Mixins (e.g., ClassifierMixin) let you add specific functionality without duplicating code."
  },
  {
    "objectID": "session4/session4v2_webpage.html#statsmodels",
    "href": "session4/session4v2_webpage.html#statsmodels",
    "title": "Session 4 – Object-Oriented Programming and Modeling Libraries",
    "section": "Statsmodels",
    "text": "Statsmodels"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#links",
    "href": "session3/DataStructuresDemo_2.html#links",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Links",
    "text": "Links\n \n\nGuide to Python Data Structures\n\n  \n\nCancer Dataset"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#topic-5-what-is-pandas",
    "href": "session3/DataStructuresDemo_2.html#topic-5-what-is-pandas",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Topic 5: What Is Pandas?",
    "text": "Topic 5: What Is Pandas?\nPandas is a powerful open-source data analysis and manipulation library in Python. It provides data structures, primarily the DataFrame and Series, which are optimized for handling and analyzing large datasets efficiently.\n\nData Structures:\n\nSeries: A one-dimensional labeled array, suitable for handling single columns or rows of data.\nDataFrame: A two-dimensional table with labeled axes (rows and columns), much like a spreadsheet or SQL table, allowing you to work with data in rows and columns simultaneously.\n\n\n\nData Manipulation:\n\nPandas has functions for merging, reshaping, and aggregating datasets, which helps streamline data cleaning and preparation.\nIt can handle missing data, making it easy to filter or fill gaps in datasets.\n\n\n\nData Analysis:\n\nPandas provides extensive functionality for descriptive statistics, grouping data, and handling time series.\nIntegrates well with other libraries, making it easy to move data between libraries like NumPy for numerical computations and Matplotlib or Seaborn for visualization."
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#creating-folders-for-project-housekeeping",
    "href": "session3/DataStructuresDemo_2.html#creating-folders-for-project-housekeeping",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Creating folders for project housekeeping",
    "text": "Creating folders for project housekeeping\n\n###Example Folder Structure\n'''\nproject_name/\n    data/\n        raw/\n        processed/\n    scripts/\n    results/\n    logs/\n'''\n\n#To Create folders\n\nimport os\n\n#Defining working directory\nbase_dir = \"G:\\\\dir_demo\"\n\n#Defining Project folder\nproject_name = os.path.join(base_dir, \"my_project\")\n\n# Define the subdirectories\nsubdirs = [\n    \"data/raw\",\n    \"data/processed\",\n    \"scripts\",\n    \"results\",\n    \"logs\",\n]\n\n# Create directories\nfor subdir in subdirs:\n    path = os.path.join(project_name, subdir)\n    os.makedirs(path, exist_ok=True)  #ensures no error if the folder already exists\n    print(f\"Created directory: {path}\")\n\n\n\nCreated directory: G:\\dir_demo\\my_project\\data/raw\nCreated directory: G:\\dir_demo\\my_project\\data/processed\nCreated directory: G:\\dir_demo\\my_project\\scripts\nCreated directory: G:\\dir_demo\\my_project\\results\nCreated directory: G:\\dir_demo\\my_project\\logs\n\n\n\nLoading the Dataset\n\nimport os\nimport pandas as pd\n\n# Load the dataset\ncancer_data = pd.read_csv(\"..\\session3\\example_data\\Cancer_Data.csv\")\n\nprint (type(cancer_data))\n\n\n# Display the first few rows of the dataset\ncancer_data.head()\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\n...\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\nUnnamed: 32\n\n\n\n\n0\n842302\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\nNaN\n\n\n1\n842517\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\nNaN\n\n\n2\n84300903\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\nNaN\n\n\n3\n84348301\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\nNaN\n\n\n4\n84358402\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\nNaN\n\n\n\n\n5 rows × 33 columns"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#viewing-basic-information",
    "href": "session3/DataStructuresDemo_2.html#viewing-basic-information",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Viewing Basic Information",
    "text": "Viewing Basic Information\n\nChecking the Dataset’s Shape\n\n\n.shape returns a tuple with (number of rows, number of columns), which provides a basic overview of the dataset size.\n\n# Display the shape of the dataset\nprint(\"Dataset Shape:\", cancer_data.shape)\n\n\n\nDataset Shape: (569, 33)\n\n\n\nSummarizing Column Information\n\n\n\n.info() lists all columns, their data types, and counts of non-null values, helping identify any columns that may have missing data.\n\n# Display column names, data types, and non-null counts\ncancer_data.info()\n\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 33 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   id                       569 non-null    int64  \n 1   diagnosis                569 non-null    object \n 2   radius_mean              569 non-null    float64\n 3   texture_mean             569 non-null    float64\n 4   perimeter_mean           569 non-null    float64\n 5   area_mean                569 non-null    float64\n 6   smoothness_mean          569 non-null    float64\n 7   compactness_mean         569 non-null    float64\n 8   concavity_mean           569 non-null    float64\n 9   concave points_mean      569 non-null    float64\n 10  symmetry_mean            569 non-null    float64\n 11  fractal_dimension_mean   569 non-null    float64\n 12  radius_se                569 non-null    float64\n 13  texture_se               569 non-null    float64\n 14  perimeter_se             569 non-null    float64\n 15  area_se                  569 non-null    float64\n 16  smoothness_se            569 non-null    float64\n 17  compactness_se           569 non-null    float64\n 18  concavity_se             569 non-null    float64\n 19  concave points_se        569 non-null    float64\n 20  symmetry_se              569 non-null    float64\n 21  fractal_dimension_se     569 non-null    float64\n 22  radius_worst             569 non-null    float64\n 23  texture_worst            569 non-null    float64\n 24  perimeter_worst          569 non-null    float64\n 25  area_worst               569 non-null    float64\n 26  smoothness_worst         569 non-null    float64\n 27  compactness_worst        569 non-null    float64\n 28  concavity_worst          569 non-null    float64\n 29  concave points_worst     569 non-null    float64\n 30  symmetry_worst           569 non-null    float64\n 31  fractal_dimension_worst  569 non-null    float64\n 32  Unnamed: 32              0 non-null      float64\ndtypes: float64(31), int64(1), object(1)\nmemory usage: 146.8+ KB\n\n\n\n\nViewing Column Names\n\n\n.columns lists column headers, while .tolist() converts it into a standard Python list for easier viewing.\n\n# Display column names\nprint(\"Column Names:\", cancer_data.columns.tolist())\n\n\n\nColumn Names: ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32']"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#summary-statistics",
    "href": "session3/DataStructuresDemo_2.html#summary-statistics",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Summary Statistics",
    "text": "Summary Statistics\n\n.describe() generates essential statistics (mean, std, min, max, percentiles) for numeric columns, useful for identifying data distributions.\n\n# Generate summary statistics for numeric columns\ncancer_data.describe()\n\n\n\n\n\n\n\n\n\n\nid\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\n...\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\nUnnamed: 32\n\n\n\n\ncount\n5.690000e+02\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n...\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n0.0\n\n\nmean\n3.037183e+07\n14.127292\n19.289649\n91.969033\n654.889104\n0.096360\n0.104341\n0.088799\n0.048919\n0.181162\n...\n25.677223\n107.261213\n880.583128\n0.132369\n0.254265\n0.272188\n0.114606\n0.290076\n0.083946\nNaN\n\n\nstd\n1.250206e+08\n3.524049\n4.301036\n24.298981\n351.914129\n0.014064\n0.052813\n0.079720\n0.038803\n0.027414\n...\n6.146258\n33.602542\n569.356993\n0.022832\n0.157336\n0.208624\n0.065732\n0.061867\n0.018061\nNaN\n\n\nmin\n8.670000e+03\n6.981000\n9.710000\n43.790000\n143.500000\n0.052630\n0.019380\n0.000000\n0.000000\n0.106000\n...\n12.020000\n50.410000\n185.200000\n0.071170\n0.027290\n0.000000\n0.000000\n0.156500\n0.055040\nNaN\n\n\n25%\n8.692180e+05\n11.700000\n16.170000\n75.170000\n420.300000\n0.086370\n0.064920\n0.029560\n0.020310\n0.161900\n...\n21.080000\n84.110000\n515.300000\n0.116600\n0.147200\n0.114500\n0.064930\n0.250400\n0.071460\nNaN\n\n\n50%\n9.060240e+05\n13.370000\n18.840000\n86.240000\n551.100000\n0.095870\n0.092630\n0.061540\n0.033500\n0.179200\n...\n25.410000\n97.660000\n686.500000\n0.131300\n0.211900\n0.226700\n0.099930\n0.282200\n0.080040\nNaN\n\n\n75%\n8.813129e+06\n15.780000\n21.800000\n104.100000\n782.700000\n0.105300\n0.130400\n0.130700\n0.074000\n0.195700\n...\n29.720000\n125.400000\n1084.000000\n0.146000\n0.339100\n0.382900\n0.161400\n0.317900\n0.092080\nNaN\n\n\nmax\n9.113205e+08\n28.110000\n39.280000\n188.500000\n2501.000000\n0.163400\n0.345400\n0.426800\n0.201200\n0.304000\n...\n49.540000\n251.200000\n4254.000000\n0.222600\n1.058000\n1.252000\n0.291000\n0.663800\n0.207500\nNaN\n\n\n\n\n8 rows × 32 columns\n\n\n\n\n\nUsing value_counts() on a Single Column\nThis method is straightforward if you want to check the frequency distribution of one specific categorical column. Returns a pandas series object\n\n# Count occurrences of each unique value in the 'diagnosis' column\ndiagnosis_counts = cancer_data['diagnosis'].value_counts()\nprint(\"Diagnosis Counts:\\n\", diagnosis_counts)\n\n\n\nDiagnosis Counts:\n diagnosis\nB    357\nM    212\nName: count, dtype: int64\n\n\n\n\nTo see summary statistics grouped by a categorical variable in pandas, you can use the groupby() method along with describe() or specific aggregation functions like mean(), sum(), etc.\n\n# Group by 'diagnosis' and get summary statistics for each group\ngrouped_summary = cancer_data.groupby('diagnosis').mean()\nprint(grouped_summary)\n\n\n#Group by 'diagnosis' and get summary statistics for only one variable\ngrouped_radius_mean = cancer_data.groupby('diagnosis')['radius_mean'].mean()\nprint(grouped_radius_mean)\n\n\n\n                     id  radius_mean  texture_mean  perimeter_mean  \\\ndiagnosis                                                            \nB          2.654382e+07    12.146524     17.914762       78.075406   \nM          3.681805e+07    17.462830     21.604906      115.365377   \n\n            area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\ndiagnosis                                                                  \nB          462.790196         0.092478          0.080085        0.046058   \nM          978.376415         0.102898          0.145188        0.160775   \n\n           concave points_mean  symmetry_mean  ...  texture_worst  \\\ndiagnosis                                      ...                  \nB                     0.025717       0.174186  ...      23.515070   \nM                     0.087990       0.192909  ...      29.318208   \n\n           perimeter_worst   area_worst  smoothness_worst  compactness_worst  \\\ndiagnosis                                                                      \nB                87.005938   558.899440          0.124959           0.182673   \nM               141.370330  1422.286321          0.144845           0.374824   \n\n           concavity_worst  concave points_worst  symmetry_worst  \\\ndiagnosis                                                          \nB                 0.166238              0.074444        0.270246   \nM                 0.450606              0.182237        0.323468   \n\n           fractal_dimension_worst  Unnamed: 32  \ndiagnosis                                        \nB                         0.079442          NaN  \nM                         0.091530          NaN  \n\n[2 rows x 32 columns]\ndiagnosis\nB    12.146524\nM    17.462830\nName: radius_mean, dtype: float64"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#renaming-columns",
    "href": "session3/DataStructuresDemo_2.html#renaming-columns",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Renaming Columns",
    "text": "Renaming Columns\nTo make column names more readable or consistent, you can use rename() to change specific names. Here’s how to rename columns like radius_mean to Radius Mean.\n\n# Rename specific columns for readability. 'old': 'new'\n\nnew_columns={\n    'radius_mean': 'Radius Mean',\n    'texture_mean': 'Texture Mean',\n    'perimeter_mean': 'Perimeter Mean'\n}\n\ncancer_data = cancer_data.rename(columns=new_columns)\n\n# Display the new column names to verify the changes\nprint(\"\\nUpdated Column Names:\", cancer_data.columns.tolist())\n\n\n\n\nUpdated Column Names: ['id', 'diagnosis', 'Radius Mean', 'Texture Mean', 'Perimeter Mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32']"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#missing-values-in-python",
    "href": "session3/DataStructuresDemo_2.html#missing-values-in-python",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Missing values in Python",
    "text": "Missing values in Python\nMissing values are common in data analysis. Python provides multiple ways to represent missing values, including None, np.nan, and pd.NA. Understanding their behavior is crucial for data cleaning, processing, and analysis.\n\nMissing Values in Python: None\n\nDefinition: None is a built-in Python object representing “no value.”\nUse Cases: Works with general Python objects but does not support mathematical operations.\n\n\nx = None\nif x is None:  # Best practice\n    print(\"x is missing\")\n\n\n\nx is missing\n\n\n\n\nIssue with None in arithmetic:\n\ntry: \n    print(x + 1)\nexcept TypeError: \n    print(\"TypeError: Unsupported operand type(s)\")  # TypeError: unsupported operand type(s)\nprint(x==x)\n\n\n\nTypeError: Unsupported operand type(s)\nTrue"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#missing-values-in-numpy-np.nan",
    "href": "session3/DataStructuresDemo_2.html#missing-values-in-numpy-np.nan",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Missing Values in NumPy: np.nan",
    "text": "Missing Values in NumPy: np.nan\n\nnp.nan represents missing values in numerical computations.\nnp.nan is a floating-point value (float64).\nCannot be checked with == because np.nan != np.nan.\n\n\nimport numpy as np\nx = np.nan\nif np.isnan(x):  # Correct way to check for np.nan\n    print(\"x is missing\")\n\n\n\nx is missing\n\n\n\nBehavior in Math Operations\n\n\nprint(x + 10)  # Output: nan\nprint(x == x)  # Output: False\n\n\n\nnan\nFalse"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#missing-values-in-pandas-pd.na",
    "href": "session3/DataStructuresDemo_2.html#missing-values-in-pandas-pd.na",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Missing Values in Pandas: pd.NA",
    "text": "Missing Values in Pandas: pd.NA\n\npd.NA is Pandas’ missing value representation, introduced in Pandas 1.0.\nWorks with nullable data types (Int64, Float64, boolean, string).\nAvoids automatic type conversion (e.g., integers remain integers).\nBehavior in Math Operations\n\n\nimport pandas as pd\nx = pd.NA\nprint(x+1)\n\n\n\n&lt;NA&gt;\n\n\n\n#Checking for missingness \n\n\nx = pd.NA\nif pd.isna(x):  # Correct way\n    print(\"x is missing\")\n\ntry:   \n    if x==pd.na:\n        print(x)\nexcept AttributeError:\n    print('#Incorrect way: if x=pd.NA')\n\nprint(x==x)\n\n\n\nx is missing\n#Incorrect way: if x=pd.NA\n&lt;NA&gt;"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#comparing-none-np.nan-and-pd.na",
    "href": "session3/DataStructuresDemo_2.html#comparing-none-np.nan-and-pd.na",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Comparing None, np.nan, and pd.NA",
    "text": "Comparing None, np.nan, and pd.NA\n\n\n\n\n\n\n\n\n\nFeature\nNone\nnp.nan\npd.NA\n\n\n\n\nType\nNoneType\nfloat64\nSpecial Pandas scalar\n\n\nUse Case\nGeneral Python\nNumPy/Pandas numeric data\nPandas nullable types\n\n\nArithmetic Ops\nFails (None + 1)\nWorks but returns nan\nWorks but returns &lt;NA&gt;\n\n\nComparison (==)\nNone == None → True\nnp.nan == np.nan → False\npd.NA == pd.NA → &lt;NA&gt;\n\n\nCheck Method\nif x is None:\nif np.isnan(x):\nif pd.isna(x):"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#handling-missing-values-in-pandas",
    "href": "session3/DataStructuresDemo_2.html#handling-missing-values-in-pandas",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Handling Missing Values in Pandas",
    "text": "Handling Missing Values in Pandas\n\nimport numpy as np\ndf = pd.DataFrame({\"A\": [1, np.nan, 3, None, pd.NA]})\nprint(df.isna())  # Identifies missing values\nfor x in df.iloc[:, 0]:\n    print(type(x))\n\n\n\n       A\n0  False\n1   True\n2  False\n3   True\n4   True\n&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n&lt;class 'int'&gt;\n&lt;class 'NoneType'&gt;\n&lt;class 'pandas._libs.missing.NAType'&gt;\n\n\n\nFilling Missing Values\n\ndf[\"A\"]=df[\"A\"].fillna(pd.NA).astype('Float64')  # Replaces missing values with pd.NA, and then changes the column type to 'FLoat64, pandas' nullable float datatype.\nfor value in df.iloc[:,0]:\n        print(value)\n        print(type(value))\n\nprint(df[\"A\"].dtype)\n\n\n\n1.0\n&lt;class 'numpy.float64'&gt;\n&lt;NA&gt;\n&lt;class 'pandas._libs.missing.NAType'&gt;\n3.0\n&lt;class 'numpy.float64'&gt;\n&lt;NA&gt;\n&lt;class 'pandas._libs.missing.NAType'&gt;\n&lt;NA&gt;\n&lt;class 'pandas._libs.missing.NAType'&gt;\nFloat64\n\n\n\n\nMake sure you typecast the column as a pandas nullable data type.\n\ndf[\"A\"]=df[\"A\"].astype('float64')  #typecasting as lowercase \"float\" changes all pd.NA back to np.nan because \"float64\" (lowercase) is not supported by pd.NA\nfor value in df.iloc[:,0]:\n        print(value)\n        print(type(value))\n\nprint(df[\"A\"].dtype)\n\n\n\n1.0\n&lt;class 'float'&gt;\nnan\n&lt;class 'float'&gt;\n3.0\n&lt;class 'float'&gt;\nnan\n&lt;class 'float'&gt;\nnan\n&lt;class 'float'&gt;\nfloat64"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#dropping-missing-values",
    "href": "session3/DataStructuresDemo_2.html#dropping-missing-values",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Dropping Missing Values",
    "text": "Dropping Missing Values\n\ndf=df.dropna()\nprint(df[\"A\"])\n# Removes rows with missing values\n\n\n\n0    1.0\n2    3.0\nName: A, dtype: float64"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#best-practices",
    "href": "session3/DataStructuresDemo_2.html#best-practices",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse None for general Python objects.\n\nUse np.nan for numerical missing values in NumPy.\n\nUse pd.NA for missing values in Pandas with nullable data types.\n\nAlways use isna() or isnull() when working with missing data in Pandas."
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#other-pandas-nullable-data-types",
    "href": "session3/DataStructuresDemo_2.html#other-pandas-nullable-data-types",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Other Pandas Nullable Data Types",
    "text": "Other Pandas Nullable Data Types\n\n\n\n\n\n\n\n\nPandas Nullable Data Type\nDescription\nTypical Usage\n\n\n\n\nInt8, Int16, Int32, Int64\nNullable integer types (can hold pd.NA)\nUse when you want integers with missing values\n\n\nFloat32, Float64\nNullable float types (standard floats also support NaN)\nNumeric data with decimals, missing values\n\n\nboolean\nNullable Boolean type (True, False, pd.NA)\nBinary categories with missing info\n\n\nstring\nPandas string data type (nullable)\nText data with potential nulls\n\n\ncategory\nCategorical type (can include NaN or pd.NA)\nCategorical data, efficient storage\n\n\ndatetime64[ns] with pd.NaT\nDatetime with nanosecond precision\nTime series, datetime columns\n\n\ntimedelta64[ns] with pd.NaT\nTimedeltas (differences between datetimes)\nDuration calculations"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#more-on-missing-values",
    "href": "session3/DataStructuresDemo_2.html#more-on-missing-values",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "More on missing values",
    "text": "More on missing values\nTo find missing values, you can use isnull() with sum() to calculate the total number of missing values in each column.\n\n# Count missing values in each column\nmissing_values = cancer_data.isnull().sum()\nprint(\"Missing Values per Column:\")\nprint(missing_values)\n\n\n\nMissing Values per Column:\nid                           0\ndiagnosis                    0\nRadius Mean                  0\nTexture Mean                 0\nPerimeter Mean               0\narea_mean                    0\nsmoothness_mean              0\ncompactness_mean             0\nconcavity_mean               0\nconcave points_mean          0\nsymmetry_mean                0\nfractal_dimension_mean       0\nradius_se                    0\ntexture_se                   0\nperimeter_se                 0\narea_se                      0\nsmoothness_se                0\ncompactness_se               0\nconcavity_se                 0\nconcave points_se            0\nsymmetry_se                  0\nfractal_dimension_se         0\nradius_worst                 0\ntexture_worst                0\nperimeter_worst              0\narea_worst                   0\nsmoothness_worst             0\ncompactness_worst            0\nconcavity_worst              0\nconcave points_worst         0\nsymmetry_worst               0\nfractal_dimension_worst      0\nUnnamed: 32                569\ndtype: int64\n\n\n\nDropping Columns with Excessive Missing Data Since Unnamed: 32 has no data, it can be dropped from the DataFrame using .drop().\n\n# Drop the 'Unnamed: 32' column if it contains no data\ncancer_data = cancer_data.drop(columns=['Unnamed: 32'])\n\n# Verify the column has been dropped\nprint(\"\\nColumns after dropping 'Unnamed: 32':\", cancer_data.columns.tolist())\n\n\n\n\nColumns after dropping 'Unnamed: 32': ['id', 'diagnosis', 'Radius Mean', 'Texture Mean', 'Perimeter Mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#column-selection",
    "href": "session3/DataStructuresDemo_2.html#column-selection",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Column Selection",
    "text": "Column Selection\nSelecting specific columns is essential for focusing on particular aspects of the dataset. Here are some examples of both single and multiple column selections.\n\n# Select the 'diagnosis' column - diagnosis_column will be a series\ndiagnosis_column = cancer_data['diagnosis']\nprint(\"Diagnosis Column:\\n\", diagnosis_column.head())\n\n\n\nDiagnosis Column:\n 0    M\n1    M\n2    M\n3    M\n4    M\nName: diagnosis, dtype: object\n\n\n\nAlternatively, you can select multiple columns.\n\n# Select multiple columns: 'diagnosis', 'radius_mean', and 'area_mean' - selected_columns will be a pandas DataFrame\n\nselected_columns = cancer_data[['diagnosis', 'Radius Mean', 'area_mean']]\nprint(\"Selected Columns:\\n\", selected_columns.head())\n\n\n\nSelected Columns:\n   diagnosis  Radius Mean  area_mean\n0         M        17.99     1001.0\n1         M        20.57     1326.0\n2         M        19.69     1203.0\n3         M        11.42      386.1\n4         M        20.29     1297.0"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#row-selection",
    "href": "session3/DataStructuresDemo_2.html#row-selection",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Row Selection",
    "text": "Row Selection\nSelecting rows based on labels or positions is helpful for inspecting specific data points or subsets.\n\nLabel-Based Indexing with loc\n\n\nloc allows selection based on labels (e.g., column names or index labels) and is particularly useful for data subsets.\n\n# Select rows by labels (assuming integer index here) and specific columns\nselected_rows_labels = cancer_data.loc[0:4, ['diagnosis', 'Radius Mean', 'area_mean']]\nprint(\"Selected Rows with loc:\\n\", selected_rows_labels)\n\n\n\nSelected Rows with loc:\n   diagnosis  Radius Mean  area_mean\n0         M        17.99     1001.0\n1         M        20.57     1326.0\n2         M        19.69     1203.0\n3         M        11.42      386.1\n4         M        20.29     1297.0\n\n\n\n\nInteger-Based Indexing with iloc\n\n\niloc allows selection based purely on integer positions, making it convenient for slicing and position-based operations.\n\n# Select rows by integer position and specific columns\nselected_rows_position = cancer_data.iloc[0:5, [1, 2, 3]]  # Select first 5 rows and columns at position 1, 2, 3\nprint(\"Selected Rows with iloc:\\n\", selected_rows_position)\n\n\n\nSelected Rows with iloc:\n   diagnosis  Radius Mean  Texture Mean\n0         M        17.99         10.38\n1         M        20.57         17.77\n2         M        19.69         21.25\n3         M        11.42         20.38\n4         M        20.29         14.34"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#filtering",
    "href": "session3/DataStructuresDemo_2.html#filtering",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Filtering",
    "text": "Filtering\nFiltering enables you to create subsets of data that match specific conditions. For example, we can filter by diagnosis to analyze only malignant (M) or benign (B) cases.\n\n# Filter rows where 'diagnosis' is \"M\" (Malignant)\nmalignant_cases = cancer_data[cancer_data['diagnosis'] == 'M']\nprint(\"Malignant Cases:\\n\", malignant_cases.head(20))\n\n\n\nMalignant Cases:\n           id diagnosis  Radius Mean  Texture Mean  Perimeter Mean  area_mean  \\\n0     842302         M        17.99         10.38          122.80     1001.0   \n1     842517         M        20.57         17.77          132.90     1326.0   \n2   84300903         M        19.69         21.25          130.00     1203.0   \n3   84348301         M        11.42         20.38           77.58      386.1   \n4   84358402         M        20.29         14.34          135.10     1297.0   \n5     843786         M        12.45         15.70           82.57      477.1   \n6     844359         M        18.25         19.98          119.60     1040.0   \n7   84458202         M        13.71         20.83           90.20      577.9   \n8     844981         M        13.00         21.82           87.50      519.8   \n9   84501001         M        12.46         24.04           83.97      475.9   \n10    845636         M        16.02         23.24          102.70      797.8   \n11  84610002         M        15.78         17.89          103.60      781.0   \n12    846226         M        19.17         24.80          132.40     1123.0   \n13    846381         M        15.85         23.95          103.70      782.7   \n14  84667401         M        13.73         22.61           93.60      578.3   \n15  84799002         M        14.54         27.54           96.73      658.8   \n16    848406         M        14.68         20.13           94.74      684.5   \n17  84862001         M        16.13         20.68          108.10      798.8   \n18    849014         M        19.81         22.15          130.00     1260.0   \n22   8511133         M        15.34         14.26          102.50      704.4   \n\n    smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0           0.11840           0.27760         0.30010              0.14710   \n1           0.08474           0.07864         0.08690              0.07017   \n2           0.10960           0.15990         0.19740              0.12790   \n3           0.14250           0.28390         0.24140              0.10520   \n4           0.10030           0.13280         0.19800              0.10430   \n5           0.12780           0.17000         0.15780              0.08089   \n6           0.09463           0.10900         0.11270              0.07400   \n7           0.11890           0.16450         0.09366              0.05985   \n8           0.12730           0.19320         0.18590              0.09353   \n9           0.11860           0.23960         0.22730              0.08543   \n10          0.08206           0.06669         0.03299              0.03323   \n11          0.09710           0.12920         0.09954              0.06606   \n12          0.09740           0.24580         0.20650              0.11180   \n13          0.08401           0.10020         0.09938              0.05364   \n14          0.11310           0.22930         0.21280              0.08025   \n15          0.11390           0.15950         0.16390              0.07364   \n16          0.09867           0.07200         0.07395              0.05259   \n17          0.11700           0.20220         0.17220              0.10280   \n18          0.09831           0.10270         0.14790              0.09498   \n22          0.10730           0.21350         0.20770              0.09756   \n\n    ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n0   ...         25.38          17.33           184.60      2019.0   \n1   ...         24.99          23.41           158.80      1956.0   \n2   ...         23.57          25.53           152.50      1709.0   \n3   ...         14.91          26.50            98.87       567.7   \n4   ...         22.54          16.67           152.20      1575.0   \n5   ...         15.47          23.75           103.40       741.6   \n6   ...         22.88          27.66           153.20      1606.0   \n7   ...         17.06          28.14           110.60       897.0   \n8   ...         15.49          30.73           106.20       739.3   \n9   ...         15.09          40.68            97.65       711.4   \n10  ...         19.19          33.88           123.80      1150.0   \n11  ...         20.42          27.28           136.50      1299.0   \n12  ...         20.96          29.94           151.70      1332.0   \n13  ...         16.84          27.66           112.00       876.5   \n14  ...         15.03          32.01           108.80       697.7   \n15  ...         17.46          37.13           124.10       943.2   \n16  ...         19.07          30.88           123.40      1138.0   \n17  ...         20.96          31.48           136.80      1315.0   \n18  ...         27.32          30.88           186.80      2398.0   \n22  ...         18.07          19.08           125.10       980.9   \n\n    smoothness_worst  compactness_worst  concavity_worst  \\\n0             0.1622             0.6656           0.7119   \n1             0.1238             0.1866           0.2416   \n2             0.1444             0.4245           0.4504   \n3             0.2098             0.8663           0.6869   \n4             0.1374             0.2050           0.4000   \n5             0.1791             0.5249           0.5355   \n6             0.1442             0.2576           0.3784   \n7             0.1654             0.3682           0.2678   \n8             0.1703             0.5401           0.5390   \n9             0.1853             1.0580           1.1050   \n10            0.1181             0.1551           0.1459   \n11            0.1396             0.5609           0.3965   \n12            0.1037             0.3903           0.3639   \n13            0.1131             0.1924           0.2322   \n14            0.1651             0.7725           0.6943   \n15            0.1678             0.6577           0.7026   \n16            0.1464             0.1871           0.2914   \n17            0.1789             0.4233           0.4784   \n18            0.1512             0.3150           0.5372   \n22            0.1390             0.5954           0.6305   \n\n    concave points_worst  symmetry_worst  fractal_dimension_worst  \n0                0.26540          0.4601                  0.11890  \n1                0.18600          0.2750                  0.08902  \n2                0.24300          0.3613                  0.08758  \n3                0.25750          0.6638                  0.17300  \n4                0.16250          0.2364                  0.07678  \n5                0.17410          0.3985                  0.12440  \n6                0.19320          0.3063                  0.08368  \n7                0.15560          0.3196                  0.11510  \n8                0.20600          0.4378                  0.10720  \n9                0.22100          0.4366                  0.20750  \n10               0.09975          0.2948                  0.08452  \n11               0.18100          0.3792                  0.10480  \n12               0.17670          0.3176                  0.10230  \n13               0.11190          0.2809                  0.06287  \n14               0.22080          0.3596                  0.14310  \n15               0.17120          0.4218                  0.13410  \n16               0.16090          0.3029                  0.08216  \n17               0.20730          0.3706                  0.11420  \n18               0.23880          0.2768                  0.07615  \n22               0.23930          0.4667                  0.09946  \n\n[20 rows x 32 columns]\n\n\n\nYou can also filter based on multiple conditions, such as finding rows where the diagnosis is “M” and radius_mean is greater than 15.\nNote: You can’t use ‘and’ python operator here, because ‘and’ is a keyword for Python’s boolean operations, which work with single True or False values, not arrays or Series.\n\n# Filter for Malignant cases with radius_mean &gt; 15\nlarge_malignant_cases = cancer_data[(cancer_data['diagnosis'] == 'M') & (cancer_data['Radius Mean'] &gt; 15)]\nprint(\"Large Malignant Cases (Radius Mean &gt; 15):\\n\", large_malignant_cases.head())\n\n\n\nLarge Malignant Cases (Radius Mean &gt; 15):\n          id diagnosis  Radius Mean  Texture Mean  Perimeter Mean  area_mean  \\\n0    842302         M        17.99         10.38           122.8     1001.0   \n1    842517         M        20.57         17.77           132.9     1326.0   \n2  84300903         M        19.69         21.25           130.0     1203.0   \n4  84358402         M        20.29         14.34           135.1     1297.0   \n6    844359         M        18.25         19.98           119.6     1040.0   \n\n   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0          0.11840           0.27760          0.3001              0.14710   \n1          0.08474           0.07864          0.0869              0.07017   \n2          0.10960           0.15990          0.1974              0.12790   \n4          0.10030           0.13280          0.1980              0.10430   \n6          0.09463           0.10900          0.1127              0.07400   \n\n   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n0  ...         25.38          17.33            184.6      2019.0   \n1  ...         24.99          23.41            158.8      1956.0   \n2  ...         23.57          25.53            152.5      1709.0   \n4  ...         22.54          16.67            152.2      1575.0   \n6  ...         22.88          27.66            153.2      1606.0   \n\n   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n0            0.1622             0.6656           0.7119                0.2654   \n1            0.1238             0.1866           0.2416                0.1860   \n2            0.1444             0.4245           0.4504                0.2430   \n4            0.1374             0.2050           0.4000                0.1625   \n6            0.1442             0.2576           0.3784                0.1932   \n\n   symmetry_worst  fractal_dimension_worst  \n0          0.4601                  0.11890  \n1          0.2750                  0.08902  \n2          0.3613                  0.08758  \n4          0.2364                  0.07678  \n6          0.3063                  0.08368  \n\n[5 rows x 32 columns]"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#adding-and-modifying-columns",
    "href": "session3/DataStructuresDemo_2.html#adding-and-modifying-columns",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Adding and Modifying Columns",
    "text": "Adding and Modifying Columns\nYou can create new columns in a DataFrame based on calculations using existing columns. For example, we can calculate the area_ratio by dividing area_worst by area_mean.\n\n# Add a new column 'area_ratio' by dividing 'area_worst' by 'area_mean'\ncancer_data['area_ratio'] = cancer_data['area_worst'] / cancer_data['area_mean']\nprint(\"New Column 'area_ratio':\\n\", cancer_data[['area_worst', 'area_mean', 'area_ratio']].head())\n\n\n\nNew Column 'area_ratio':\n    area_worst  area_mean  area_ratio\n0      2019.0     1001.0    2.016983\n1      1956.0     1326.0    1.475113\n2      1709.0     1203.0    1.420615\n3       567.7      386.1    1.470344\n4      1575.0     1297.0    1.214341\n\n\n\nChanging a Value Using .at\n\n\nSuppose you have a DataFrame and want to update the value in the radius_mean column for a particular index.\n\n# Access and print the original value at index 0 and column 'radius_mean'\noriginal_value = cancer_data.at[0, 'Radius Mean']\nprint(\"Original Radius Mean at index 0:\", original_value)\n\n\n# Change the value at index 0 and column 'radius_mean' to 18.5\ncancer_data.at[0, 'Radius Mean'] = 18.5\n\n\n# Verify the updated value\nupdated_value = cancer_data.at[0, 'Radius Mean']\nprint(\"Updated Radius Mean at index 0:\", updated_value)\n\n\n\nOriginal Radius Mean at index 0: 17.99\nUpdated Radius Mean at index 0: 18.5"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#sorting-by-columns",
    "href": "session3/DataStructuresDemo_2.html#sorting-by-columns",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Sorting by Columns",
    "text": "Sorting by Columns\nYou can sort a dataset by columns. Here’s how to sort by diagnosis first and then by area_mean in ascending order.\n\n# Sort by 'diagnosis' first, then by 'area_mean' within each diagnosis group\nsorted_by_diagnosis_area = cancer_data.sort_values(by=['diagnosis', 'area_mean'], ascending=[True, True])\nprint(\"Data sorted by Diagnosis and Area Mean:\\n\", sorted_by_diagnosis_area[['diagnosis', 'area_mean', 'Radius Mean']].head())\n\n\n\nData sorted by Diagnosis and Area Mean:\n     diagnosis  area_mean  Radius Mean\n101         B      143.5        6.981\n539         B      170.4        7.691\n538         B      178.8        7.729\n568         B      181.0        7.760\n46          B      201.9        8.196"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#reordering-columns-to-move-a-column-to-the-end",
    "href": "session3/DataStructuresDemo_2.html#reordering-columns-to-move-a-column-to-the-end",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Reordering Columns to Move a Column to the End",
    "text": "Reordering Columns to Move a Column to the End\nYou might also want to move a specific column to the end of the DataFrame, such as moving area_ratio to the last position.\n\n# Move 'area_ratio' to the end of the DataFrame\ncolumns_reordered = [col for col in cancer_data.columns if col != 'area_ratio'] + ['area_ratio']\ncancer_data_with_area_ratio_last = cancer_data[columns_reordered]\n\n# Display the reordered columns\nprint(\"Data with 'area_ratio' at the end:\\n\", cancer_data_with_area_ratio_last.head())\n\n\n\nData with 'area_ratio' at the end:\n          id diagnosis  Radius Mean  Texture Mean  Perimeter Mean  area_mean  \\\n0    842302         M        18.50         10.38          122.80     1001.0   \n1    842517         M        20.57         17.77          132.90     1326.0   \n2  84300903         M        19.69         21.25          130.00     1203.0   \n3  84348301         M        11.42         20.38           77.58      386.1   \n4  84358402         M        20.29         14.34          135.10     1297.0   \n\n   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0          0.11840           0.27760          0.3001              0.14710   \n1          0.08474           0.07864          0.0869              0.07017   \n2          0.10960           0.15990          0.1974              0.12790   \n3          0.14250           0.28390          0.2414              0.10520   \n4          0.10030           0.13280          0.1980              0.10430   \n\n   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n0  ...          17.33           184.60      2019.0            0.1622   \n1  ...          23.41           158.80      1956.0            0.1238   \n2  ...          25.53           152.50      1709.0            0.1444   \n3  ...          26.50            98.87       567.7            0.2098   \n4  ...          16.67           152.20      1575.0            0.1374   \n\n   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n0             0.6656           0.7119                0.2654          0.4601   \n1             0.1866           0.2416                0.1860          0.2750   \n2             0.4245           0.4504                0.2430          0.3613   \n3             0.8663           0.6869                0.2575          0.6638   \n4             0.2050           0.4000                0.1625          0.2364   \n\n   fractal_dimension_worst  area_ratio  \n0                  0.11890    2.016983  \n1                  0.08902    1.475113  \n2                  0.08758    1.420615  \n3                  0.17300    1.470344  \n4                  0.07678    1.214341  \n\n[5 rows x 33 columns]"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#method-chaining-in-pandas",
    "href": "session3/DataStructuresDemo_2.html#method-chaining-in-pandas",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Method Chaining in Pandas",
    "text": "Method Chaining in Pandas\nIn Pandas, you can chain multiple methods together to create a pipeline.\n\nimport pandas as pd\n\n# Sample DataFrame\ndf = pd.DataFrame({\n    \"name\": [\" Alice \", \"BOB\", \"Charlie\", None],\n    \"score\": [85, 92, None, 74]\n})\n\n# Clean the data using method chaining\nclean_df = (\n    df\n    .dropna()                # Method: drop rows with any NaNs\n    .assign(                 # Method: add or update columns\n        name_clean=lambda d: d[\"name\"].str.strip().str.title()\n    )\n    .sort_values(\"score\", ascending=False)  # Method: sort by score\n)\n\nprint(clean_df)\n\n\n\n      name  score name_clean\n1      BOB   92.0        Bob\n0   Alice    85.0      Alice\n\n\n\nWhy .str.strip() and not just .strip()?\n\n# This works:\ndf[\"name\"].str.strip()\n\n# This does NOT:\ntry:\n    df[\"name\"].strip()  # ❌ AttributeError\nexcept AttributeError: \n    print(\"AttributeError: .strip is used for single strings, not a Series of strings\")\n\n\n\nAttributeError: .strip is used for single strings, not a Series of strings\n\n\n\n\nWhy? - df[\"name\"] is a Series — not a string. - .strip() is a string method that works on single strings. - .str is the accessor that tells pandas: “apply this string method to each element in the Series.”"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#rule-of-thumb",
    "href": "session3/DataStructuresDemo_2.html#rule-of-thumb",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Rule of Thumb:",
    "text": "Rule of Thumb:\n\n\n\n\n\n\n\n\nYou have…\nUse…\nWhy?\n\n\n\n\nA single string\n\"hello\".strip()\nIt’s just Python\n\n\nA Series of strings\ndf[\"col\"].str.strip()\nIt’s pandas, operating on many strings"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#applying-functions-in-pandas",
    "href": "session3/DataStructuresDemo_2.html#applying-functions-in-pandas",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Applying Functions in Pandas",
    "text": "Applying Functions in Pandas\nApplying Functions to Columns\nUsing apply() to Apply Custom Functions\n\nThe .apply() method in pandas lets you apply a custom function to each element in a Series (column) or DataFrame. Here’s how to use it to categorize tumors based on area_mean.\n\n\nExample: Categorizing Tumors by Size Let’s create a custom function to categorize tumors as “Small”, “Medium”, or “Large” based on area_mean.\n\n# Define a custom function to categorize tumors by area_mean\ndef categorize_tumor(size):\n    if size &lt; 500:\n        return 'Small'\n    elif 500 &lt;= size &lt; 1000:\n        return 'Medium'\n    else:\n        return 'Large'\n\n# Apply the function to the 'area_mean' column and create a new column 'tumor_size_category'\ncancer_data['tumor_size_category'] = cancer_data['area_mean'].apply(categorize_tumor)\n\n# Display the new column to verify the transformation\nprint(\"Tumor Size Categories:\\n\", cancer_data[['area_mean', 'tumor_size_category']].head())\n\n\n\nTumor Size Categories:\n    area_mean tumor_size_category\n0     1001.0               Large\n1     1326.0               Large\n2     1203.0               Large\n3      386.1               Small\n4     1297.0               Large\n\n\n\n\nUsing Lambda Functions for Quick Transformations\n\n\nLambda functions are useful for simple, one-line operations. For example, we can use a lambda function to convert diagnosis into numerical codes (0 for Benign, 1 for Malignant).\n\n# Apply a lambda function to classify 'diagnosis' into numerical codes\ncancer_data['diagnosis_code'] = cancer_data['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n\n# Display the new column to verify the transformation\nprint(\"Diagnosis Codes:\\n\", cancer_data[['diagnosis', 'diagnosis_code']].head())\n\n\n\nDiagnosis Codes:\n   diagnosis  diagnosis_code\n0         M               1\n1         M               1\n2         M               1\n3         M               1\n4         M               1"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#applying-multiple-conditions-with-apply",
    "href": "session3/DataStructuresDemo_2.html#applying-multiple-conditions-with-apply",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Applying Multiple Conditions with apply()",
    "text": "Applying Multiple Conditions with apply()\nYou can also use apply() with a lambda function for more complex, multi-condition classifications.\n\nExample: Adding a Column with Risk Levels Suppose we want to create a new column, risk_level, based on both diagnosis and area_mean:\n\n“High Risk” for Malignant tumors with area_mean above 1000.\n“Moderate Risk” for Malignant tumors with area_mean below 1000.\n“Low Risk” for Benign tumors.\n\n\n# Apply a lambda function with multiple conditions to create a 'risk_level' column\ncancer_data['risk_level'] = cancer_data.apply(\n    lambda row: 'High Risk' if row['diagnosis'] == 'M' and row['area_mean'] &gt; 1000 \n    else ('Moderate Risk' if row['diagnosis'] == 'M' else 'Low Risk'), axis=1\n)\n\n# Display the new column to verify the transformation\nprint(\"Risk Levels:\\n\", cancer_data[['diagnosis', 'area_mean', 'risk_level']].head())\n\n#Axis=1 tells the function to apply it to the rows. axis=0 (default) applies function to the columns\n\n\n\nRisk Levels:\n   diagnosis  area_mean     risk_level\n0         M     1001.0      High Risk\n1         M     1326.0      High Risk\n2         M     1203.0      High Risk\n3         M      386.1  Moderate Risk\n4         M     1297.0      High Risk"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#when-to-apply-axis",
    "href": "session3/DataStructuresDemo_2.html#when-to-apply-axis",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "When to apply axis=",
    "text": "When to apply axis=\n\n\n\n\n\n\n\n\nYou’re applying to…\nUse .apply() on…\nDo you need axis?\n\n\n\n\nA single column (Series)\ndf['col'].apply(func)\nNo\n\n\nMultiple columns (row-wise)\ndf.apply(func, axis=1)\nYes (axis=1)\n\n\nColumn-wise (less common)\ndf.apply(func) or axis=0\nOptional (default is 0)"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#to-export-a-pandas-dataframe-to-csv-or-xlsx",
    "href": "session3/DataStructuresDemo_2.html#to-export-a-pandas-dataframe-to-csv-or-xlsx",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "To export a Pandas dataframe to CSV or XLSX",
    "text": "To export a Pandas dataframe to CSV or XLSX\n\n# Export to CSV\n'''\n\ndf.to_csv('/path/to/directory/example.csv', index=False)  # index=False excludes the row indices\n\n\n'''\n#Export to xlsx\n\n'''\n\ndf.to_excel('/path/to/directory/example.xlsx', index=False)\n\n'''\n\n\"\\n\\ndf.to_excel('/path/to/directory/example.xlsx', index=False)\\n\\n\""
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#great_tables-for-table-generation",
    "href": "session3/DataStructuresDemo_2.html#great_tables-for-table-generation",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "great_tables for table generation",
    "text": "great_tables for table generation\nYou can use the great_tables Python module from the great-tables package to explore and display data from a dataset in a clean and interactive format.\n\nWe’ll load the data, summarize it, and then build styled tables using great_tables."
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#load-and-inspect-the-data",
    "href": "session3/DataStructuresDemo_2.html#load-and-inspect-the-data",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Load and Inspect the Data",
    "text": "Load and Inspect the Data\n\nimport pandas as pd\n\n# Load the dataset\ndf = pd.read_csv(\"..\\session3\\example_data\\Cancer_Data.csv\")\n\n# Drop unnamed column\ndf = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n\n# Show the shape and first few rows\ndf.shape, df.head()\n\n\n\n((569, 32),\n          id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n 0    842302         M        17.99         10.38          122.80     1001.0   \n 1    842517         M        20.57         17.77          132.90     1326.0   \n 2  84300903         M        19.69         21.25          130.00     1203.0   \n 3  84348301         M        11.42         20.38           77.58      386.1   \n 4  84358402         M        20.29         14.34          135.10     1297.0   \n \n    smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n 0          0.11840           0.27760          0.3001              0.14710   \n 1          0.08474           0.07864          0.0869              0.07017   \n 2          0.10960           0.15990          0.1974              0.12790   \n 3          0.14250           0.28390          0.2414              0.10520   \n 4          0.10030           0.13280          0.1980              0.10430   \n \n    ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n 0  ...         25.38          17.33           184.60      2019.0   \n 1  ...         24.99          23.41           158.80      1956.0   \n 2  ...         23.57          25.53           152.50      1709.0   \n 3  ...         14.91          26.50            98.87       567.7   \n 4  ...         22.54          16.67           152.20      1575.0   \n \n    smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n 0            0.1622             0.6656           0.7119                0.2654   \n 1            0.1238             0.1866           0.2416                0.1860   \n 2            0.1444             0.4245           0.4504                0.2430   \n 3            0.2098             0.8663           0.6869                0.2575   \n 4            0.1374             0.2050           0.4000                0.1625   \n \n    symmetry_worst  fractal_dimension_worst  \n 0          0.4601                  0.11890  \n 1          0.2750                  0.08902  \n 2          0.3613                  0.08758  \n 3          0.6638                  0.17300  \n 4          0.2364                  0.07678  \n \n [5 rows x 32 columns])"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#preview-of-dataset-using-great_tables",
    "href": "session3/DataStructuresDemo_2.html#preview-of-dataset-using-great_tables",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Preview of dataset using great_tables",
    "text": "Preview of dataset using great_tables\n\nfrom great_tables import GT\n\n# Select a subset of the columns for preview\npreview_df = df[['id', 'diagnosis', 'radius_mean', 'texture_mean', 'area_mean']].head(5)\n\nGT(preview_df).tab_header(\n    title=\"Breast Cancer Diagnosis Preview\",\n    subtitle=\"Selected features from the first 5 records\"\n).fmt_number(columns=[\"radius_mean\", \"texture_mean\", \"area_mean\"], decimals=2)\n\n\n\n\n\n\n\n\n\nBreast Cancer Diagnosis Preview\n\n\nSelected features from the first 5 records\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\narea_mean\n\n\n\n\n842302\nM\n17.99\n10.38\n1,001.00\n\n\n842517\nM\n20.57\n17.77\n1,326.00\n\n\n84300903\nM\n19.69\n21.25\n1,203.00\n\n\n84348301\nM\n11.42\n20.38\n386.10\n\n\n84358402\nM\n20.29\n14.34\n1,297.00"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#enhance-the-table-with-styling",
    "href": "session3/DataStructuresDemo_2.html#enhance-the-table-with-styling",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Enhance the Table with Styling",
    "text": "Enhance the Table with Styling\nLet’s add conditional formatting to highlight larger tumor areas.\n\nGT(preview_df).tab_header(\n    title=\"Styled Cancer Data Table\",\n    subtitle=\"With conditional formatting on tumor area\"\n).fmt_number(columns=[\"radius_mean\", \"texture_mean\", \"area_mean\"], decimals=2\n).data_color(\n    columns=\"area_mean\",\n    palette=[\"blue\", \"red\"]\n)\n\n\n\n\n\n\n\n\n\nStyled Cancer Data Table\n\n\nWith conditional formatting on tumor area\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\narea_mean\n\n\n\n\n842302\nM\n17.99\n10.38\n1,001.00\n\n\n842517\nM\n20.57\n17.77\n1,326.00\n\n\n84300903\nM\n19.69\n21.25\n1,203.00\n\n\n84348301\nM\n11.42\n20.38\n386.10\n\n\n84358402\nM\n20.29\n14.34\n1,297.00"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#group-statistics-by-diagnosis",
    "href": "session3/DataStructuresDemo_2.html#group-statistics-by-diagnosis",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "Group Statistics by Diagnosis",
    "text": "Group Statistics by Diagnosis\nLet’s summarize average values by diagnosis group (malignant vs. benign).\n\nsummary_df = df.groupby(\"diagnosis\")[[\"radius_mean\", \"texture_mean\", \"area_mean\"]].mean().reset_index()\n\nGT(summary_df).tab_header(\n    title=\"Group-wise Summary\",\n    subtitle=\"Mean values for radius, texture, and area by diagnosis\"\n).fmt_number(columns=[\"radius_mean\", \"texture_mean\", \"area_mean\"], decimals=2)\n\n\n\n\n\n\n\n\n\nGroup-wise Summary\n\n\nMean values for radius, texture, and area by diagnosis\n\n\ndiagnosis\nradius_mean\ntexture_mean\narea_mean\n\n\n\n\nB\n12.15\n17.91\n462.79\n\n\nM\n17.46\n21.60\n978.38"
  },
  {
    "objectID": "session3/DataStructuresDemo_2.html#you-try",
    "href": "session3/DataStructuresDemo_2.html#you-try",
    "title": "Session 3: Intro to Pandas and great_tables",
    "section": "You Try!",
    "text": "You Try!\nNavigate to the follow-along file and try the practice problems!"
  }
]