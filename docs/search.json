[
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html",
    "href": "session4_newMLDemo/session4v2_webpage.html",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "",
    "text": "Scikit-learn Documentation\n\nIntroduction to OOP in Python (Real Python)\n\nPlotnine Reference\nSeaborn Reference"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#pre-reading-for-this-session",
    "href": "session4_newMLDemo/session4v2_webpage.html#pre-reading-for-this-session",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "",
    "text": "Scikit-learn Documentation\n\nIntroduction to OOP in Python (Real Python)\n\nPlotnine Reference\nSeaborn Reference"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#note-the-tutorial-below-is-the-same-information-well-be-covering-in-session-4",
    "href": "session4_newMLDemo/session4v2_webpage.html#note-the-tutorial-below-is-the-same-information-well-be-covering-in-session-4",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Note: The tutorial below is the same information we‚Äôll be covering in session 4!",
    "text": "Note: The tutorial below is the same information we‚Äôll be covering in session 4!"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#session-overview",
    "href": "session4_newMLDemo/session4v2_webpage.html#session-overview",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Session Overview",
    "text": "Session Overview\nIn this session, we‚Äôll explore how Python‚Äôs object-oriented nature affects our modeling workflows. \nTopics:\n\n\n\nIntro to OOP and how it makes modeling in Python different from R\n\n\nBuilding and extending classes using inheritance and mixins\n\n\nApplying OOP to machine learning through demos with scikit-learn\n\n\n\nCreating and using models\n\n\nPlotting data with plotnine and seaborn"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#why-python",
    "href": "session4_newMLDemo/session4v2_webpage.html#why-python",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Why Python? üêç",
    "text": "Why Python? üêç\n\n\n\nR: Built by Statisticians for Statisticians\n\nExcels at:\n\nStatistical analysis and modeling\n\nClean outputs and tables from models\nBeautiful data visualizations with simple code\n\n\n\n\n\nPython: General-Purpose Language\n\nExcels at:\n\nMachine Learning, Neural Networks & Deep Learning (scikit-learn, PyTorch, TensorFlow)\n\nImage & Genomic Data Analysis (scikit-image, biopython, scanpy)\nSoftware & Command Line Interfaces, Web Scraping, Automation\n\n\n\n\n\nPython‚Äôs broader ecosystem makes it the go-to language in domains like AI, bioinformatics, data engineering, and computational biology.\n\nNote: Packages like rpy2 and reticulate make it possible to use both R and Python in the same project, but those are beyond the scope of this course.\nA primer on reticulate is available here: https://www.r-bloggers.com/2022/04/getting-started-with-python-using-r-and-reticulate/"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#programming-styles-r-vs-python",
    "href": "session4_newMLDemo/session4v2_webpage.html#programming-styles-r-vs-python",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Programming Styles: R vs Python",
    "text": "Programming Styles: R vs Python\n In the first session, we talked briefly about functional vs object-oriented programming:\n\n\nFunctional programming: focuses on functions as the primary unit of code  Object-oriented programming: uses objects with attached attributes(data) and methods(behaviors) \n\n\nR leans heavily on the functional paradigm ‚Äî you pass data into functions and get back results, in most cases without altering the original data. Functions and pipes (%&gt;%) dominate most workflows.\nIn Python, everything is an object, even basic things like lists, strings, and dataframes. A lot of ‚Äòfunctions‚Äô are instead written as object-associated methods. Some of these methods modify the objects in-place by altering their attributes. Understanding how this works is key to using Python effectively!\n\n\nYou‚Äôve already seen this object-oriented style in Sessions 2 and 3 ‚Äî you create objects like lists or dataframes, then call methods on them like .append() or .sort_values(). In python, instead of piping, we sometimes chain methods together."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#modeling-in-python",
    "href": "session4_newMLDemo/session4v2_webpage.html#modeling-in-python",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Modeling in Python",
    "text": "Modeling in Python\nPython absolutely uses functions‚Äîjust like R! They‚Äôre helpful for data transformation, wrangling, and automation tasks like looping and parallelization. \nBut when it comes to modeling, libraries are designed around classes: blueprints for creating objects that store data (attributes) and define behaviors (methods). \n\nscikit-learn is great for getting started‚Äîeverything follows a simple, consistent OOP interface. Its API is also consistant with other modeling packages, like xgboost and scvi-tools.\nscikit-survival is built on top of scikit-learn. https://scikit-survival.readthedocs.io/en/stable/user_guide/00-introduction.html is a good tutorial for it.\nPyTorch and TensorFlow are essential if you go deeper into neural networks or custom models‚Äîyou‚Äôll define your own model classes with attributes and methods, but the basic structure is similar to scikit-learn.\n\nstatsmodels is an alternative to scikit-learn for statistical analyses and has R-like syntax and outputs. It‚Äôs a bit more complex than scikit-learn and a bit less consistant with other packages in the python ecosystem. https://wesmckinney.com/book/modeling is a good tutorial for statsmodels.\n\n\nüí° To work effectively in Python, especially for tasks involving modeling or model training, it helps to think in terms of objects and classes, not just functions."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#why-does-oop-matter-in-python-modeling",
    "href": "session4_newMLDemo/session4v2_webpage.html#why-does-oop-matter-in-python-modeling",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Why Does OOP Matter in Python Modeling?",
    "text": "Why Does OOP Matter in Python Modeling?\nIn Python modeling frameworks:\n\n\n\nModels are instances of classes\n\n\nYou call methods like .fit(), .predict(), .score()\n\n\nInternal model details like coefficients or layers are stored as attributes\n\n\n\nThis makes model behavior consistent between model classes and even libraries. It also simplifies creating/using pre-trained models: both the architecture and learned weights are bundled into a single object with expected built-in methods like .predict() or .fine_tune().\n\n\nInstead of having a separate results object, like in R, you would retrieve your results by accessing an attribute or using a method that is attached to the model object itself.\n\n\n We‚Äôll focus on scikit-learn in this session, but these ideas carry over to other libraries like xgboost, statsmodels, and PyTorch."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#key-oop-principles-recap",
    "href": "session4_newMLDemo/session4v2_webpage.html#key-oop-principles-recap",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Key OOP Principles (Recap)",
    "text": "Key OOP Principles (Recap)\nIn OOP, code is structured around objects (as opposed to functions). This paradigm builds off the following principles:\n\n\nEncapsulation: Bundling data and methods together in a single unit.\n\nA StandardScaler object stores mean and variance data and has .fit() and .transform() methods\n\n\n\n\n\nInheritance: Creating new classes based on existing ones.\n\nsklearn.LinearRegression inherits attributes and methods from a general regression model class.\n\n\n\n\n\n\nAbstraction: Hiding implementation details and exposing only essential functionality.\n\ne.g., .fit() works the same way from the outside, regardless of model complexity\n\n\n\n\n\n\nPolymorphism: Objects of different types can be treated the same way if they implement the same methods.\n\nPython‚Äôs duck typing:\n\nü¶Ü ‚ÄúIf it walks like a duck and quacks like a duck, then it must be a duck.‚Äù ü¶Ü\n\nex: If different objects all have a .summarize() method, we can loop over them and call .summarize() without needing to check their class. As long as the method exists, Python will know what to do.\nThis lets us easily create pipelines that can work for many types of models.\n\n\n\n\nWe won‚Äôt cover pipelines here, but they are worth looking into!"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#classes-and-objects",
    "href": "session4_newMLDemo/session4v2_webpage.html#classes-and-objects",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Classes and Objects",
    "text": "Classes and Objects\nClasses are blueprints for creating objects. Each object contains:\n\n\n\nAttributes (data): model coefficients, class labels\n\n\nMethods (behaviors): .fit(), .predict()\n\n\nüëâ To Get the class of an object, use:\n\ntype(object) # Returns the type of the object\n\nüëâ To check if an object is an instance of a particular class, use:\n\nisinstance(object, class)  # Returns True if `object` is an instance of `class`.\n\n\n\nKnowing what class an object belongs to helps us understand what methods and attributes it provides."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#base-classes",
    "href": "session4_newMLDemo/session4v2_webpage.html#base-classes",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Base Classes",
    "text": "Base Classes\nA base class (or parent class) serves as a template for creating objects. Other classes can inherit from it to reuse its properties and methods.\nClasses are defined using the class keyword, and their structure is specified using an __init__() method for initialization.\n\nFor example, we can define a class called Dog and give it attributes that store data about a given dog and methods that represent behaviors an object of the Dog class can perform. We can also edit the special or ‚Äúdunder‚Äù methods (short for double underscore) that define how objects behave in certain contexts.\n\nclass Dog: ## begin class definition\n    def __init__(self, name, breed): ## define init method\n        self.name = name ## add attributes\n        self.breed = breed\n\n    def speak(self): ## add methods\n        return f\"{self.name} says woof!\"\n\n    def __str__(self): # __str__(self) tells python what to display when an object is printed\n        return f\"Our dog {self.name}\"\n\n    def __repr__(self): # add representation to display when dog is called in console\n        return f\"Dog(name={self.name!r}, breed={self.breed!r})\""
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#creating-a-dog",
    "href": "session4_newMLDemo/session4v2_webpage.html#creating-a-dog",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Creating a dog",
    "text": "Creating a dog\nCreating an instance of the Dog class lets us model a particular dog:\n\nbuddy = Dog(\"Buddy\", \"Golden Retriever\")\nprint(f\"Buddy is an object of class {type(buddy)}\")\n\nBuddy is an object of class &lt;class '__main__.Dog'&gt;\n\n\n\n\nWe set the value of the attributes [name and breed], which are then stored as part of the buddy object\n\n\nWe can use any methods defined in the Dog class on buddy\n\n\n\n## if we want to see what kind of dog our dog is\n## we can call buddy's attributes\nprint(f\"Our dog {buddy.name} is a {buddy.breed}.\")\n\n## we can also call any Dog methods\nprint(buddy.speak())  \n\n## including special methods\nbuddy ## displays what was in the __repr__() method\n\nOur dog Buddy is a Golden Retriever.\nBuddy says woof!\n\n\nDog(name='Buddy', breed='Golden Retriever')\n\n\nNote: For python methods, the self argument is assumed to be passed and therefore we do not put anything in the parentheses when calling .speak(). For attributes, we do not put () at all."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#derived-child-classes",
    "href": "session4_newMLDemo/session4v2_webpage.html#derived-child-classes",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Derived (Child) Classes",
    "text": "Derived (Child) Classes\nDerived/child classes build on base classes using the principle of inheritence. \nNow that we have a Dog class, we can build on it to create a specialized GuardDog class.\n\nclass GuardDog(Dog):  # GuardDog inherits from Dog\n    def __init__(self, name, breed, training_level): ## in addition to name and breed, we can \n        # define a training level. \n        # Call the parent (Dog) class's __init__ method\n        super().__init__(name, breed)\n        self.training_level = training_level  # New attribute for GuardDog that stores the \n        # training level for the dog\n\n    def guard(self): ## checks if the training level is &gt; 5 and if not says train more\n        if self.training_level &gt; 5:\n            return f\"{self.name} is guarding the house!\"\n        else:\n            return f\"{self.name} needs more training before guarding.\"\n    \n    def train(self): # modifies the training_level attribute to increase the dog's training level\n        self.training_level = self.training_level + 1\n        return f\"Training {self.name}. {self.name}'s training level is now {self.training_level}\"\n\n# Creating an instance of GuardDog\nrex = GuardDog(\"Rex\", \"German Shepherd\", training_level= 5)\n\n\nNow that we have a dog (rex), we can call on any of the methods/attributes introduced in the Dog class as well as the new GuardDog class.\nUsing methods from the base class:\n\nprint(rex.speak())\nrex\n\nRex says woof!\n\n\nDog(name='Rex', breed='German Shepherd')\n\n\n\nUsing a method from the child class:\n\nprint(f\"{rex.name}'s training level is {rex.training_level}.\")\nprint(rex.guard()) \n\nRex's training level is 5.\nRex needs more training before guarding.\n\n\n. . .\nThis is the power of inheritance‚Äîwe don‚Äôt have to rewrite everything from scratch!\n\nUnlike standalone functions, methods in Python often update objects in-place‚Äîmeaning they modify the object itself rather than returning a new one.\nWe can use the .train() method to increase rex‚Äôs training level.\n\nprint(rex.train())\n\nTraining Rex. Rex's training level is now 6\n\n\n. . .\n\nNow if we check,\n\nprint(f\"{rex.name}'s training level is {rex.training_level}.\")\nprint(rex.guard()) \n\nRex's training level is 6.\nRex is guarding the house!\n\n\n\n. . .\nAs with Rex, child classes inherit all attributes (.name and .breed) and methods (.speak() __repr__()) from parent classes. They can also have new methods (.train())."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#mixins",
    "href": "session4_newMLDemo/session4v2_webpage.html#mixins",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Mixins",
    "text": "Mixins\nA mixin is a special kind of class designed to add functionality to another class. Unlike base classes, mixins aren‚Äôt used alone.\n\nFor example, scikit-learn uses mixins like:\n- sklearn.base.ClassifierMixin (adds classifier-specific methods)\n- sklearn.base.RegressorMixin (adds regression-specific methods)\nwhich it adds to the BaseEstimator class to add functionality.  \nTo finish up our dog example, we are going to define a mixin class that adds learning tricks to the base Dog class and use it to create a new class called SmartDog.\n\n\nWhen creating a mixin class, we let the other base classes carry most of the initialization\n\nclass TrickMixin: ## mixin that will let us teach a dog tricks\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)  # Ensures proper initialization in multi inheritance\n        self.tricks = []  # Add attribute to store tricks\n\n## add trick methods\n    def learn_trick(self, trick):\n        \"\"\"Teaches the dog a new trick.\"\"\"\n        if trick not in self.tricks:\n            self.tricks.append(trick)\n            return f\"{self.name} learned a new trick: {trick}!\"\n        return f\"{self.name} already knows {trick}!\"\n\n    def perform_tricks(self):\n        \"\"\"Returns a list of tricks the dog knows.\"\"\"\n        if self.tricks:\n            return f\"{self.name} can perform: {', '.join(self.tricks)}.\"\n        return f\"{self.name} hasn't learned any tricks yet.\"\n\n## note: the TrickMixin class is not a standalone class!\n\n\nBy including both Dog and TrickMixin as base classes, we give objects of class SmartDog the ability to speak and learn tricks!\n\nclass SmartDog(Dog, TrickMixin):\n    def __init__(self, name, breed):\n        super().__init__(name, breed)  # Initialize Dog class\n        TrickMixin.__init__(self)  # Initialize TrickMixin separately\n\n# a SmartDog object can use methods from both parent object `Dog` and mixin `TrickMixin`.\nmy_smart_dog = SmartDog(\"Buddy\", \"Border Collie\")\nprint(my_smart_dog.speak()) \n\nBuddy says woof!\n\n\n\n\nprint(my_smart_dog.learn_trick(\"Sit\"))  \nprint(my_smart_dog.learn_trick(\"Roll Over\")) \nprint(my_smart_dog.learn_trick(\"Sit\"))  \n\nBuddy learned a new trick: Sit!\nBuddy learned a new trick: Roll Over!\nBuddy already knows Sit!\n\n\n\n\nprint(my_smart_dog.perform_tricks()) \n\nBuddy can perform: Sit, Roll Over."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#duck-typing",
    "href": "session4_newMLDemo/session4v2_webpage.html#duck-typing",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Duck Typing",
    "text": "Duck Typing\n\nü¶Ü ‚ÄúIf it quacks like a duck and walks like a duck, it‚Äôs a duck.‚Äù ü¶Ü\n\nPython‚Äôs duck typing makes our lives a lot easier, and is one of the main benefits of methods over functions:\n\n\nRepurposing old code - methods by the same name work the same for different model types\n\n\nNot necessary to check types before using methods - methods are assumed to work on the object they‚Äôre attached to\n\n\nWe can demonstrate this by defining two new base classes that are different than Dog but also have a speak() method.\n. . .\n\nclass Human:\n    def __init__(self, name):\n        self.name = name\n\n    def speak(self):\n        return f\"{self.name} says hello!\"\n\nclass Parrot:\n    def __init__(self, name):\n        self.name = name\n\n    def speak(self):\n        return f\"{self.name} says squawk!\""
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#duck-typing-in-action",
    "href": "session4_newMLDemo/session4v2_webpage.html#duck-typing-in-action",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Duck Typing in Action",
    "text": "Duck Typing in Action\nEven though Dog, Human and Parrot are entirely different classes‚Ä¶\n\ndef call_speaker(obj):\n    print(obj.speak())\n\ncall_speaker(Dog(\"Fido\", \"Labrador\"))\ncall_speaker(Human(\"Alice\"))\ncall_speaker(Parrot(\"Polly\"))\n\nFido says woof!\nAlice says hello!\nPolly says squawk!\n\n\n. . .\nThey all implement .speak(), so Python treats them the same!\nIn the context of our work, this would allow us to make a pipeline using models from different libraries that have the same methods.\n\nWhile our dog example was very simple, this is the same way that model classes work in python!"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#example-oop-in-machine-learning-and-modeling",
    "href": "session4_newMLDemo/session4v2_webpage.html#example-oop-in-machine-learning-and-modeling",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Example: OOP in Machine Learning and Modeling",
    "text": "Example: OOP in Machine Learning and Modeling\nMachine learning models in Python are implemented as classes.\n\n\n\nWhen you create a model, you‚Äôre instantiating an object of a predefined class (e.g., LogisticRegression()).\n\n\nThat model has attributes (parameters, coefficients) and methods (like .fit() and .predict()).\n\n\nFor example LogisticRegression is a model class that inherits from SparseCoefMixin and BaseEstimator.\nclass LogisticRegression(LinearClassifierMixin, SparseCoefMixin, BaseEstimator):\nTo perform logistic regression, we create an instance of the LogisticRegression class.\n## Example: \nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()  # Creating an instance of the LogisticRegression class\nmodel.fit(X_train, y_train)   # Calling a method to train the model\npredictions = model.predict(X_test)  # Calling a method to make predictions\ncoefs = model.coef_ # Access model coefficients using attribute"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#key-benefits-of-oop-in-machine-learning",
    "href": "session4_newMLDemo/session4v2_webpage.html#key-benefits-of-oop-in-machine-learning",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Key Benefits of OOP in Machine Learning",
    "text": "Key Benefits of OOP in Machine Learning\n\nEncapsulation ‚Äì Models store parameters and methods inside a single object.\n\nInheritance ‚Äì New models can build on base models, reusing existing functionality.\n\nAbstraction ‚Äì .fit() should work as expected, regardless of complexity of underlying implimentation.\nPolymorphism (Duck Typing) ‚Äì Different models share the same method names (.fit(), .predict()), making them easy to use interchangeably, particularly in analysis pipelines.\n\nUnderstanding base classes and mixins is especially important when working with deep learning frameworks like PyTorch and TensorFlow, which require us to create our own model classes."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#mini-project-classifying-penguins-with-scikit-learn",
    "href": "session4_newMLDemo/session4v2_webpage.html#mini-project-classifying-penguins-with-scikit-learn",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "üêß Mini Project: Classifying Penguins with scikit-learn",
    "text": "üêß Mini Project: Classifying Penguins with scikit-learn\nNow that you understand classes and data structures in Python, let‚Äôs apply that knowledge to classify penguin species using two features:\n\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nWe‚Äôll explore:\n\n\nUnsupervised learning with K-Means clustering (model doesn‚Äôt ‚Äòknow‚Äô y)\n\n\nSupervised learning with a k-NN classifier (model trained w/ y information)\n\n\nAll scikit-learn models are designed to have\n\n\nCommon Methods:\n\n\n\n.fit() ‚Äî Train the model\n\n\n.predict() ‚Äî Make predictions\n\n\n\nCommon Attributes:\n\n\n.classes_, .n_clusters_, etc.\n\n\n\n\n\nThis is true of the scikit-survival package too!"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#import-libraries",
    "href": "session4_newMLDemo/session4v2_webpage.html#import-libraries",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Import Libraries",
    "text": "Import Libraries\nBefore any analysis, we must import the necessary libraries.\nFor large libraries like scikit-learn, PyTorch, or TensorFlow, we usually do not import the entire package. Instead, we selectively import the classes and functions we need.\n\n\nClasses\n- StandardScaler ‚Äî for feature scaling\n- KNeighborsClassifier ‚Äî for supervised k-NN classification\n- KMeans ‚Äî for unsupervised clustering\n\n\nüî§ Naming Tip:\n- CamelCase = Classes\n- snake_case = Functions\n\n\nFunctions\n- train_test_split() ‚Äî to split data into training and test sets\n- accuracy_score() ‚Äî to evaluate classification accuracy\n- classification_report() ‚Äî to print precision, recall, F1 (balance of precision and recall), Support (number of true instances per class) - adjusted_rand_score() ‚Äî to evaluate clustering performance"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#import-libraries-1",
    "href": "session4_newMLDemo/session4v2_webpage.html#import-libraries-1",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Import Libraries",
    "text": "Import Libraries\n\n## imports\nimport pandas as pd\nimport numpy as np\n\nfrom plotnine import *\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom great_tables import GT\n\n## sklearn imports\n\n## import classes\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\n\n## import functions\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, adjusted_rand_score"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#data-preparation",
    "href": "session4_newMLDemo/session4v2_webpage.html#data-preparation",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Data Preparation",
    "text": "Data Preparation\n\n# Load the Penguins dataset\npenguins = sns.load_dataset(\"penguins\").dropna()\n\n# Make a summary table for the penguins dataset, grouping by species. \nsummary_table = penguins.groupby(\"species\").agg({\n    \"bill_length_mm\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"bill_depth_mm\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"sex\": lambda x: x.value_counts().to_dict()  # Count of males and females\n})\n\n# Round numeric values to 1 decimal place (excluding the 'sex' column)\nfor col in summary_table.columns:\n    if summary_table[col].dtype in [float, int]:\n        summary_table[col] = summary_table[col].round(1)\n\n# Display the result\ndisplay(summary_table)\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nsex\n\n\n\nmean\nstd\nmin\nmax\nmean\nstd\nmin\nmax\n&lt;lambda&gt;\n\n\nspecies\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n38.8\n2.7\n32.1\n46.0\n18.3\n1.2\n15.5\n21.5\n{'Male': 73, 'Female': 73}\n\n\nChinstrap\n48.8\n3.3\n40.9\n58.0\n18.4\n1.1\n16.4\n20.8\n{'Female': 34, 'Male': 34}\n\n\nGentoo\n47.6\n3.1\n40.9\n59.6\n15.0\n1.0\n13.1\n17.3\n{'Male': 61, 'Female': 58}"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#data-visualization",
    "href": "session4_newMLDemo/session4v2_webpage.html#data-visualization",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Data Visualization",
    "text": "Data Visualization\nTo do visualization, we can use either seaborn or plotnine. plotnine mirrors ggplot2 syntax from R and is great for layered grammar-of-graphics plots, while seaborn seaborn is more convienient if you want to put multiple plots on the same figure. \n\nPlotting with Plotnine vs Seaborn\n\n\nPlotnine (like ggplot2 in R) The biggest differences between plotnine and ggplot2 syntax are:\n\n\nWith plotnine the whole call is wrapped in () parentheses\n\n\nVariables are called with strings (\"\" are needed!)\n\n\nIf you don‚Äôt use from plotnine import *, you will need to import each individual function you plan to use!\n\n\n\nSeaborn (base matplotlib + enhancements)\n\n\nDesigned for quick, polished plots\n\n\nWorks well with pandas DataFrames or NumPy arrays\n\n\nIntegrates with matplotlib for customization\n\n\nGood for things like decision boundaries or heatmaps\n\n\nHarder to customize than plotnine plots"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#scatterplot-with-plotnine",
    "href": "session4_newMLDemo/session4v2_webpage.html#scatterplot-with-plotnine",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Scatterplot with plotnine",
    "text": "Scatterplot with plotnine\nTo take a look at the distribution of our species by bill length and bill depth before clustering‚Ä¶\n\nplot1 = (ggplot(penguins, aes(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"species\"))\n + geom_point()\n + ggtitle(\"Penguin Species\")\n + theme_bw())\n\ndisplay(plot1)"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#scatterplot-with-seaborn",
    "href": "session4_newMLDemo/session4v2_webpage.html#scatterplot-with-seaborn",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Scatterplot with seaborn",
    "text": "Scatterplot with seaborn\nWe can make a similar plot in seaborn. This time, let‚Äôs include sex by setting the point style\n\n# Create the figure and axes obects\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Create a plot \nsns.scatterplot(\n    data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\",\n    hue=\"species\", ## hue = fill\n    style=\"sex\",  ## style = style of dots\n    palette=\"Set2\", ## sets color pallet\n    edgecolor=\"black\", s=300, ## line color and point size \n    ax=ax              ## Draw plot on ax      \n)\n\n# Use methods on ax to set title, labels\nax.set_title(\"Penguin Bill Length vs Depth by Species\")\nax.set_xlabel(\"Bill Length (mm)\")\nax.set_ylabel(\"Bill Depth (mm)\")\nax.legend(title=\"Species\")\n\n# Plot the figure\nfig.tight_layout()"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#scaling-the-data---understanding-the-standard-scaler-class",
    "href": "session4_newMLDemo/session4v2_webpage.html#scaling-the-data---understanding-the-standard-scaler-class",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Scaling the data - Understanding the Standard Scaler class",
    "text": "Scaling the data - Understanding the Standard Scaler class\nFor our clustering to work well, the predictors should be on the same scale. To achieve this, we use an instance of the StandardScaler class.\nclass sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)\n. . .\nParameters are supplied by user\n- copy, with_mean, with_std \nAttributes contain the data of the object\n- scale_: scaling factor\n- mean_: mean value for each feature\n- var_: variance for each feature\n- n_features_in_: number of features seen during fit\n- n_samples_seen: number of samples processed for each feature \nMethods describe the behaviors of the object and/or modify its attributes\n- fit(X): computes mean and std used for scaling and ‚Äòfits‚Äô scaler to data X\n- transform(X): performs standardization by centering and scaling X with fitted scaler\n- fit_transform(X): does both"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#scaling-data",
    "href": "session4_newMLDemo/session4v2_webpage.html#scaling-data",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Scaling Data",
    "text": "Scaling Data\n\n# Selecting features for clustering -&gt; let's just use bill length and bill depth.\nX = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]]\ny = penguins[\"species\"]\n\n# Standardizing the features for better clustering performance\nscaler = StandardScaler() ## create instance of StandardScaler\nX_scaled = scaler.fit_transform(X) \n\n\n\n\n\n\n\n\n\n\nOriginal vs Scaled Features\n\n\nFeature\nOriginal\nScaled\n\n\nOriginal Mean\nOriginal Std\nScaled Mean\nScaled Std\n\n\n\n\nmean\n44\n17\n0\n0\n\n\nstd\n5\n2\n1\n1\n\n\n\n\n\n\n        \n\n\n\n\nShow table code\n## Make X_scaled a pandas df\nX_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n\n# Compute summary statistics and round to 2 sig figs\noriginal_stats = X.agg([\"mean\", \"std\"])\nscaled_stats = X_scaled_df.agg([\"mean\", \"std\"])\n\n# Combine into a single table with renamed columns\nsummary_table = pd.concat([original_stats, scaled_stats], axis=1)\nsummary_table.columns = [\"Original Mean\", \"Original Std\", \"Scaled Mean\", \"Scaled Std\"]\nsummary_table.index.name = \"Feature\"\n\n# Display nicely with great_tables\n(\n    GT(summary_table.reset_index()).tab_header(\"Original vs Scaled Features\")\n    .fmt_number(n_sigfig = 2)\n    .tab_spanner(label=\"Original\", columns=[\"Original Mean\", \"Original Std\"])\n    .tab_spanner(label=\"Scaled\", columns=[\"Scaled Mean\", \"Scaled Std\"])\n    .tab_options(table_font_size = 20)\n)"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#understanding-the-kmeans-model-class",
    "href": "session4_newMLDemo/session4v2_webpage.html#understanding-the-kmeans-model-class",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Understanding the KMeans model class",
    "text": "Understanding the KMeans model class\nclass sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, \ntol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')\nParameters: Set by user at time of instantiation\n- n_clusters, max_iter, algorithm \nAttributes: Store object data\n- cluster_centers_: stores coordinates of cluster centers\n- labels_: stores labels of each point - n_iter_: number of iterations run (will be changed during method run)\n- n_features_in and feature_names_in_: store info about features seen during fit \nMethods: Define object behaviors\n- fit(X): fits model to data X - predict(X): predicts closest cluster each sample in X belongs to\n- transform(X): transforms X to cluster-distance space\n\n\nCreate model\n\n## Choosing 3 clusters b/c we have 3 species\nkmeans = KMeans(n_clusters=3, random_state=42) ## make an instance of the K means class\nkmeans\n\nKMeans(n_clusters=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†KMeans?Documentation for KMeansiNot fittedKMeans(n_clusters=3, random_state=42) \n\n\n\n\n\nFit model to data\n\n## the fit\npenguins[\"kmeans_cluster\"] = kmeans.fit_predict(X_scaled)\n\n## now that we fit the model, we should have cluster centers\nprint(\"Coordinates of cluster centers:\", kmeans.cluster_centers_)\n\n## shows that model is fitted\nkmeans\n\nCoordinates of cluster centers: [[-0.95023997  0.55393493]\n [ 0.58644397 -1.09805504]\n [ 1.0886843   0.79503579]]\n\n\nKMeans(n_clusters=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.¬†¬†KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3, random_state=42)"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#use-function-to-calculate-ari",
    "href": "session4_newMLDemo/session4v2_webpage.html#use-function-to-calculate-ari",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Use function to calculate ARI",
    "text": "Use function to calculate ARI\nTo check how good our model is, we can use one of the functions included in the sklearn library.\nThe adjusted_rand_score() function evaluates how well the cluster groupings agree with the species groupings while adjusting for chance.\n\n# Calculate clustering performance using Adjusted Rand Index (ARI)\nkmeans_ari = adjusted_rand_score(penguins['species'], penguins[\"kmeans_cluster\"])\nprint(f\"k-Means Adjusted Rand Index: {kmeans_ari:.2f}\")\n\nk-Means Adjusted Rand Index: 0.82\n\n\n\n\nWe can also use methods on our data structure to create new data\n\nWe can use the .groupby() method to help us plot cluster agreement with species label as a heatmap\nIf we want to add sex as a variable to see if that is why our clusters don‚Äôt agree with our species, we can use a scatterplot\nUsing seaborn and matplotlib, we can easily put both of these plots on the same figure. \n\n\n# Count occurrences of each species-cluster-sex combination\n# (.size gives the count as index, use reset_index to get count column.)\nscatter_data = (penguins.groupby([\"species\", \"kmeans_cluster\", \"sex\"])\n                .size()\n                .reset_index(name=\"count\"))\nspecies_order = list(scatter_data['species'].unique()) ## defining this for later\n\n# Create a mapping to add horizontal jitter for each sex for scatterplot\nsex_jitter = {'Male': -0.1, 'Female': 0.1}\nscatter_data['x_jittered'] = scatter_data.apply(\n    lambda row: scatter_data['species'].unique().tolist().index(row['species']) +\n     sex_jitter.get(row['sex'], 0),\n    axis=1\n)\n\nheatmap_data = scatter_data.pivot_table(index=\"kmeans_cluster\", columns=\"species\", \nvalues=\"count\", aggfunc=\"sum\", fill_value=0)"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#creating-plots",
    "href": "session4_newMLDemo/session4v2_webpage.html#creating-plots",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Creating Plots",
    "text": "Creating Plots\n\n# Prepare the figure with 2 subplots; the axes object will contain both plots\nfig2, axes = plt.subplots(1, 2, figsize=(16, 7)) ## 1 row 2 columns\n\n# Plot heatmap on the first axis\nsns.heatmap(data = heatmap_data, cmap=\"Blues\", linewidths=0.5, linecolor='white', annot=True, \nfmt='d', ax=axes[0])\naxes[0].set_title(\"Heatmap of KMeans Clustering by Species\")\naxes[0].set_xlabel(\"Species\")\naxes[0].set_ylabel(\"KMeans Cluster\")\n\n# Scatterplot with jitter\nsns.scatterplot(data=scatter_data, x=\"x_jittered\", y=\"kmeans_cluster\",\n    hue=\"species\", style=\"sex\", size=\"count\", sizes=(100, 500),\n    alpha=0.8, ax=axes[1], legend=\"brief\")\naxes[1].set_xticks(range(len(species_order)))\naxes[1].set_xticklabels(species_order)\naxes[1].set_title(\"Cluster Assignment by Species and Sex (Jittered)\")\naxes[1].set_ylabel(\"KMeans Cluster\")\naxes[1].set_xlabel(\"Species\")\naxes[1].set_yticks([0, 1, 2])\naxes[1].legend(bbox_to_anchor=(1.05, 0.5), loc='center left', borderaxespad=0.0, title=\"Legend\")\n\nfig2.tight_layout()"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#project-2-knn-classification",
    "href": "session4_newMLDemo/session4v2_webpage.html#project-2-knn-classification",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Project 2: KNN classification",
    "text": "Project 2: KNN classification\nFor our KNN classification, the model is supervised (meaning it is dependent on the outcome ‚Äòy‚Äô data). This time, we need to split our data into a training and test set. \n. . .\nThe function train_test_split() from scikit-learn is helpful here!\n\n# Splitting dataset into training and testing sets (still using scaled X!)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n\n. . .\n\nUnlike R functions, which return a single object (often a list when multiple outputs are needed), Python functions can return multiple values as a tuple‚Äîletting you unpack them directly into separate variables."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#understanding-kneighborsclassifier-class",
    "href": "session4_newMLDemo/session4v2_webpage.html#understanding-kneighborsclassifier-class",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Understanding KNeighborsClassifier class",
    "text": "Understanding KNeighborsClassifier class\nclass sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', \nalgorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n. . .\nParameters: Set by user at time of instantiation\n- n_neigbors, weights, algorithm, etc. \nAttributes: Store object data\n- classes_: class labels known to the classifier\n- effective_metric_: distance metric used\n- effective_metric_params_: parameters for the metric function\n- n_features_in and feature_names_in_: store info about features seen during fit\n- n_samples_fit_: number of samples in fitted data \nMethods: Define object behaviors\n- .fit(X, y): fit knn classifier from training dataset (X and y)\n- .predict(X): predict class labels for provided data X\n- .predict_proba(X): return probability estimates for test data X\n- .score(X, y): return mean accuracy on given test data X and labels y"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#making-an-instance-of-kneighborsclassifier-and-fitting-to-training-data",
    "href": "session4_newMLDemo/session4v2_webpage.html#making-an-instance-of-kneighborsclassifier-and-fitting-to-training-data",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Making an instance of KNeighborsClassifier and fitting to training data",
    "text": "Making an instance of KNeighborsClassifier and fitting to training data\n\nFor a supervised model, y_train is included in .fit()!\n\n\n## perform knn classification\n# Applying k-NN classification with 5 neighbors\nknn = KNeighborsClassifier(n_neighbors=5) ## make an instance of the KNeighborsClassifier class\n# and set the n_neighbors parameter to be 5. \n\n# Use the fit method to fit the model to the training data\nknn.fit(X_train, y_train)\nknn"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#once-the-model-is-fit",
    "href": "session4_newMLDemo/session4v2_webpage.html#once-the-model-is-fit",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Once the model is fit‚Ä¶",
    "text": "Once the model is fit‚Ä¶\n-We can look at its attributes (ex: .classes_) which gives the class labels as known to the classifier\n\nprint(knn.classes_)\n\n['Adelie' 'Chinstrap' 'Gentoo']\n\n\n. . .\n-And use fitted model to predict species for test data\n\n# Use the predict method on the test data to get the predictions for the test data\ny_pred = knn.predict(X_test)\n\n# Also can take a look at the prediction probabilities, \n# and use the .classes_ attribute to put the column labels in the right order\nprobs = pd.DataFrame(\n    knn.predict_proba(X_test),\n    columns = knn.classes_)\nprobs['y_pred'] = y_pred\n\nprint(\"Predicted probabilities: \\n\", probs.head())\n\nPredicted probabilities: \n    Adelie  Chinstrap  Gentoo     y_pred\n0     1.0        0.0     0.0     Adelie\n1     0.0        0.0     1.0     Gentoo\n2     1.0        0.0     0.0     Adelie\n3     0.0        0.6     0.4  Chinstrap\n4     1.0        0.0     0.0     Adelie"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#scatterplot-for-k-nn-classification-of-test-data",
    "href": "session4_newMLDemo/session4v2_webpage.html#scatterplot-for-k-nn-classification-of-test-data",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Scatterplot for k-NN classification of test data",
    "text": "Scatterplot for k-NN classification of test data\n\nCreate dataframe of unscaled X_test, bill_length_mm, and bill_depth_mm.\nAdd to it the actual and predicted species labels\n\n\n## First unscale the test data\nX_test_unscaled = scaler.inverse_transform(X_test)\n\n## create dataframe \npenguins_test = pd.DataFrame(\n    X_test_unscaled,\n    columns=['bill_length_mm', 'bill_depth_mm']\n)\n\n## add actual and predicted species \npenguins_test['y_actual'] = y_test.values\npenguins_test['y_pred'] = y_pred\npenguins_test['correct'] = penguins_test['y_actual'] == penguins_test['y_pred']\n\nprint(\"Results: \\n\", penguins_test.head())\n\nResults: \n    bill_length_mm  bill_depth_mm   y_actual     y_pred  correct\n0            39.5           16.7     Adelie     Adelie     True\n1            46.9           14.6     Gentoo     Gentoo     True\n2            42.1           19.1     Adelie     Adelie     True\n3            49.8           17.3  Chinstrap  Chinstrap     True\n4            41.1           18.2     Adelie     Adelie     True"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#plotnine-scatterplot-for-k-nn-classification-of-test-data",
    "href": "session4_newMLDemo/session4v2_webpage.html#plotnine-scatterplot-for-k-nn-classification-of-test-data",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Plotnine scatterplot for k-NN classification of test data",
    "text": "Plotnine scatterplot for k-NN classification of test data\nTo see how well our model did at classifying the remaining penguins‚Ä¶\n\n## Build the plot\nplot3 = (ggplot(penguins_test, aes(x=\"bill_length_mm\", y=\"bill_depth_mm\", \ncolor=\"y_actual\", fill = 'y_pred', shape = 'correct'))\n + geom_point(size=4, stroke=1.1)  # Stroke controls outline thickness\n + scale_shape_manual(values={True: 'o', False: '^'})  # Circle and triangle\n + ggtitle(\"k-NN Classification Results\")\n + theme_bw())\n\ndisplay(plot3)"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#visualizing-decision-boundary-with-seaborn-and-matplotlib",
    "href": "session4_newMLDemo/session4v2_webpage.html#visualizing-decision-boundary-with-seaborn-and-matplotlib",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Visualizing Decision Boundary with seaborn and matplotlib",
    "text": "Visualizing Decision Boundary with seaborn and matplotlib\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create and fit label encoder for y (just makes y numeric because it makes the scatter plot happy)\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Create the plot objects\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Create display object\ndisp = DecisionBoundaryDisplay.from_estimator(\n    knn,\n    X_test,\n    response_method = 'predict',\n    plot_method = 'pcolormesh',\n    xlabel = \"bill_length_scaled\",\n    ylabel = \"bill_depth_scaled\",\n    shading = 'auto',\n    alpha = 0.5,\n    ax = ax\n)\n\n# Use method from display object to create scatter plot\nscatter = disp.ax_.scatter(X_scaled[:,0], X_scaled[:,1], c=y_encoded, edgecolors = 'k')\ndisp.ax_.legend(scatter.legend_elements()[0], knn.classes_, loc = 'lower left', title = 'Species')\n_ = disp.ax_.set_title(\"Penguin Classification\")\n\nplt.show()"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#evaluate-knn-performance",
    "href": "session4_newMLDemo/session4v2_webpage.html#evaluate-knn-performance",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Evaluate KNN performance",
    "text": "Evaluate KNN performance\nTo check the performance of our KNN classifier, we can check the accuracy score and print a classification report.\n- accuracy_score and classification_report are both functions!\n- They are not unique to scikit-learn classes so it makes sense for them to be functions not methods\n\n## eval knn performance\nknn_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"k-NN Accuracy: {knn_accuracy:.2f}\")\nprint(\"Classification Report: \\n\", classification_report(y_test, y_pred))\n\nk-NN Accuracy: 0.94\nClassification Report: \n               precision    recall  f1-score   support\n\n      Adelie       0.98      0.98      0.98        48\n   Chinstrap       0.80      0.89      0.84        18\n      Gentoo       0.97      0.91      0.94        34\n\n    accuracy                           0.94       100\n   macro avg       0.92      0.93      0.92       100\nweighted avg       0.94      0.94      0.94       100"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#make-a-summary-table-of-metrics-for-both-models",
    "href": "session4_newMLDemo/session4v2_webpage.html#make-a-summary-table-of-metrics-for-both-models",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Make a Summary Table of Metrics for Both Models",
    "text": "Make a Summary Table of Metrics for Both Models\n\nsummary_table = pd.DataFrame({\n    \"Metric\": [\"k-Means Adjusted Rand Index\", \"k-NN Accuracy\"],\n    \"Value\": [kmeans_ari, knn_accuracy]\n})\n(\n    GT(summary_table)\n    .tab_header(title = \"Model Results Summary\")\n    .fmt_number(columns = \"Value\", n_sigfig = 2)\n    .tab_options(table_font_size = 20)\n)\n\n\n\n\n\n\n\nModel Results Summary\n\n\nMetric\nValue\n\n\n\n\nk-Means Adjusted Rand Index\n0.82\n\n\nk-NN Accuracy\n0.94"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#key-takeaways-from-this-session",
    "href": "session4_newMLDemo/session4v2_webpage.html#key-takeaways-from-this-session",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Key Takeaways from This Session",
    "text": "Key Takeaways from This Session\n\n\n\n\nPython workflows rely on object-oriented structures in addition to functions: Understanding the OOP paradigm makes Python a lot easier!\n\n\nEverything is an object!\n\n\nDuck Typing: If an object has a method, that method can be called regardless of the object type. Caveat being, make sure the arguments (if any) in the method are specified correctly for all objects!\n\n\nPython packages use common methods that make it easy to change between model types without changing a lot of code."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_webpage.html#additional-insights",
    "href": "session4_newMLDemo/session4v2_webpage.html#additional-insights",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Additional Insights",
    "text": "Additional Insights\n\n\nPredictable APIs enable seamless model switching: Swapping models like LogisticRegression ‚Üí RandomForestClassifier usually requires minimal code changes.\n\n\nscikit-learn prioritizes interoperability: Its consistent class design integrates with tools like Pipeline, GridSearchCV, and cross_val_score.\n\n\nClass attributes improve model transparency: Access attributes like .coef_, .classes_, and .feature_importances_ for model interpretation and debugging.\n\n\nCustom classes are central to deep learning: Frameworks like PyTorch and TensorFlow require you to define your own model classes by subclassing base models.\n\n\nMixins support modular design: Mixins (e.g., ClassifierMixin) let you add specific functionality without duplicating code."
  },
  {
    "objectID": "session2a/session2.html",
    "href": "session2a/session2.html",
    "title": "Python Machine Learning Demo",
    "section": "",
    "text": "Guide to Python Data Structures\n\n  \n\nDownload Follow Along File"
  },
  {
    "objectID": "session2a/session2.html#links",
    "href": "session2a/session2.html#links",
    "title": "Python Machine Learning Demo",
    "section": "",
    "text": "Guide to Python Data Structures\n\n  \n\nDownload Follow Along File"
  },
  {
    "objectID": "session2a/session2.html#getting-started",
    "href": "session2a/session2.html#getting-started",
    "title": "Python Machine Learning Demo",
    "section": "Getting Started",
    "text": "Getting Started\nBefore doing anything else, we should first activate the conda environment we want to use.\n\n\nRefresher: How to activate conda environment\n\n\n\n\nFrom terminal, type:\n\n&gt; conda activate ENVNAME\n\n\n\n\nWhen in VS code, you might get a popup message like the one below, confirming that the environment was activated:\n\nSelected conda environment was successfully activated, even though ‚Äú(ENVNAME)‚Äù indicator may not be present in the terminal prompt.\n\n\nIf we want to make sure we have the packages we‚Äôll need installed in the environment before we try to import them, we can either check on anaconda or use the terminal:\n\n&gt; conda list\n\n\n\n\nOtherwise, we will get an error message if we try to import packages that are not installed.\n\n\nRefresher: How to install packages\n\nTo install packages, we can either use the ‚Äúanaconda‚Äù dashboard, or we can use the command line. Make sure your environment is active before installing packages or the packages will not be available in your environment.\nTo install from the command line, we open a terminal and type:\n\n&gt; conda install {package}\n\nor\n\n&gt; pip install {package}\n\nIf a package is not available via conda it might be available via pip."
  },
  {
    "objectID": "session2a/session2.html#topic-1-lists",
    "href": "session2a/session2.html#topic-1-lists",
    "title": "Python Machine Learning Demo",
    "section": "Topic 1: Lists",
    "text": "Topic 1: Lists\nThe most important thing about lists:\nIn Python, lists are mutable, meaning their elements can be changed after the list is created, allowing for modification such as adding, removing, or updating items. This flexibility makes lists powerful for handling dynamic collections of data.\nWe will learn how to do things such as: Create lists Modify lists Sort lists, Loop over elements of a list with a for-loop or using list comprehension, Slice a list, Append to a list\nTo create a list:\n\nmy_list=[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\nprint(my_list)\n\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\n\n\nA list is simply a collection of objects. We can find the length of a list using the len() function\n\nmy_list=[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\nlen(my_list)\n\n7\n\n\nIn Python, lists are objects like all other data types, and the class for lists is named ‚Äòlist‚Äô with a lowercase ‚ÄòL‚Äô. To transform another Python object into a list, you can use the list() function, which is essentially the constructor of the list class. This function accepts a single argument: an iterable. So, you can use it to turn any iterable, such as a range, set, or tuple, into a list of concrete values.\nAside: Python indices begin at 0. In addition, certain built-in python functions such as range will terminate at n-1 in the second argument.\n\nfirst_range=range(5)\nfirst_range_list=list(first_range)\nprint(first_range_list)\n\nsecond_range=range(5,10)\nsecond_range_list=list(second_range)\nprint(second_range_list)\n\n[0, 1, 2, 3, 4]\n[5, 6, 7, 8, 9]\n\n\nAccessing Python list elements\nTo access an individual list element, you need to know its position. Since python starts counting at 0, the first element is in position 0, and the second element is in position 1. You can also access nested elements within a list, or access the list in reverse.\nExamples using my_list from above\n\nprint(my_list)\n\nexample_1=my_list[0]\n\nexample_2=my_list[6][1]\n\nexample_3=my_list[-1]\n\nprint(example_1, example_2, example_3)\n\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\n1 z [4, 'z']\n\n\nMutability of lists\nSince lists are mutable objects, we can directly change their elements.\n\nsome_list=[1,2,3]\nprint(some_list)\n\nsome_list[0]=\"hello\"\nprint(some_list)\n\n[1, 2, 3]\n['hello', 2, 3]\n\n\nAppending an element to a list\nWhen calling append on a list, we append an object to the end of the list:\n\nprint(my_list)\n\nmy_list.append(5)\n\nprint(my_list)\n\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z'], 5]\n\n\nCombining lists\nWe can combine lists with the ‚Äú+‚Äù operator. This keeps the original lists intact\n\nlist_1=[1,2,3]\n\nlist_2=['a','b','c']\n\ncombined_lists=list_1+list_2\n\nprint(combined_lists)\n\n[1, 2, 3, 'a', 'b', 'c']\n\n\nAnother method is to extend one list onto another.\n\nlist_1=[1,2,3]\n\nlist_2=['a','b','c']\n\nlist_1.extend(list_2)\n\nprint(list_1)\n\n[1, 2, 3, 'a', 'b', 'c']\n\n\nThe pop() method removes and returns the last item by default unless you give it an index argument. If you‚Äôre familiar with stacks, this method as well as .append() can be used to create one!\n\nlist_1=[1,2,3]\n\nelement_1=list_1.pop()\nelement_2=list_1.pop(1)\n\nprint(element_1, element_2)\n\n3 2\n\n\nDeleting items by index\ndel removes an item without returning anything. In fact, you can delete any object, including the entire list, using del:\n\nlist_1=[1,2,3]\n\ndel list_1[0]\n\nprint(list_1)\n\n[2, 3]\n\n\nDeleting items by value\nThe .remove() method deletes a specific value from the list. This method will remove the first occurrence of the given object in a list.\n\nlist_1=[1,2,3]\n\nlist_1.remove(1)\n\nprint(list_1)\n\n[2, 3]\n\n\nLists vs.¬†sets, and deleting duplicates from a list:\nThe difference between a list and a set is that a set is an unordered collection of distinct elements, while a list is ordered and can contain repeats of an element. Sets are denoted by curly brackets {}. We can use this knowledge to easily delete duplicates from a list, since there is no built-in method to do so.\n\nlist_1=[1,2,3,1,2]\nprint(list_1)\n\nset_1=set(list_1)\nprint(set_1)\n\nlist_2=list(set_1)\nprint (list_2)\n\n[1, 2, 3, 1, 2]\n{1, 2, 3}\n[1, 2, 3]\n\n\nSorting a list\nThere are two ways to sort a list in Python:\n.sort() modifies the original list itself. Nothing is returned. sorted() returns a new list, which is a sorted version of the original list.\nreverse=True: Use this parameter to sort the list in reverse order.\n\nnumber_list_1=[3,5,2,1,6,19]\nnumber_list_1.sort()\nprint(number_list_1)\n\nnumber_list_2=sorted(number_list_1, reverse=True)\n\n\nalphabet_list_1=['a','z','e','b']\nalphabet_list_1.sort()\nprint(alphabet_list_1)\n\n\nalphabet_list_2=sorted(alphabet_list_1, reverse=True)\nprint(alphabet_list_2)\n\n[1, 2, 3, 5, 6, 19]\n['a', 'b', 'e', 'z']\n['z', 'e', 'b', 'a']\n\n\n\nmixed_list_1=[1,5,3,'a','c','b']\ntry:\n    mixed_list_1.sort()\n    print(mixed_list_1)\nexcept TypeError:\n    print(\"Can't sort a list of mixed elements\")\n\nCan't sort a list of mixed elements\n\n\nList comprehension\nList comprehension offers a shorter syntax when you want to create a new list based on the values of an existing list (or other object)\n\n#Longer syntax with for loop\n\n\n#Example 1:\n\nsome_list=[1,2,3,'a', 'b', 'c']\nnew_list=[]\nfor item in some_list:\n    if type(item)==str:\n        new_list.append(item)\nprint(new_list)\n\n\n#Example 2:\n\nlowercase_list=['joe', 'sarah', 'emily']\ncapital_list=[]\nfor item in lowercase_list:\n    capital_item=item.upper()\n    capital_list.append(capital_item)\nprint(capital_list)\n\n\n#Example 3:\n\nsome_string=\"patrick\"\npatrick_list=[]\nfor letter in some_string:\n    if letter=='t' or letter=='a':\n        patrick_list.append(letter)\nprint(patrick_list)\n\n['a', 'b', 'c']\n['JOE', 'SARAH', 'EMILY']\n['a', 't']\n\n\n\n#Shorter syntax with list comprehension\n\n\n#Example 1:\nsome_list=[1,2,3,'a', 'b', 'c']\nnew_list=[x for x in some_list if type(x)==str]\nprint(new_list)\n\n\n#Example 2:\n\n\nlowercase_list=['joe', 'sarah', 'emily']\ncapital_list=[name.upper() for name in lowercase_list]\nprint(capital_list)\n\n\n#Example 3:\nsome_string=\"patrick\"\npatrick_list=[x for x in some_string if x=='t' or x=='a']\nprint(patrick_list)\n\n['a', 'b', 'c']\n['JOE', 'SARAH', 'EMILY']\n['a', 't']"
  },
  {
    "objectID": "session2a/session2.html#topic-2-tuples",
    "href": "session2a/session2.html#topic-2-tuples",
    "title": "Python Machine Learning Demo",
    "section": "Topic 2: Tuples",
    "text": "Topic 2: Tuples\nA tuple is similar to a list, but with one key difference: tuples are immutable. This means that once you create a tuple, you cannot modify its elements. Tuples are useful for storing data that should not be changed after creation, such as coordinates, days of the week, or fixed pairs.\nJust like lists, tuples are objects, and the class for tuples is tuple. To transform another Python object into a tuple, you can use the tuple() constructor. It accepts a single iterable, such as a list, range, or string.\nTo create a tuple, you use parentheses () rather than square brackets [].\n\n# Creating a tuple\nmy_tuple = (10, 20, 30)\n\n# Accessing elements by index\nprint(\"First element:\", my_tuple[0])\nprint(\"Last element:\", my_tuple[-1])\n\nFirst element: 10\nLast element: 30\n\n\nTuples are immutable, so you can‚Äôt modify their elements. Attempting to change a tuple will result in an error.\n\n# Trying to modify a tuple element (this will raise an error)\ntry:\n    my_tuple[1] = 99\nexcept TypeError:\n    print(\"Tuples are immutable and cannot be changed!\")\n\nTuples are immutable and cannot be changed!\n\n\nFunctions can return multiple values as a tuple. This is useful for returning multiple results in a single function call.\n\n# Function that returns multiple values as a tuple\ndef min_max(nums):\n    return min(nums), max(nums)  # Returns a tuple of (min, max)\n\n# Calling the function and unpacking the tuple\n\nnumbers = [3, 7, 1, 5]\n\nour_tuple = min_max(numbers)\n\nmin_val, max_val = min_max(numbers) #Unpacking in the function call\n\nprint(our_tuple)\nprint(\"Min:\", min_val)\nprint(\"Max:\", max_val)\n\n(1, 7)\nMin: 1\nMax: 7"
  },
  {
    "objectID": "session2a/session2.html#topic-3-strings",
    "href": "session2a/session2.html#topic-3-strings",
    "title": "Python Machine Learning Demo",
    "section": "Topic 3: Strings",
    "text": "Topic 3: Strings\nCreating a string - You can use single or double quotes to define a string (but keep it consistent!)\n\nmy_string = \"Hello, World!\"\nprint(my_string)\n\nHello, World!\n\n\nYou can also create a multiline string using triple quotes:\n\nmulti_line_string = \"\"\"This is\na multiline\nstring.\"\"\"\nprint(multi_line_string)\n\nThis is\na multiline\nstring.\n\n\nYou can find the length of a string using the len() funtion, just like with lists.\n\nmy_string = \"Hello, World!\"\nprint(len(my_string))\n\n13\n\n\nAccessing characters in a string\nStrings are indexed like lists, with the first character having index 0. You can access individual characters using their index.\n\nmy_string = \"Hello, World!\"\n\n# First character\nfirst_char = my_string[0]\n\n# Last character (using negative indexing)\nlast_char = my_string[-1]\n\n# Accessing a range of characters (slicing)\nsubstring = my_string[0:5]\n\nprint(first_char, last_char, substring)\n\nH ! Hello\n\n\nStrings are immutable!\nUnlike lists, strings cannot be changed after creation. If you try to change an individual character, you‚Äôll get an error.\n\nmy_string = \"Hello\"\ntry:\n    my_string[0] = \"h\"  # This will raise an error\nexcept TypeError:\n    print(\"Strings are immutable!\")\n\nStrings are immutable!\n\n\nConcatenating Strings\nYou can concatenate (combine) strings using the + operator:\n\ngreeting = \"Hello\"\nname = \"Patrick\"\ncombined_string = greeting + \", \" + name + \"!\"\nprint(combined_string)\n\nHello, Patrick!\n\n\nString methods\nPython provides many built-in methods for manipulating strings. Some common ones are:\nupper() and lower() These methods convert a string to uppercase or lowercase.\n\nmy_string = \"Hello, World!\"\nprint(my_string.upper())\nprint(my_string.lower())\n\nHELLO, WORLD!\nhello, world!\n\n\nstrip() This method removes any leading or trailing whitespace from the string.\n\nmy_string = \"   Hello, World!   \"\nprint(my_string.strip())\n\nHello, World!\n\n\nreplace() You can replace parts of a string with another string.\n\nmy_string = \"Hello, World!\"\nnew_string = my_string.replace(\"World\", \"Patrick\")\nprint(new_string)\n\nHello, Patrick!\n\n\nThe split() method divides a string into a list of substrings based on a delimiter (default is whitespace).\n\nmy_string = \"Hello, World!\"\nwords = my_string.split()\nprint(words)\n\n\nanother_string=\"Hello-World!\"\nmore_words=another_string.split(\"-\")\nprint(more_words)\n\n['Hello,', 'World!']\n['Hello', 'World!']\n\n\nThe join() method takes an iterable (like a list) and concatenates its elements into a string with a specified separator between them.\n\nmy_list=['Hello,', 'my', 'name', 'is', 'Patrick']\n\nmy_string=' '.join(my_list)\n\nprint(my_string)\n\nHello, my name is Patrick\n\n\nf-strings (Python 3.6+) You can insert variables directly into strings using f-strings.\n\nname = \"Patrick\"\nage = 30\nformatted_string = f\"My name is {name} and I am {age} years old.\"\nprint(formatted_string)\n\nMy name is Patrick and I am 30 years old.\n\n\n\nmy_string = \"Hello, World!\"\n\n# Extract all vowels from the string\nvowels = str([char for char in my_string if char.lower() in \"aeiou\"])\nprint(vowels)\n\n['e', 'o', 'o']\n\n\nIn this last example, we will slice a string using different combinations of start, end, and step to extract different parts of the string.\n\n#To slice a string, follow the string[start:end:step] format\n\n# Original string\nmy_string = \"Python is awesome!\"\n\n# Slice from index 0 to 6 (not inclusive), stepping by 1 (default)\n# This will extract \"Python\"\nsubstring_1 = my_string[0:6]\n\n# Slice from index 7 to the end of the string, stepping by 1 (default)\n# This will extract \"is awesome!\"\nsubstring_2 = my_string[7:]\n\n# Slice the entire string but take every second character\n# This will extract \"Pto saeoe\"\nsubstring_3 = my_string[::2]\n\n# Slice from index 0 to 6, stepping by 2\n# This will extract \"Pto\"\nsubstring_4 = my_string[0:6:2]\n\n# Slice from index 11 to 6, stepping backward by -1\n# This will extract \"wa si\" (reverse slice)\nsubstring_5 = my_string[11:6:-1]\n\n# Print the results\nprint(f\"Original string: {my_string}\")\nprint(f\"Substring 1 (0:6): {substring_1}\")\nprint(f\"Substring 2 (7:): {substring_2}\")\nprint(f\"Substring 3 (every second character): {substring_3}\")\nprint(f\"Substring 4 (0:6:2): {substring_4}\")\nprint(f\"Substring 5 (11:6:-1): {substring_5}\")\n\nOriginal string: Python is awesome!\nSubstring 1 (0:6): Python\nSubstring 2 (7:): is awesome!\nSubstring 3 (every second character): Pto saeoe\nSubstring 4 (0:6:2): Pto\nSubstring 5 (11:6:-1): wa si"
  },
  {
    "objectID": "session2a/session2.html#topic-4-dictionaries",
    "href": "session2a/session2.html#topic-4-dictionaries",
    "title": "Python Machine Learning Demo",
    "section": "Topic 4: Dictionaries",
    "text": "Topic 4: Dictionaries\nA dictionary is a collection in Python that stores data as key-value pairs. It‚Äôs similar to a real-world dictionary where you look up a word (the key) to get its definition (the value). In Python, dictionaries are mutable, meaning you can add, remove, and change items.\nTo create a dictionary, use curly braces {}, with each key-value pair separated by a colon (:), and pairs separated by commas.\n\n# Creating a dictionary\nmy_dictionary = {\n    'name': 'Alice',\n    'age': 25,\n    'city': 'New York'\n}\nprint(my_dictionary)\n\n{'name': 'Alice', 'age': 25, 'city': 'New York'}\n\n\nAccessing dictionary values To access a specific value in a dictionary, use the key in square brackets. You can also use the .get() method, which returns None if the key does not exist, instead of raising an error.\n\nprint(my_dictionary['name'])      # Using key\nprint(my_dictionary.get('age'))   # Using .get() method\nprint(my_dictionary.get('gender', 'Not specified'))  # Providing a default value\n\nAlice\n25\nNot specified\n\n\nAdding and updating dictionary items\nDictionaries are mutable, so you can add new items or update existing ones using assignment.\n\nmy_dictionary['job'] = 'Engineer'        # Adding a new key-value pair\nmy_dictionary['age'] = 26                # Updating an existing value\nprint(my_dictionary)\n\n{'name': 'Alice', 'age': 26, 'city': 'New York', 'job': 'Engineer'}\n\n\nDictionary Methods\nPython dictionaries have several useful methods for managing data:\nkeys(): Returns a list of all the keys in the dictionary. values(): Returns a list of all values in the dictionary. items(): Returns a list of key-value pairs as tuples.\n\n# Getting all keys\nprint(my_dictionary.keys())\n\n# Getting all values\nprint(my_dictionary.values())\n\n# Getting all key-value pairs\nprint(my_dictionary.items())\n\ndict_keys(['name', 'age', 'city', 'job'])\ndict_values(['Alice', 26, 'New York', 'Engineer'])\ndict_items([('name', 'Alice'), ('age', 26), ('city', 'New York'), ('job', 'Engineer')])\n\n\nPython Dictionaries for Cancer Research Data\nIn cancer research, dictionaries can be used to store patient data, genetic mutations, and statistical results as key-value pairs. This allows for easy lookup, organization, and analysis of data.\n\n#Let‚Äôs create a dictionary to store basic patient information, where each patient has a unique ID, and each ID maps to a dictionary containing information about the patient‚Äôs age, cancer type, and stage.\n\n# Dictionary of patients with nested dictionaries\npatient_data = {\n    'P001': {'age': 50, 'cancer_type': 'Lung Cancer', 'stage': 'II'},\n    'P002': {'age': 60, 'cancer_type': 'Breast Cancer', 'stage': 'I'},\n    'P003': {'age': 45, 'cancer_type': 'Melanoma', 'stage': 'III'}\n}\n\nprint(patient_data)\n\n{'P001': {'age': 50, 'cancer_type': 'Lung Cancer', 'stage': 'II'}, 'P002': {'age': 60, 'cancer_type': 'Breast Cancer', 'stage': 'I'}, 'P003': {'age': 45, 'cancer_type': 'Melanoma', 'stage': 'III'}}\n\n\nYou can access a patient‚Äôs information using their unique ID. To access nested data, chain the keys. For example, to retrieve the cancer type of a specific patient, you‚Äôd use the following:\n\n# Accessing specific information\n\npatient_id = 'P002'\ncancer_type = patient_data[patient_id]['cancer_type']\nprint(f\"Cancer type for {patient_id}: {cancer_type}\")\n\n# Updating a patient‚Äôs stage\npatient_data['P003']['stage'] = 'IV'\nprint(f\"Updated stage for P003: {patient_data['P003']['stage']}\")\n\nCancer type for P002: Breast Cancer\nUpdated stage for P003: IV\n\n\nAdding and Removing Data New patient data can be added using assignment, and pop() or del can remove a patient‚Äôs data.\n\n# Adding a new patient\npatient_data['P004'] = {'age': 70, 'cancer_type': 'Prostate Cancer', 'stage': 'II'}\nprint(\"Added new patient:\", patient_data['P004'])\n\n# Removing a patient\nremoved_patient = patient_data.pop('P001')\nprint(\"Removed patient:\", removed_patient)\n\nAdded new patient: {'age': 70, 'cancer_type': 'Prostate Cancer', 'stage': 'II'}\nRemoved patient: {'age': 50, 'cancer_type': 'Lung Cancer', 'stage': 'II'}\n\n\nDictionary Methods: Data Summary Dictionaries allow you to retrieve keys, values, or entire key-value pairs. Here‚Äôs how to use these methods to get an overview of the data.\nkeys(): Retrieves all patient IDs. values(): Retrieves all patient records. items(): Retrieves patient records as key-value pairs\n\n# Getting all patient IDs\nprint(\"Patient IDs:\", patient_data.keys())\n\n# Getting all patient details\nprint(\"Patient Details:\", patient_data.values())\n\n# Looping through each patient's data\nfor patient_id, details in patient_data.items():\n    print(f\"Patient {patient_id} - Age: {details['age']}, Cancer Type: {details['cancer_type']}, Stage: {details['stage']}\")\n\nPatient IDs: dict_keys(['P002', 'P003', 'P004'])\nPatient Details: dict_values([{'age': 60, 'cancer_type': 'Breast Cancer', 'stage': 'I'}, {'age': 45, 'cancer_type': 'Melanoma', 'stage': 'IV'}, {'age': 70, 'cancer_type': 'Prostate Cancer', 'stage': 'II'}])\nPatient P002 - Age: 60, Cancer Type: Breast Cancer, Stage: I\nPatient P003 - Age: 45, Cancer Type: Melanoma, Stage: IV\nPatient P004 - Age: 70, Cancer Type: Prostate Cancer, Stage: II"
  },
  {
    "objectID": "sessions.html",
    "href": "sessions.html",
    "title": "Sessions",
    "section": "",
    "text": "Session links\nTutorial: Get Started with Python \nGo to Session2a: Intro to Python Data Structures \nGo to Session2b: Intro to Pandas \nGo to Object-oriented Programming Demo"
  },
  {
    "objectID": "session1/session1.html",
    "href": "session1/session1.html",
    "title": "Intro to Python",
    "section": "",
    "text": "The overall goal of the is to learn how to program in Python using modern, reproducible tools.\n\nSession 1 will help you set up a flexible, interactive, working environment for Python programming (Miniconda, VS Code IDE, Jupyter Notebook, etc.)\nSession 2 and 3 will focus on the Python basics such as data structure, list comprehensive, and functions. We will also learn data frame manipulations with pandas.\nSession 4 will introduce you to object-oriented programming (OOP) with examples from the statistics library statsmodels and machine learning library scikit-learn."
  },
  {
    "objectID": "session1/session1.html#workshop-at-a-glance",
    "href": "session1/session1.html#workshop-at-a-glance",
    "title": "Intro to Python",
    "section": "",
    "text": "The overall goal of the is to learn how to program in Python using modern, reproducible tools.\n\nSession 1 will help you set up a flexible, interactive, working environment for Python programming (Miniconda, VS Code IDE, Jupyter Notebook, etc.)\nSession 2 and 3 will focus on the Python basics such as data structure, list comprehensive, and functions. We will also learn data frame manipulations with pandas.\nSession 4 will introduce you to object-oriented programming (OOP) with examples from the statistics library statsmodels and machine learning library scikit-learn."
  },
  {
    "objectID": "session1/session1.html#what-youll-need-for-the-workshop",
    "href": "session1/session1.html#what-youll-need-for-the-workshop",
    "title": "Intro to Python",
    "section": "‚ùóWhat You‚Äôll Need for the Workshop",
    "text": "‚ùóWhat You‚Äôll Need for the Workshop\n\nBring your laptop (Windows/MacOS)\nBasic GitHub knowledge & MSK GitHub Enterprise account‚ÄìThe workshop website is hosted on MSK Enterprise GitHub, which might require you logging in with MSK credentials to access links/download files. Check out Biostatistics Resource Guide for GitHub-related training.\nInstall Miniconda and Visual Studio Code before session 1 (see this guide)\n\n\n\n\n\n\nImportant\n\n\n\nWhen following the installation instructions in this article, we recommend that you install software on your MSK laptop or workstation. Downloading/installing software files on VDI is extremely slow and might not install at all if the software is too large."
  },
  {
    "objectID": "session1/session1.html#about-this-guide",
    "href": "session1/session1.html#about-this-guide",
    "title": "Intro to Python",
    "section": "üìñ About this Guide",
    "text": "üìñ About this Guide\nThis handout is a reading for the first session of the Introduction to Python workshop. It serves as a follow-along guide to help you install Python, set up essential tools like Miniconda and VS Code, and prepare for coding in Python for the upcoming sessions.\nAlong the way, we will also discuss some important questions‚ÄìWhat makes Python useful? Why would we, as biostatisticians, want to learn it? You will get an overview of the key features of Python, what it can do in relation to biostatistics/bioinformatics research, and how it compared to R. In the upcoming sessions, we will dive deeper into some of Python‚Äôs features and functions through hands-on programming practices.\n\n\n\n\n\n\nüìùLearning objectives of the article\n\n\n\nThis handout walks you through installing the necessary tools and setting up your Python environment. Please follow each step before the first workshop session.\nüí°Aim 1: know key features of Python and its applications.\nüí°Aim 2: know what Miniconda is and the steps to install Python through it.\nüí°Aim 3: know what Python virtual environments are and how to create them with conda.\nüí°Aim 4: know what integrated development environments (IDE) are and steps to set up an interactive Python coding environment in Visual Studio Code."
  },
  {
    "objectID": "session1/session1.html#what-is-python",
    "href": "session1/session1.html#what-is-python",
    "title": "Intro to Python",
    "section": "What is Python?",
    "text": "What is Python?\n\n\nPython is a high-level, interpreted, and general-purpose programming language first developed by Guido van Rossum in 1991.\n\nPython has gained much popularity in the past 20 years. Its user group has expanded into a large and active scientific computing and developer community that spans numerous academic and industrial fields. Nowadays, Python has a powerful ecosystem of external packages (libraries) for data science, artificial intelligence, and software development.\nPython is cross-platform and open-source. In Python, you can easily install packages with the built-in installer pip or package manager conda, just as you do with install.packages() in R."
  },
  {
    "objectID": "session1/session1.html#what-can-python-do",
    "href": "session1/session1.html#what-can-python-do",
    "title": "Intro to Python",
    "section": "What can Python do?",
    "text": "What can Python do?\nJust like R, Python is an open source and versatile programming language that allows users to perform a wide range of data analysis and computational tasks. While R is particularly useful in statistical analysis and visualizations, Python has been used in many distinct areas, such as:\n\nMachine Learning and Deep learning\nWeb Development\nScripting & Automation\nCloud Computing\nGame Development\nCybersecurity"
  },
  {
    "objectID": "session1/session1.html#why-learn-pythonas-biostatisticians",
    "href": "session1/session1.html#why-learn-pythonas-biostatisticians",
    "title": "Intro to Python",
    "section": "Why learn Python‚Äìas Biostatisticians?",
    "text": "Why learn Python‚Äìas Biostatisticians?\nWithin the field of biostatistics/ bioinformatics, Python has become a core tool for biomedical data analysis due to its versatility, reproducibility, and strong ecosystem of scientific libraries. Below are some areas where Python can be useful and some essential libraries.\n\nStatistical analysis\nWhile R is the go-to tool for statistical analysis, Python has caught up with many equivalent libraries and functions:\n\nstatsmodels/ scipy.stats provide regression modeling and hypothesis testing.\nlifelines/ scikit-survival support survival analysis and plotting.\n\nML/DL ecosystem\nPython dominates in machine learning and AI development:\n\nscikit-learn is a rich machine learning library that supports both supervised regression an dclassification (e.g., random forests, gradient boosting) and unsupervised clustering (e.g., K-means).\nTensorFlow, PyTorch are deep learning libraries widely used for computer vision and natural language processing.\noptuna, Ray can be integrated into ML/DL workflows for easy and efficient model training, hyperparameter tuning, fine-tuning, etc.\n\nOmics data analysis\nEmerging packages that provide standard omics data preprocessing and analysis pipelines allow Python to become increasingly popular in the field of bioinformatics:\n\nscanpy, anndata are libraries for single-cell RNA-seq data loading, preprocessing, and analysis.\nBiopython is a set of tools for biological computation that performs file parsering (BLAST, FASTA, GenBank, etc.), sequence analysis, clustering algorithms, etc.\npysam works with BAM/SAM/VCF files."
  },
  {
    "objectID": "session1/session1.html#python-vs.-r-differences",
    "href": "session1/session1.html#python-vs.-r-differences",
    "title": "Intro to Python",
    "section": "Python vs.¬†R: Differences",
    "text": "Python vs.¬†R: Differences\nWhile both programming languages are popular for data analysis and computation, Python and R differ in their underlying code structure, the scope of functionality, and the extensibility of tasks they can perform. Here is a non-exhaustive summary of some key differences:\n\n\n\n\n\n\n\n\n\nFeature/Task\nR\nPython\n\n\n\n\nProgramming logic\nMostly function-oriented\nFunction and object-oriented ‚Äì structured around classes\n\n\nGeneral-purpose programming\n‚ö†Ô∏è Less ideal ‚Äì designed mainly for working with data\n‚úÖ Strong ‚Äì ML & AI, software development, scripting, etc.\n\n\nComputational power\n‚úÖ Vectorization allows operating on all elements of a vector at once  ‚úÖ Best for statistical analysis  ‚ö†Ô∏è Memory-intensive; often slow for reading large data and performing large computations\n‚úÖ Generally faster for loops  ‚úÖ Strong support for GPU computing  ‚úÖ Memory-efficient for handling large objects and complex computations\n\n\nPackage availability\n‚úÖ Excellent for statistical analysis (glm, survival, ggplot2)  ‚ö†Ô∏è Good options for ML (caret, mlr3) but few DL packages  ‚úÖ Great for omics-focused analysis (Bioconductor, ComplexHeatmap, Seurat)\n‚òëÔ∏è Improving on statistical packages (statsmodels, lifelines)  ‚úÖ Best for ML/DL (scikit-learn, pytorch, keras)  ‚úÖ Established packages specialized in processing large omics datasets (scanpy, scvi-tools)\n\n\nIDE & Reproducible environments (notebooks etc.)\n‚úÖ RStudio  ‚úÖ RMarkdown, Quarto\n‚úÖ Visual Studio Code, JupyterLab, PyCharm, Spyder, etc.  ‚úÖ Jupyter Notebooks, Quarto"
  },
  {
    "objectID": "session1/session1.html#essential-tools-for-python-programming",
    "href": "session1/session1.html#essential-tools-for-python-programming",
    "title": "Intro to Python",
    "section": "Essential Tools for Python Programming",
    "text": "Essential Tools for Python Programming\nTo get the most out of Python‚Äìespecially for data science and reproducible research‚Äìit‚Äôs important to set up an integrated, flexible programming environment.\nHere is a list of tools we recommend using:\n\nConda: a powerful package and environment manager for Python.\nVisual Studio Code: a lightweight code editor that integrates programming + plots + terminal + etc.\nJupyer Notebook: an interactive computing tool that combines code execution, text documentation, and visualizations.\nGit (GitHub): for version control, collaboration, and publishing code.\n\n\nConda (via Miniconda)\nConda is a package manager, much like CRAN + Bioconductor, and can be utilized across languages (Python, R, C/C++ etc.). It also simplifies Python environment management, similar to renv in R but more powerful and flexible, ensuring dependency isolation without cluttering the global system.\nConda can be installed via either Miniconda (lightweight version) or the Anaconda Distribution (full version). We will use the former for the purpose of this workshop series.\n\n\nVisual Studio Code\nAkin to RStudio, Visual Studio Code (often called VS Code) is an IDE for multi-language coding (Python, R, Java, etc.). It has many features integrated within it, including interactive coding via Jupyter Notebook or Quarto, version control with Git, and built-in terminal and debugging tools.\nNext, we will walk through installing and setting up these tools."
  },
  {
    "objectID": "session1/session1.html#follow-along-install-miniconda-python-conda",
    "href": "session1/session1.html#follow-along-install-miniconda-python-conda",
    "title": "Intro to Python",
    "section": "‚ñ∂Ô∏èFollow-Along: Install Miniconda (Python + conda)",
    "text": "‚ñ∂Ô∏èFollow-Along: Install Miniconda (Python + conda)\nLet‚Äôs walk through steps to install Minconda.\n\nFor the latest Miniconda installers for Python 3.12, navigate to the Anaconda website.\nDownload the 64-bit graphical installer according to your system (Windows or MacOS):\nNote: Make sure you are downloading from the Miniconda Installers section, not Anaconda!\n\nAlternatively, check out the Quick command line installation guide to install Miniconda through command line interface.\n\nRun the installer (.exe for Windows / .pkg for MacOS)\n\nselect Just Me for installation type ‚Äì recommended; doesn‚Äôt require admin rights.\n\n\n\n\n\n\n\nInstalling for current user only\n\n\n\nYou don‚Äôt need to install for all users most of the time. This option requires admin privileges which you might not have on your MSK laptop.\n\n\nKeep the default for installation location. E.g.,\n\nWindows: C:\\Users\\&lt;user_name&gt;\\AppData\\Local\\miniconda3\nMacOS: /Users/yourname/miniconda3\n\nCustomize the advanced installation options:\n\n‚ùå Add Miniconda to my PATH environment variable ‚Äì NOT recommended\n‚úÖ Register Miniconda3 as my default Python 3.12\n\n\n\n\n\n\n\n\n\nDo not add Miniconda to PATH‚ö†Ô∏è\n\n\n\nIt is recommended that you do not add Miniconda to system‚Äôs PATH environment variable, as it might lead to conflicts with your other Python installations or accidentally break software using the system Python.\nInstead, you could later run conda init in Anaconda Prompt to configure the terminal shells (like PowerShell or Command Prompt) to recognize the conda command.\n\n\nComplete installation. This might take a few minutes to complete.\nCheck installation‚Äìverify that Python and Conda are successfully installed.\n\nWindows:\n\nOpen the Start Menu and run Anaconda Prompt.\nType the following command.\nconda --version\npython --version\nYou should see the current versions of your Python and Conda being returned‚Äìsuch as conda 24.9.2 and Python 3.12.4 (the exact numbers might differ). This means that Miniconda is properly installed and initialized.\n\nMacOS:\n\nOpen Terminal. Configure your shell to make the conda command available.\nsource ~/miniconda3/bin/activate\nconda init zsh # or conda init bash if you are using bash\nThen restart your Terminal and type:\nconda --version\npython --version\nYou should see the current versions of your Python and Conda being returned, which means everything has been correctly installed."
  },
  {
    "objectID": "session1/session1.html#what-is-conda",
    "href": "session1/session1.html#what-is-conda",
    "title": "Intro to Python",
    "section": "What is Conda?",
    "text": "What is Conda?\nConda is a cross-platform command line tool for managing packages and environment via the conda command line interface (CLI). It can handle both Python and non-Python dependencies (R, C, system binaries, etc.), making it particularly powerful.\nYou can install conda via installers such as Miniconda or the Anaconda Distribution."
  },
  {
    "objectID": "session1/session1.html#conda-for-managing-packages",
    "href": "session1/session1.html#conda-for-managing-packages",
    "title": "Intro to Python",
    "section": "Conda for Managing Packages",
    "text": "Conda for Managing Packages\nConda installs packages from channels(repositories), such as the default Anaconda channel or community-maintained channels like conda-forge.\nTo install packages from the default Anaconda repository:\n# Install a single package\nconda install scipy\n\n# Install a specific version of a package\nconda install scipy=0.15.0 \n\n# Install multiple packages\nconda install scipy=0.15.0 pandas matplotlib\nIf the package is located in another channel, such as conda-forge, you can manually specify the channel when installing the package. For example:\nconda install conda-forge::pytorch \n# or \nconda install pytorch --channel conda-forge\nTo update a package:\nconda update scipy\nNote that this automatically updates the package to the highest version supported by the current Python series. For example, Python 3.9 updates to the highest available in the 3.x series.\nTo remove a package (or multiple packages at once):\nconda remove scipy pandas matplotlib\n\nConda vs.¬†Pip\nIf a Python package is not available through any conda channel, consider using the pip package manager:\npip install\n\n\n\n\n\n\nDifference Between conda install and pip install\n\n\n\nLong story short: Pip installs Python libraries only, while conda can install both Python and non-Python packages (e.g., R, C/C++, system binaries).\nIt is generally recommended that you only use conda install within a conda environment, as anything installed via pip won‚Äôt be recognized by conda and vice versa. Using the two interchangeably might overwrite or break packages and mess up the environment.\nWhat if the Python package is unavailable through conda?\nThe best practice is to install everything with conda first, then use pip only when the package is not available in conda.\nCheck out this blog for more information on using pip in a conda environment."
  },
  {
    "objectID": "session1/session1.html#what-is-a-virtual-environment",
    "href": "session1/session1.html#what-is-a-virtual-environment",
    "title": "Intro to Python",
    "section": "What is a Virtual Environment?",
    "text": "What is a Virtual Environment?\nA virtual environment is an isolated, self-contained workspace that includes its own language interpreter and package dependencies. Each environment operates independently, ensuring that projects are isolated from one another and from the system‚Äôs global setup.\nIn the previous section, we installed Python 3.12 via Miniconda and set it as the default (global) Python. However, you might need a different version of Python‚Äìsay, Python 3.8‚Äìor a different set of packages for a particular project. In this case, creating a virtual environment allows you to maintain a completely separate Python setup, including its own Python version and /site-packages folder.\nYou can create as many environments as needed ‚Äî ideal for managing multiple projects with different requirements."
  },
  {
    "objectID": "session1/session1.html#why-use-virtual-environments",
    "href": "session1/session1.html#why-use-virtual-environments",
    "title": "Intro to Python",
    "section": "Why Use Virtual Environments?",
    "text": "Why Use Virtual Environments?\nYou may find the flexibility of environments beneficial in many cases.\n\nAvoid Conflicts. Creating virtual environments can help resolve potential conflicts between different projects that might require different Python version or conflicting dependencies. Changes made to one environment won‚Äôt affect other projects that use different environments.\nEasy Management. When your work is temporary or that you simply want to experiment things without having to worry about breaking things, you can work within a virtual environment and later delete it when needed.\nSharing Environment. You can share your Python environment and whole list of dependencies with other people through a copy of the .yml file.\nReproducibility. They work as time capsules, allowing you to come back to an older project at any time later by recreating the virtual environment."
  },
  {
    "objectID": "session1/session1.html#follow-along-create-a-conda-virtual-environment",
    "href": "session1/session1.html#follow-along-create-a-conda-virtual-environment",
    "title": "Intro to Python",
    "section": "‚ñ∂Ô∏èFollow-Along: Create a Conda Virtual Environment",
    "text": "‚ñ∂Ô∏èFollow-Along: Create a Conda Virtual Environment\nUsing conda, we can create, activate, update, export, and remove virtual environments, each with its own Python version and set of packages. Let‚Äôs practice creating a virtual environment and installing packages into it using the command line.\n\n\n\n\n\n\nüìåPrerequisite\n\n\n\nBe sure you have installed Miniconda by following the previous tutorial to access the conda command line interface.\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe Anaconda Navigator is an alternative to creating conda environments without the terminal skills. However, for users comfortable using the command line tool, we recommend the conda approach for better speed, control, and stability,\n\n\n\nOpen Anaconda Prompt (Windows) or Terminal (MacOS)\nCreate the virtual environment. Replace &lt;env-name&gt; with the name you want to give your environment.\nconda create --name &lt;env-name&gt;\nNote: you can use -n (shorthand) and --name interchangeably.\nTo create an environment with a specific Python version:\nconda create -n &lt;env-name&gt; python=3.10\nTo create an environment with a specific package(s):\nconda create -n &lt;env-name&gt; python=3.10 scipy pandas matplotlib\nor install packages later with separate commands:\nconda create -n &lt;env-name&gt; python=3.10\nconda install -n &lt;env-name&gt; scipy pandas matplotlib\nYou can also install packages from channels other than the defaults (your can pass multiple channels for the package search):\nconda install -n &lt;env-name&gt; scipy --channel conda-forge --channel bioconda\nor explicitly specify the channel from which you want the package to be installed:\n conda install -n &lt;env-name&gt; conda-forge::scipy\nNow, activate your environment.\nconda activate &lt;env-name&gt;\nYou can verify that your installation was successful by looking up the list of all current environments on your computer.\nconda env list\nThe default location for the installed conda environments (except for the base conda environment) is ..\\miniconda3\\envs\\&lt;env-name&gt;\nDeactivate the conda environment.\nconda deactivate\n\n\n\n\n\n\nAvoid activating on top of another virtual environment!\n\n\n\nAlways conda deactivate first before activating another one because environments can be stacked. This can lead to chaos in the packages in both environments.\nüí°Tip: make sure you see (base) at the beginning of the terminal prompt when you are about to activate an environment.\n\n\nRemoving an environment.\n\nRemove by environment name:\nconda env remove -n &lt;env-name&gt;\nRemove by environment folder path:\nconda env remove --prefix &lt;/path/to/your/env&gt;\n\n\n\nüëâCreate an Environment from an environment.yml File\n\nYou can also create a virtual environment from a .yml configuration file.\nconda env create -f environment.yml\nExample: An environment file contains information about the environment name, channels, and dependencies:\nname: python310\nchannels:\n  - defaults\ndependencies:\n  - python=3.10\n  - pandas\n  - numpy\n\n\n\n\n\n\nNote\n\n\n\nDownload the .yml file for this Python workshop series here. This file includes the Python version and required channels and dependencies for completing the workshop sessions.\n\n\nThen activate the new environment:\nconda activate python310\nThis way, we can skip the cumbersome steps to set up an environment from scratch and easily recreate an environment shared by others or share our environment settings with others.\n\n\nSummary: A list of useful conda commands for managing environments.\n\n\n\n\n\n\nTask\nCommand\n\n\n\n\nCreate an environment\nconda create --name &lt;env-name&gt;\n\n\nList all environments\nconda env list\n\n\nRemove an environment\nconda remove --name &lt;env-name&gt; --all\n\n\nList packages in current environment\nconda list\n\n\nExport environment to .yml\nconda env export &gt; environment.yml\n\n\nRecreate environment from .yml file\nconda env create -f environment.yml"
  },
  {
    "objectID": "session1/session1.html#visual-studio-code-1",
    "href": "session1/session1.html#visual-studio-code-1",
    "title": "Intro to Python",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\n\n\nVisual Studio Code (VS Code) is one of the most popular open-source code editors with many features.\n\n\nMulti-Language Programming. VS Code supports multiple programming languages including Python, R, C/C++, JavaScript, etc.\nIntegrated Git Source Control. VS Code automatically recognizes and uses the computer‚Äôs Git installation. You can easily track changes, stage, and commit changes to your working branch.\nVariety of Project Development Support. You can add extra features such as language packs, debugging tools, Git/Github features, and remote server connector by installing extensions from the Extension Marketplace."
  },
  {
    "objectID": "session1/session1.html#follow-along-set-up-jupyter-notebook-in-vs-code",
    "href": "session1/session1.html#follow-along-set-up-jupyter-notebook-in-vs-code",
    "title": "Intro to Python",
    "section": "‚ñ∂Ô∏èFollow-Along: Set up Jupyter Notebook in VS Code",
    "text": "‚ñ∂Ô∏èFollow-Along: Set up Jupyter Notebook in VS Code\nWe‚Äôll now walk through setting up your Python coding environment in VS Code with full support for virtual environments and Jupyter notebooks.\n\n\n\n\n\n\nüí°Prerequisites\n\n\n\nEnsure you have the following:\n\nMiniconda: see ‚ñ∂Ô∏èFollow-Along: Install Miniconda (Python + conda)\nVS Code: Download and install VS Code\n\n\n\n\nStep 1: Install Required VS Code Extensions\n\nLaunch VS Code.\nOpen the Extensions panel from the left toolbar (or Ctrl+Shift+X on Windows/ Cmd+Shift+X on Mac).\n\nInstall the following extensions:\n\nPython: To support Python language, debugging, documentations, etc.\nJupyter: To support rendering Python documents from Jupyter Notebooks or Quarto files.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou still need to install Python (Miniconda) to your computer for these extensions to fully function.\n\n\n\n\n\nStep 2: Create a Conda Virtual Environment\nYou can create an environment in one step using the shared environment.yml file.\nconda env create -f environment.yml\nIf you want to create your own environment or use existing ones, install these necessary packages to allow your Python to work with Jupyter Notebooks in VS Code:\n\njupyter\nipykernel\npyyaml\n\nconda install jupyter ipykernel pyyaml -c conda-forge\n\n\nStep 3: Configure the Environment in VS Code.\n\nOpen the Command Palette (Ctrl+Shift+P on Windows / Cmd+Shift+X on Mac).\nSearch for and select ‚ÄúPython: Select Interpreter‚Äù.\nChoose your conda environment (e.g., python-intro-env).\n\nIf it doesn‚Äôt pop up, click Enter interpreter path‚Ä¶\nManually enter the path to your conda virtual environment Python executable:\n\nWindows: C:\\Users\\&lt;username&gt;\\AppData\\Local\\miniconda3\\envs\\&lt;env-name&gt;\\python.exe\nmacOS: Users/&lt;username&gt;/miniconda/envs/&lt;env-name&gt;/python\n\n\n\n\n\n\n\n\nüîéFind your conda Python executable path\n\n\n\nTo search for the conda Python interpreter location on your computer, open the Anaconda Prompt or terminal:\nconda activate &lt;env-name&gt;\nThen, locate your Python executable by typing the following:\n\nWindows: where python\nmacOS: which python\n\n\n\n\n\n\nStep 4: Open a Python Project and Create a Jupyter Notebook File\n\nIf you have an existing Python project you wish to work on in VS Code, you may open the project folder in VS Code.\n\nOpen from the VS Code Welcome page:\n\nOr by selecting File &gt; Open Folder (Ctrl+K Ctrl+O)\n\nTo create a new Jupyter Notebook file, go to File &gt; New File and select Jupyter Notebook (.ipynb).\nSelect the correct kernel (conda environment):\n\nIn the top-right corner of the notebook, you will see a Select Kernel button\nChoose the kernel that matches your conda environment (e.g., python-intro-env (Python 3.10.0))\n\nIf you don‚Äôt see the environment showing up:\n\nMake sure your conda env has the dependencies installed:\nconda install ipykernel jupyter pyyaml\nThen restart VS Code or reload the window (Ctrl+Shift+P &gt; Reload Window).\nIf all packages are installed but the issue persists, manually specify the Python path (see üîéFind your conda Python executable path)\n\n\nYou are all set!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Python Workshops",
    "section": "",
    "text": "This workshop series is geared toward R and SAS users who are interested in exploring Python‚Äìa powerful, versatile programming language that excels in many areas such as machine learning, large-scale data processing, and broader data science applications.\nOur goal is to help you build a solid foundation in Python and gain skills that can be integrated into your own work. This series will guide you through installing and setting up Python, understanding basic Python data structures, data manipulation through pandas, and, finally, applying machine learning methods using libraries such as scikit-learn.\nAll workshop materials, including pre-session handouts, coding demos, assignments, session slides, and video recordings, will be uploaded here. Be sure to check out the FAQ tab for common questions. If you have any questions, suggestions, or ideas for future sessions, please feel free to share them on our GitHub Discussions page. We value your feedback and aim to continually improve the workshops. We look forward to learning with you!"
  },
  {
    "objectID": "index.html#recordings",
    "href": "index.html#recordings",
    "title": "Introduction to Python Workshops",
    "section": "‚ñ∂Ô∏èRecordings",
    "text": "‚ñ∂Ô∏èRecordings\nRecordings will be uploaded after each session"
  },
  {
    "objectID": "index.html#useful-links",
    "href": "index.html#useful-links",
    "title": "Introduction to Python Workshops",
    "section": "üîóUseful Links",
    "text": "üîóUseful Links\n\nTutorials and Handouts\n\nTutorial: Installing Python and Essential Tools\nGo to Session2a: Intro to Python Data Structures \nGo to Session2b: Intro to Pandas \nSession 4: Object-Oriented Programming and Intro to ML Libraries - Extended tutorial\n\n\n\nDownloads\n \n\nInstall Anaconda\n\n \n\nInstall VS Code\n\n \n\nInstall Quarto\n\n\n \n\nDownload environment YML file\n\n \n\nDownload Follow Along Files\n\n\n \n\nDownload Intro to Pandas Dataset"
  },
  {
    "objectID": "FAQ.html",
    "href": "FAQ.html",
    "title": "FAQ",
    "section": "",
    "text": "Question: Is it possible (or recommended) to create a virtual environment in a specific project folder? Are there advantages or disadvantages to keeping the virtual environment in the default location versus a network drive (e.g., H drive)?\nAnswer: It is definitely possible to create an environment in a specific project folder/on the H drive (conda create --prefix /path/to/your/project_folder/env_name)! Note that, to activate the environment, you need to activate it by the full path, not by a name‚Äìe.g., conda activate /path/to/your/project_folder/env_name.\n\nAdvantages:\n\nBackup and Portability: Keeping the environment with the project makes it easier to back up as a single unit and enables access on multiple devices without recreating the environment each time.\n\nDisadvantages:\n\nPerformance: Access speed may be slower on a network drive, and network or permissions issues could make the environment temporarily inaccessible. If you typically work off a network drive, storing your environments within the project folder can be beneficial for organization and consistency.\n\n\n\n\n\n\n\n\nActivating environments with custom folder locations\n\n\n\nYou need to activate the conda environment stored outside of the default location by the path, not by a name.\nconda activate /path/to/your/project_folder/env_name\nYou may also create a symbolic link in the default envs folder to point to your custom environment location.\n\nOn macOS:\nln -s /path/to/your/project_folder/env_name /path/to/conda/envs/&lt;custom_env_name&gt;\nOn Windows:\nmklink /J \"C:\\path\\to\\conda\\envs\\&lt;custom_env_name&gt;\" \"H:\\path\\to\\your\\project_folder\\env_name\"\n\nNow, you will be able to activate the environment by name only.\nconda activate &lt;custom_env_name&gt;\n\n\n\nQuestion: Will creating a virtual environment for each project lead to memory or storage issues? What is the best practice for closing out a project environment?\nAnswer: In general, having a virtual environment for each active project shouldn‚Äôt cause memory issues. However, it‚Äôs good practice to clean up environments when a project concludes.\n\nHere‚Äôs a recommended process:\n\nActivating the environment you want to export.\nExporting the environment to a .yaml file with conda env export &gt; environment.yaml.\nAfter the .yaml file is created (make sure to check before deleting the environment!), you can delete the environment with conda env remove --name env_name (or conda env remove --prefix path/to/env_name if using a specific path).\n\nTip: By exporting the environment before deletion, you can always recreate the environment later if needed by using:\nconda env create --prefix /path/to/your/project_folder/env_name -f environment.yaml\nconda activate /path/to/your/project_folder/env_name"
  },
  {
    "objectID": "FAQ.html#managing-virtual-environments-with-conda",
    "href": "FAQ.html#managing-virtual-environments-with-conda",
    "title": "FAQ",
    "section": "",
    "text": "Question: Is it possible (or recommended) to create a virtual environment in a specific project folder? Are there advantages or disadvantages to keeping the virtual environment in the default location versus a network drive (e.g., H drive)?\nAnswer: It is definitely possible to create an environment in a specific project folder/on the H drive (conda create --prefix /path/to/your/project_folder/env_name)! Note that, to activate the environment, you need to activate it by the full path, not by a name‚Äìe.g., conda activate /path/to/your/project_folder/env_name.\n\nAdvantages:\n\nBackup and Portability: Keeping the environment with the project makes it easier to back up as a single unit and enables access on multiple devices without recreating the environment each time.\n\nDisadvantages:\n\nPerformance: Access speed may be slower on a network drive, and network or permissions issues could make the environment temporarily inaccessible. If you typically work off a network drive, storing your environments within the project folder can be beneficial for organization and consistency.\n\n\n\n\n\n\n\n\nActivating environments with custom folder locations\n\n\n\nYou need to activate the conda environment stored outside of the default location by the path, not by a name.\nconda activate /path/to/your/project_folder/env_name\nYou may also create a symbolic link in the default envs folder to point to your custom environment location.\n\nOn macOS:\nln -s /path/to/your/project_folder/env_name /path/to/conda/envs/&lt;custom_env_name&gt;\nOn Windows:\nmklink /J \"C:\\path\\to\\conda\\envs\\&lt;custom_env_name&gt;\" \"H:\\path\\to\\your\\project_folder\\env_name\"\n\nNow, you will be able to activate the environment by name only.\nconda activate &lt;custom_env_name&gt;\n\n\n\nQuestion: Will creating a virtual environment for each project lead to memory or storage issues? What is the best practice for closing out a project environment?\nAnswer: In general, having a virtual environment for each active project shouldn‚Äôt cause memory issues. However, it‚Äôs good practice to clean up environments when a project concludes.\n\nHere‚Äôs a recommended process:\n\nActivating the environment you want to export.\nExporting the environment to a .yaml file with conda env export &gt; environment.yaml.\nAfter the .yaml file is created (make sure to check before deleting the environment!), you can delete the environment with conda env remove --name env_name (or conda env remove --prefix path/to/env_name if using a specific path).\n\nTip: By exporting the environment before deletion, you can always recreate the environment later if needed by using:\nconda env create --prefix /path/to/your/project_folder/env_name -f environment.yaml\nconda activate /path/to/your/project_folder/env_name"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "link.html",
    "href": "link.html",
    "title": "Links",
    "section": "",
    "text": "Links for downloading and installing mentioned software/files.\n \n\nInstall Anaconda\n\n\n \n\nDownload environment YML file\n\n\n \n\nDownload Follow Along File\n\n\n \n\nDownload Dataset for Intro to Pandas"
  },
  {
    "objectID": "session4_newMLDemo/sess4preread.html",
    "href": "session4_newMLDemo/sess4preread.html",
    "title": "Session 4 ‚Äì Pre-read",
    "section": "",
    "text": "We will be using sci-kit learn implementations of both of these algorithms during the session 4 tutorial.\n\n\nKNN is a supervised learning algorithm used for classification (and sometimes regression):\n\nYou train the model on labeled data (i.e., you know the ‚Äúanswer‚Äù or class).\nWhen predicting a new sample, the model finds the k training samples closest to it (its ‚Äúneighbors‚Äù) and uses them to assign a label.\nCloseness is usually based on Euclidean distance.\n\n\nExample: Given a penguin with known bill length and depth, predict its species by looking at its 5 nearest neighbors in the training data.\n\n\n\n\n\nK-Means is an unsupervised learning algorithm used for clustering:\n\nYou do not provide the true labels.\nThe algorithm tries to split your data into k groups based on similarity.\nIt randomly initializes cluster centers, assigns points to the nearest one, then updates the centers iteratively.\n\n\nExample: Given penguin data without species labels, group them into 3 clusters based on bill length and depth.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nKNN\nK-Means\n\n\n\n\nLearning Type\nSupervised\nUnsupervised\n\n\nGoal\nClassification (or Regression)\nClustering\n\n\nInput Labels\nRequired\nNot used\n\n\nOutput\nPredicted class\nCluster assignment\n\n\nModel Type\nLazy (no training phase)\nIterative center updates"
  },
  {
    "objectID": "session4_newMLDemo/sess4preread.html#knn-vs-k-means-supervised-vs-unsupervised-learning",
    "href": "session4_newMLDemo/sess4preread.html#knn-vs-k-means-supervised-vs-unsupervised-learning",
    "title": "Session 4 ‚Äì Pre-read",
    "section": "",
    "text": "We will be using sci-kit learn implementations of both of these algorithms during the session 4 tutorial.\n\n\nKNN is a supervised learning algorithm used for classification (and sometimes regression):\n\nYou train the model on labeled data (i.e., you know the ‚Äúanswer‚Äù or class).\nWhen predicting a new sample, the model finds the k training samples closest to it (its ‚Äúneighbors‚Äù) and uses them to assign a label.\nCloseness is usually based on Euclidean distance.\n\n\nExample: Given a penguin with known bill length and depth, predict its species by looking at its 5 nearest neighbors in the training data.\n\n\n\n\n\nK-Means is an unsupervised learning algorithm used for clustering:\n\nYou do not provide the true labels.\nThe algorithm tries to split your data into k groups based on similarity.\nIt randomly initializes cluster centers, assigns points to the nearest one, then updates the centers iteratively.\n\n\nExample: Given penguin data without species labels, group them into 3 clusters based on bill length and depth.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature\nKNN\nK-Means\n\n\n\n\nLearning Type\nSupervised\nUnsupervised\n\n\nGoal\nClassification (or Regression)\nClustering\n\n\nInput Labels\nRequired\nNot used\n\n\nOutput\nPredicted class\nCluster assignment\n\n\nModel Type\nLazy (no training phase)\nIterative center updates"
  },
  {
    "objectID": "session4_newMLDemo/sess4preread.html#plotting-in-python",
    "href": "session4_newMLDemo/sess4preread.html#plotting-in-python",
    "title": "Session 4 ‚Äì Pre-read",
    "section": "Plotting in Python",
    "text": "Plotting in Python\nPlease read the ‚ÄòParts of a Figure‚Äô and ‚ÄòCoding Styles‚Äô sections of Quick Start Guide (Matplotlib). We will briefly cover plotting with Seaborn (which is built on top of the Matplotlib package), but will not spend much time talking about base Matplotlib."
  },
  {
    "objectID": "session2b/session2_2.html",
    "href": "session2b/session2_2.html",
    "title": "Python Machine Learning Demo",
    "section": "",
    "text": "Guide to Python Data Structures\n\n  \n\nCancer Dataset"
  },
  {
    "objectID": "session2b/session2_2.html#links",
    "href": "session2b/session2_2.html#links",
    "title": "Python Machine Learning Demo",
    "section": "",
    "text": "Guide to Python Data Structures\n\n  \n\nCancer Dataset"
  },
  {
    "objectID": "session2b/session2_2.html#topic-5-intro-to-pandas",
    "href": "session2b/session2_2.html#topic-5-intro-to-pandas",
    "title": "Python Machine Learning Demo",
    "section": "Topic 5: Intro to Pandas",
    "text": "Topic 5: Intro to Pandas\nPandas is a powerful open-source data analysis and manipulation library in Python. It provides data structures, primarily the DataFrame and Series, which are optimized for handling and analyzing large datasets efficiently.\nData Structures:\nSeries: A one-dimensional labeled array, suitable for handling single columns or rows of data.\n\nDataFrame: A two-dimensional table with labeled axes (rows and columns), much like a spreadsheet or SQL table, allowing you to work with data in rows and columns simultaneously.\nData Manipulation:\nPandas has functions for merging, reshaping, and aggregating datasets, which helps streamline data cleaning and preparation.\n\nIt can handle missing data, making it easy to filter or fill gaps in datasets.\nData Analysis:\nIt provides extensive functionality for descriptive statistics, grouping data, and handling time series.\n\nIntegrates well with other libraries, making it easy to move data between libraries like NumPy for numerical computations and Matplotlib or Seaborn for visualization.\nLoading the Dataset\n\nimport os\nimport pandas as pd\n\n# Load the dataset\ncancer_data = pd.read_csv(os.path.join('example_data', 'Cancer_Data.csv'))\n\n# Display the first few rows of the dataset\ncancer_data.head()\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\n...\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\nUnnamed: 32\n\n\n\n\n0\n842302\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\nNaN\n\n\n1\n842517\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\nNaN\n\n\n2\n84300903\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\nNaN\n\n\n3\n84348301\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\nNaN\n\n\n4\n84358402\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\nNaN\n\n\n\n\n5 rows √ó 33 columns\n\n\n\nViewing Basic Information a. Checking the Dataset‚Äôs Shape\n.shape returns a tuple with (number of rows, number of columns), which provides a basic overview of the dataset size.\n\n# Display the shape of the dataset\nprint(\"Dataset Shape:\", cancer_data.shape)\n\nDataset Shape: (569, 33)\n\n\n\nSummarizing Column Information\n\n.info() lists all columns, their data types, and counts of non-null values, helping identify any columns that may have missing data.\n\n# Display column names, data types, and non-null counts\ncancer_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 33 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   id                       569 non-null    int64  \n 1   diagnosis                569 non-null    object \n 2   radius_mean              569 non-null    float64\n 3   texture_mean             569 non-null    float64\n 4   perimeter_mean           569 non-null    float64\n 5   area_mean                569 non-null    float64\n 6   smoothness_mean          569 non-null    float64\n 7   compactness_mean         569 non-null    float64\n 8   concavity_mean           569 non-null    float64\n 9   concave points_mean      569 non-null    float64\n 10  symmetry_mean            569 non-null    float64\n 11  fractal_dimension_mean   569 non-null    float64\n 12  radius_se                569 non-null    float64\n 13  texture_se               569 non-null    float64\n 14  perimeter_se             569 non-null    float64\n 15  area_se                  569 non-null    float64\n 16  smoothness_se            569 non-null    float64\n 17  compactness_se           569 non-null    float64\n 18  concavity_se             569 non-null    float64\n 19  concave points_se        569 non-null    float64\n 20  symmetry_se              569 non-null    float64\n 21  fractal_dimension_se     569 non-null    float64\n 22  radius_worst             569 non-null    float64\n 23  texture_worst            569 non-null    float64\n 24  perimeter_worst          569 non-null    float64\n 25  area_worst               569 non-null    float64\n 26  smoothness_worst         569 non-null    float64\n 27  compactness_worst        569 non-null    float64\n 28  concavity_worst          569 non-null    float64\n 29  concave points_worst     569 non-null    float64\n 30  symmetry_worst           569 non-null    float64\n 31  fractal_dimension_worst  569 non-null    float64\n 32  Unnamed: 32              0 non-null      float64\ndtypes: float64(31), int64(1), object(1)\nmemory usage: 146.8+ KB\n\n\n\nViewing Column Names\n\n.columns lists column headers, while .tolist() converts it into a standard Python list for easier viewing.\n\n# Display column names\nprint(\"Column Names:\", cancer_data.columns.tolist())\n\nColumn Names: ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32']\n\n\nSummary Statistics\n.describe() generates essential statistics (mean, std, min, max, percentiles) for numeric columns, useful for identifying data distributions.\n\n# Generate summary statistics for numeric columns\ncancer_data.describe()\n\n\n\n\n\n\n\n\nid\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\n...\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\nUnnamed: 32\n\n\n\n\ncount\n5.690000e+02\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n...\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n0.0\n\n\nmean\n3.037183e+07\n14.127292\n19.289649\n91.969033\n654.889104\n0.096360\n0.104341\n0.088799\n0.048919\n0.181162\n...\n25.677223\n107.261213\n880.583128\n0.132369\n0.254265\n0.272188\n0.114606\n0.290076\n0.083946\nNaN\n\n\nstd\n1.250206e+08\n3.524049\n4.301036\n24.298981\n351.914129\n0.014064\n0.052813\n0.079720\n0.038803\n0.027414\n...\n6.146258\n33.602542\n569.356993\n0.022832\n0.157336\n0.208624\n0.065732\n0.061867\n0.018061\nNaN\n\n\nmin\n8.670000e+03\n6.981000\n9.710000\n43.790000\n143.500000\n0.052630\n0.019380\n0.000000\n0.000000\n0.106000\n...\n12.020000\n50.410000\n185.200000\n0.071170\n0.027290\n0.000000\n0.000000\n0.156500\n0.055040\nNaN\n\n\n25%\n8.692180e+05\n11.700000\n16.170000\n75.170000\n420.300000\n0.086370\n0.064920\n0.029560\n0.020310\n0.161900\n...\n21.080000\n84.110000\n515.300000\n0.116600\n0.147200\n0.114500\n0.064930\n0.250400\n0.071460\nNaN\n\n\n50%\n9.060240e+05\n13.370000\n18.840000\n86.240000\n551.100000\n0.095870\n0.092630\n0.061540\n0.033500\n0.179200\n...\n25.410000\n97.660000\n686.500000\n0.131300\n0.211900\n0.226700\n0.099930\n0.282200\n0.080040\nNaN\n\n\n75%\n8.813129e+06\n15.780000\n21.800000\n104.100000\n782.700000\n0.105300\n0.130400\n0.130700\n0.074000\n0.195700\n...\n29.720000\n125.400000\n1084.000000\n0.146000\n0.339100\n0.382900\n0.161400\n0.317900\n0.092080\nNaN\n\n\nmax\n9.113205e+08\n28.110000\n39.280000\n188.500000\n2501.000000\n0.163400\n0.345400\n0.426800\n0.201200\n0.304000\n...\n49.540000\n251.200000\n4254.000000\n0.222600\n1.058000\n1.252000\n0.291000\n0.663800\n0.207500\nNaN\n\n\n\n\n8 rows √ó 32 columns\n\n\n\nUsing value_counts() on a Single Column\nThis method is straightforward if you want to check the frequency distribution of one specific categorical column. Returns a pandas series object\n\n# Count occurrences of each unique value in the 'diagnosis' column\ndiagnosis_counts = cancer_data['diagnosis'].value_counts()\nprint(\"Diagnosis Counts:\\n\", diagnosis_counts)\n\nDiagnosis Counts:\n diagnosis\nB    357\nM    212\nName: count, dtype: int64\n\n\nTo see summary statistics grouped by a categorical variable in pandas, you can use the groupby() method along with describe() or specific aggregation functions like mean(), sum(), etc.\n\n# Group by 'diagnosis' and get summary statistics for each group\ngrouped_summary = cancer_data.groupby('diagnosis').mean()\nprint(grouped_summary)\n\n\n#Group by 'diagnosis' and get summary statistics for only one variable\ngrouped_radius_mean = cancer_data.groupby('diagnosis')['radius_mean'].mean()\nprint(grouped_radius_mean)\n\n                     id  radius_mean  texture_mean  perimeter_mean  \\\ndiagnosis                                                            \nB          2.654382e+07    12.146524     17.914762       78.075406   \nM          3.681805e+07    17.462830     21.604906      115.365377   \n\n            area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\ndiagnosis                                                                  \nB          462.790196         0.092478          0.080085        0.046058   \nM          978.376415         0.102898          0.145188        0.160775   \n\n           concave points_mean  symmetry_mean  ...  texture_worst  \\\ndiagnosis                                      ...                  \nB                     0.025717       0.174186  ...      23.515070   \nM                     0.087990       0.192909  ...      29.318208   \n\n           perimeter_worst   area_worst  smoothness_worst  compactness_worst  \\\ndiagnosis                                                                      \nB                87.005938   558.899440          0.124959           0.182673   \nM               141.370330  1422.286321          0.144845           0.374824   \n\n           concavity_worst  concave points_worst  symmetry_worst  \\\ndiagnosis                                                          \nB                 0.166238              0.074444        0.270246   \nM                 0.450606              0.182237        0.323468   \n\n           fractal_dimension_worst  Unnamed: 32  \ndiagnosis                                        \nB                         0.079442          NaN  \nM                         0.091530          NaN  \n\n[2 rows x 32 columns]\ndiagnosis\nB    12.146524\nM    17.462830\nName: radius_mean, dtype: float64\n\n\nRenaming Columns To make column names more readable or consistent, you can use rename() to change specific names. Here‚Äôs how to rename columns like radius_mean to Radius Mean.\n\n# Rename specific columns for readability\n\nnew_columns={\n    'radius_mean': 'Radius Mean',\n    'texture_mean': 'Texture Mean',\n    'perimeter_mean': 'Perimeter Mean'\n}\n\ncancer_data = cancer_data.rename(columns=new_columns)\n\n# Display the new column names to verify the changes\nprint(\"\\nUpdated Column Names:\", cancer_data.columns.tolist())\n\n\nUpdated Column Names: ['id', 'diagnosis', 'Radius Mean', 'Texture Mean', 'Perimeter Mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32']\n\n\nTo find missing values, you can use isnull() with sum() to calculate the total number of missing values in each column.\n\n# Count missing values in each column\nmissing_values = cancer_data.isnull().sum()\nprint(\"Missing Values per Column:\")\nprint(missing_values)\n\nMissing Values per Column:\nid                           0\ndiagnosis                    0\nRadius Mean                  0\nTexture Mean                 0\nPerimeter Mean               0\narea_mean                    0\nsmoothness_mean              0\ncompactness_mean             0\nconcavity_mean               0\nconcave points_mean          0\nsymmetry_mean                0\nfractal_dimension_mean       0\nradius_se                    0\ntexture_se                   0\nperimeter_se                 0\narea_se                      0\nsmoothness_se                0\ncompactness_se               0\nconcavity_se                 0\nconcave points_se            0\nsymmetry_se                  0\nfractal_dimension_se         0\nradius_worst                 0\ntexture_worst                0\nperimeter_worst              0\narea_worst                   0\nsmoothness_worst             0\ncompactness_worst            0\nconcavity_worst              0\nconcave points_worst         0\nsymmetry_worst               0\nfractal_dimension_worst      0\nUnnamed: 32                569\ndtype: int64\n\n\nDropping Columns with Excessive Missing Data Since Unnamed: 32 has no data, it can be dropped from the DataFrame using .drop().\n\n# Drop the 'Unnamed: 32' column if it contains no data\ncancer_data = cancer_data.drop(columns=['Unnamed: 32'])\n\n# Verify the column has been dropped\nprint(\"\\nColumns after dropping 'Unnamed: 32':\", cancer_data.columns.tolist())\n\n\nColumns after dropping 'Unnamed: 32': ['id', 'diagnosis', 'Radius Mean', 'Texture Mean', 'Perimeter Mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n\n\nColumn Selection Selecting specific columns is essential for focusing on particular aspects of the dataset. Here are some examples of both single and multiple column selections.\n\n# Select the 'diagnosis' column - diagnosis_column will be a series\ndiagnosis_column = cancer_data['diagnosis']\nprint(\"Diagnosis Column:\\n\", diagnosis_column.head())\n\nDiagnosis Column:\n 0    M\n1    M\n2    M\n3    M\n4    M\nName: diagnosis, dtype: object\n\n\nAlternatively, you can select multiple columns.\n\n# Select multiple columns: 'diagnosis', 'radius_mean', and 'area_mean' - selected_columns will be a pandas DataFrame\n\nselected_columns = cancer_data[['diagnosis', 'Radius Mean', 'area_mean']]\nprint(\"Selected Columns:\\n\", selected_columns.head())\n\nSelected Columns:\n   diagnosis  Radius Mean  area_mean\n0         M        17.99     1001.0\n1         M        20.57     1326.0\n2         M        19.69     1203.0\n3         M        11.42      386.1\n4         M        20.29     1297.0\n\n\nRow Selection Selecting rows based on labels or positions is helpful for inspecting specific data points or subsets.\n\nLabel-Based Indexing with loc loc allows selection based on labels (e.g., column names or index labels) and is particularly useful for data subsets.\n\n\n# Select rows by labels (assuming integer index here) and specific columns\nselected_rows_labels = cancer_data.loc[0:4, ['diagnosis', 'Radius Mean', 'area_mean']]\nprint(\"Selected Rows with loc:\\n\", selected_rows_labels)\n\nSelected Rows with loc:\n   diagnosis  Radius Mean  area_mean\n0         M        17.99     1001.0\n1         M        20.57     1326.0\n2         M        19.69     1203.0\n3         M        11.42      386.1\n4         M        20.29     1297.0\n\n\n\nInteger-Based Indexing with iloc iloc allows selection based purely on integer positions, making it convenient for slicing and position-based operations.\n\n\n# Select rows by integer position and specific columns\nselected_rows_position = cancer_data.iloc[0:5, [1, 2, 3]]  # Select first 5 rows and columns at position 1, 2, 3\nprint(\"Selected Rows with iloc:\\n\", selected_rows_position)\n\nSelected Rows with iloc:\n   diagnosis  Radius Mean  Texture Mean\n0         M        17.99         10.38\n1         M        20.57         17.77\n2         M        19.69         21.25\n3         M        11.42         20.38\n4         M        20.29         14.34\n\n\nFiltering enables you to create subsets of data that match specific conditions. For example, we can filter by diagnosis to analyze only malignant (M) or benign (B) cases.\n\n# Filter rows where 'diagnosis' is \"M\" (Malignant)\nmalignant_cases = cancer_data[cancer_data['diagnosis'] == 'M']\nprint(\"Malignant Cases:\\n\", malignant_cases.head(20))\n\nMalignant Cases:\n           id diagnosis  Radius Mean  Texture Mean  Perimeter Mean  area_mean  \\\n0     842302         M        17.99         10.38          122.80     1001.0   \n1     842517         M        20.57         17.77          132.90     1326.0   \n2   84300903         M        19.69         21.25          130.00     1203.0   \n3   84348301         M        11.42         20.38           77.58      386.1   \n4   84358402         M        20.29         14.34          135.10     1297.0   \n5     843786         M        12.45         15.70           82.57      477.1   \n6     844359         M        18.25         19.98          119.60     1040.0   \n7   84458202         M        13.71         20.83           90.20      577.9   \n8     844981         M        13.00         21.82           87.50      519.8   \n9   84501001         M        12.46         24.04           83.97      475.9   \n10    845636         M        16.02         23.24          102.70      797.8   \n11  84610002         M        15.78         17.89          103.60      781.0   \n12    846226         M        19.17         24.80          132.40     1123.0   \n13    846381         M        15.85         23.95          103.70      782.7   \n14  84667401         M        13.73         22.61           93.60      578.3   \n15  84799002         M        14.54         27.54           96.73      658.8   \n16    848406         M        14.68         20.13           94.74      684.5   \n17  84862001         M        16.13         20.68          108.10      798.8   \n18    849014         M        19.81         22.15          130.00     1260.0   \n22   8511133         M        15.34         14.26          102.50      704.4   \n\n    smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0           0.11840           0.27760         0.30010              0.14710   \n1           0.08474           0.07864         0.08690              0.07017   \n2           0.10960           0.15990         0.19740              0.12790   \n3           0.14250           0.28390         0.24140              0.10520   \n4           0.10030           0.13280         0.19800              0.10430   \n5           0.12780           0.17000         0.15780              0.08089   \n6           0.09463           0.10900         0.11270              0.07400   \n7           0.11890           0.16450         0.09366              0.05985   \n8           0.12730           0.19320         0.18590              0.09353   \n9           0.11860           0.23960         0.22730              0.08543   \n10          0.08206           0.06669         0.03299              0.03323   \n11          0.09710           0.12920         0.09954              0.06606   \n12          0.09740           0.24580         0.20650              0.11180   \n13          0.08401           0.10020         0.09938              0.05364   \n14          0.11310           0.22930         0.21280              0.08025   \n15          0.11390           0.15950         0.16390              0.07364   \n16          0.09867           0.07200         0.07395              0.05259   \n17          0.11700           0.20220         0.17220              0.10280   \n18          0.09831           0.10270         0.14790              0.09498   \n22          0.10730           0.21350         0.20770              0.09756   \n\n    ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n0   ...         25.38          17.33           184.60      2019.0   \n1   ...         24.99          23.41           158.80      1956.0   \n2   ...         23.57          25.53           152.50      1709.0   \n3   ...         14.91          26.50            98.87       567.7   \n4   ...         22.54          16.67           152.20      1575.0   \n5   ...         15.47          23.75           103.40       741.6   \n6   ...         22.88          27.66           153.20      1606.0   \n7   ...         17.06          28.14           110.60       897.0   \n8   ...         15.49          30.73           106.20       739.3   \n9   ...         15.09          40.68            97.65       711.4   \n10  ...         19.19          33.88           123.80      1150.0   \n11  ...         20.42          27.28           136.50      1299.0   \n12  ...         20.96          29.94           151.70      1332.0   \n13  ...         16.84          27.66           112.00       876.5   \n14  ...         15.03          32.01           108.80       697.7   \n15  ...         17.46          37.13           124.10       943.2   \n16  ...         19.07          30.88           123.40      1138.0   \n17  ...         20.96          31.48           136.80      1315.0   \n18  ...         27.32          30.88           186.80      2398.0   \n22  ...         18.07          19.08           125.10       980.9   \n\n    smoothness_worst  compactness_worst  concavity_worst  \\\n0             0.1622             0.6656           0.7119   \n1             0.1238             0.1866           0.2416   \n2             0.1444             0.4245           0.4504   \n3             0.2098             0.8663           0.6869   \n4             0.1374             0.2050           0.4000   \n5             0.1791             0.5249           0.5355   \n6             0.1442             0.2576           0.3784   \n7             0.1654             0.3682           0.2678   \n8             0.1703             0.5401           0.5390   \n9             0.1853             1.0580           1.1050   \n10            0.1181             0.1551           0.1459   \n11            0.1396             0.5609           0.3965   \n12            0.1037             0.3903           0.3639   \n13            0.1131             0.1924           0.2322   \n14            0.1651             0.7725           0.6943   \n15            0.1678             0.6577           0.7026   \n16            0.1464             0.1871           0.2914   \n17            0.1789             0.4233           0.4784   \n18            0.1512             0.3150           0.5372   \n22            0.1390             0.5954           0.6305   \n\n    concave points_worst  symmetry_worst  fractal_dimension_worst  \n0                0.26540          0.4601                  0.11890  \n1                0.18600          0.2750                  0.08902  \n2                0.24300          0.3613                  0.08758  \n3                0.25750          0.6638                  0.17300  \n4                0.16250          0.2364                  0.07678  \n5                0.17410          0.3985                  0.12440  \n6                0.19320          0.3063                  0.08368  \n7                0.15560          0.3196                  0.11510  \n8                0.20600          0.4378                  0.10720  \n9                0.22100          0.4366                  0.20750  \n10               0.09975          0.2948                  0.08452  \n11               0.18100          0.3792                  0.10480  \n12               0.17670          0.3176                  0.10230  \n13               0.11190          0.2809                  0.06287  \n14               0.22080          0.3596                  0.14310  \n15               0.17120          0.4218                  0.13410  \n16               0.16090          0.3029                  0.08216  \n17               0.20730          0.3706                  0.11420  \n18               0.23880          0.2768                  0.07615  \n22               0.23930          0.4667                  0.09946  \n\n[20 rows x 32 columns]\n\n\nYou can also filter based on multiple conditions, such as finding rows where the diagnosis is ‚ÄúM‚Äù and radius_mean is greater than 15.\nNote: You can‚Äôt use ‚Äòand‚Äô python operator here, because ‚Äòand‚Äô is a keyword for Python‚Äôs boolean operations, which work with single True or False values, not arrays or Series.\n\n# Filter for Malignant cases with radius_mean &gt; 15\nlarge_malignant_cases = cancer_data[(cancer_data['diagnosis'] == 'M') & (cancer_data['Radius Mean'] &gt; 15)]\nprint(\"Large Malignant Cases (Radius Mean &gt; 15):\\n\", large_malignant_cases.head())\n\nLarge Malignant Cases (Radius Mean &gt; 15):\n          id diagnosis  Radius Mean  Texture Mean  Perimeter Mean  area_mean  \\\n0    842302         M        17.99         10.38           122.8     1001.0   \n1    842517         M        20.57         17.77           132.9     1326.0   \n2  84300903         M        19.69         21.25           130.0     1203.0   \n4  84358402         M        20.29         14.34           135.1     1297.0   \n6    844359         M        18.25         19.98           119.6     1040.0   \n\n   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0          0.11840           0.27760          0.3001              0.14710   \n1          0.08474           0.07864          0.0869              0.07017   \n2          0.10960           0.15990          0.1974              0.12790   \n4          0.10030           0.13280          0.1980              0.10430   \n6          0.09463           0.10900          0.1127              0.07400   \n\n   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n0  ...         25.38          17.33            184.6      2019.0   \n1  ...         24.99          23.41            158.8      1956.0   \n2  ...         23.57          25.53            152.5      1709.0   \n4  ...         22.54          16.67            152.2      1575.0   \n6  ...         22.88          27.66            153.2      1606.0   \n\n   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n0            0.1622             0.6656           0.7119                0.2654   \n1            0.1238             0.1866           0.2416                0.1860   \n2            0.1444             0.4245           0.4504                0.2430   \n4            0.1374             0.2050           0.4000                0.1625   \n6            0.1442             0.2576           0.3784                0.1932   \n\n   symmetry_worst  fractal_dimension_worst  \n0          0.4601                  0.11890  \n1          0.2750                  0.08902  \n2          0.3613                  0.08758  \n4          0.2364                  0.07678  \n6          0.3063                  0.08368  \n\n[5 rows x 32 columns]\n\n\nAdding and Modifying Columns\nAdding New Columns You can create new columns in a DataFrame based on calculations using existing columns. For example, we can calculate the area_ratio by dividing area_worst by area_mean.\n\n# Add a new column 'area_ratio' by dividing 'area_worst' by 'area_mean'\ncancer_data['area_ratio'] = cancer_data['area_worst'] / cancer_data['area_mean']\nprint(\"New Column 'area_ratio':\\n\", cancer_data[['area_worst', 'area_mean', 'area_ratio']].head())\n\nNew Column 'area_ratio':\n    area_worst  area_mean  area_ratio\n0      2019.0     1001.0    2.016983\n1      1956.0     1326.0    1.475113\n2      1709.0     1203.0    1.420615\n3       567.7      386.1    1.470344\n4      1575.0     1297.0    1.214341\n\n\nChanging a Value Using .at Suppose you have a DataFrame and want to update the value in the radius_mean column for a particular index.\n\n# Access and print the original value at index 0 and column 'radius_mean'\noriginal_value = cancer_data.at[0, 'Radius Mean']\nprint(\"Original Radius Mean at index 0:\", original_value)\n\n\n# Change the value at index 0 and column 'radius_mean' to 18.5\ncancer_data.at[0, 'Radius Mean'] = 18.5\n\n\n# Verify the updated value\nupdated_value = cancer_data.at[0, 'Radius Mean']\nprint(\"Updated Radius Mean at index 0:\", updated_value)\n\nOriginal Radius Mean at index 0: 17.99\nUpdated Radius Mean at index 0: 18.5\n\n\nSorting by Columns You can sort a dataset by columns. Here‚Äôs how to sort by diagnosis first and then by area_mean in ascending order.\n\n# Sort by 'diagnosis' first, then by 'area_mean' within each diagnosis group\nsorted_by_diagnosis_area = cancer_data.sort_values(by=['diagnosis', 'area_mean'], ascending=[True, True])\nprint(\"Data sorted by Diagnosis and Area Mean:\\n\", sorted_by_diagnosis_area[['diagnosis', 'area_mean', 'Radius Mean']].head())\n\nData sorted by Diagnosis and Area Mean:\n     diagnosis  area_mean  Radius Mean\n101         B      143.5        6.981\n539         B      170.4        7.691\n538         B      178.8        7.729\n568         B      181.0        7.760\n46          B      201.9        8.196\n\n\nReordering Columns to Move a Column to the End You might also want to move a specific column to the end of the DataFrame, such as moving area_ratio to the last position.\n\n# Move 'area_ratio' to the end of the DataFrame\ncolumns_reordered = [col for col in cancer_data.columns if col != 'area_ratio'] + ['area_ratio']\ncancer_data_with_area_ratio_last = cancer_data[columns_reordered]\n\n# Display the reordered columns\nprint(\"Data with 'area_ratio' at the end:\\n\", cancer_data_with_area_ratio_last.head())\n\nData with 'area_ratio' at the end:\n          id diagnosis  Radius Mean  Texture Mean  Perimeter Mean  area_mean  \\\n0    842302         M        18.50         10.38          122.80     1001.0   \n1    842517         M        20.57         17.77          132.90     1326.0   \n2  84300903         M        19.69         21.25          130.00     1203.0   \n3  84348301         M        11.42         20.38           77.58      386.1   \n4  84358402         M        20.29         14.34          135.10     1297.0   \n\n   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n0          0.11840           0.27760          0.3001              0.14710   \n1          0.08474           0.07864          0.0869              0.07017   \n2          0.10960           0.15990          0.1974              0.12790   \n3          0.14250           0.28390          0.2414              0.10520   \n4          0.10030           0.13280          0.1980              0.10430   \n\n   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n0  ...          17.33           184.60      2019.0            0.1622   \n1  ...          23.41           158.80      1956.0            0.1238   \n2  ...          25.53           152.50      1709.0            0.1444   \n3  ...          26.50            98.87       567.7            0.2098   \n4  ...          16.67           152.20      1575.0            0.1374   \n\n   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n0             0.6656           0.7119                0.2654          0.4601   \n1             0.1866           0.2416                0.1860          0.2750   \n2             0.4245           0.4504                0.2430          0.3613   \n3             0.8663           0.6869                0.2575          0.6638   \n4             0.2050           0.4000                0.1625          0.2364   \n\n   fractal_dimension_worst  area_ratio  \n0                  0.11890    2.016983  \n1                  0.08902    1.475113  \n2                  0.08758    1.420615  \n3                  0.17300    1.470344  \n4                  0.07678    1.214341  \n\n[5 rows x 33 columns]\n\n\nApplying Functions to Columns\nUsing apply() to Apply Custom Functions The .apply() method in pandas lets you apply a custom function to each element in a Series (column) or DataFrame. Here‚Äôs how to use it to categorize tumors based on area_mean.\nExample: Categorizing Tumors by Size Let‚Äôs create a custom function to categorize tumors as ‚ÄúSmall‚Äù, ‚ÄúMedium‚Äù, or ‚ÄúLarge‚Äù based on area_mean.\n\n# Define a custom function to categorize tumors by area_mean\ndef categorize_tumor(size):\n    if size &lt; 500:\n        return 'Small'\n    elif 500 &lt;= size &lt; 1000:\n        return 'Medium'\n    else:\n        return 'Large'\n\n# Apply the function to the 'area_mean' column and create a new column 'tumor_size_category'\ncancer_data['tumor_size_category'] = cancer_data['area_mean'].apply(categorize_tumor)\n\n# Display the new column to verify the transformation\nprint(\"Tumor Size Categories:\\n\", cancer_data[['area_mean', 'tumor_size_category']].head())\n\nTumor Size Categories:\n    area_mean tumor_size_category\n0     1001.0               Large\n1     1326.0               Large\n2     1203.0               Large\n3      386.1               Small\n4     1297.0               Large\n\n\nUsing Lambda Functions for Quick Transformations Lambda functions are useful for simple, one-line operations. For example, we can use a lambda function to convert diagnosis into numerical codes (0 for Benign, 1 for Malignant).\n\n# Apply a lambda function to classify 'diagnosis' into numerical codes\ncancer_data['diagnosis_code'] = cancer_data['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)\n\n# Display the new column to verify the transformation\nprint(\"Diagnosis Codes:\\n\", cancer_data[['diagnosis', 'diagnosis_code']].head())\n\nDiagnosis Codes:\n   diagnosis  diagnosis_code\n0         M               1\n1         M               1\n2         M               1\n3         M               1\n4         M               1\n\n\nApplying Multiple Conditions with apply()\nYou can also use apply() with a lambda function for more complex, multi-condition classifications.\nExample: Adding a Column with Risk Levels Suppose we want to create a new column, risk_level, based on both diagnosis and area_mean:\n‚ÄúHigh Risk‚Äù for Malignant tumors with area_mean above 1000. ‚ÄúModerate Risk‚Äù for Malignant tumors with area_mean below 1000. ‚ÄúLow Risk‚Äù for Benign tumors.\n\n# Apply a lambda function with multiple conditions to create a 'risk_level' column\ncancer_data['risk_level'] = cancer_data.apply(\n    lambda row: 'High Risk' if row['diagnosis'] == 'M' and row['area_mean'] &gt; 1000 \n    else ('Moderate Risk' if row['diagnosis'] == 'M' else 'Low Risk'), axis=1\n)\n\n# Display the new column to verify the transformation\nprint(\"Risk Levels:\\n\", cancer_data[['diagnosis', 'area_mean', 'risk_level']].head())\n\n#Axis=1 tells the function to apply it to the rows. axis=0 (default) applies function to the columns\n\nRisk Levels:\n   diagnosis  area_mean     risk_level\n0         M     1001.0      High Risk\n1         M     1326.0      High Risk\n2         M     1203.0      High Risk\n3         M      386.1  Moderate Risk\n4         M     1297.0      High Risk"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#session-overview",
    "href": "session4_newMLDemo/session4v2_slides.html#session-overview",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Session Overview",
    "text": "Session Overview\n\nIn this session, we‚Äôll explore how Python‚Äôs object-oriented nature affects our modeling workflows. \nTopics:\n\n\n\nIntro to OOP and how it makes modeling in Python different from R\n\n\nBuilding and extending classes using inheritance and mixins\n\n\nApplying OOP to machine learning through demos with scikit-learn\n\n\n\nCreating and using models\n\n\nPlotting data with plotnine and seaborn"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#why-python",
    "href": "session4_newMLDemo/session4v2_slides.html#why-python",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Why Python? üêç",
    "text": "Why Python? üêç\n\n\n\nR: Built by Statisticians for Statisticians\n\nExcels at:\n\nStatistical analysis and modeling\n\nClean outputs and tables from models\nBeautiful data visualizations with simple code\n\n\n\nPython: General-Purpose Language\n\nExcels at:\n\nMachine Learning, Neural Networks & Deep Learning (scikit-learn, PyTorch, TensorFlow)\n\nImage & Genomic Data Analysis (scikit-image, biopython, scanpy)\nSoftware & Command Line Interfaces, Web Scraping, Automation\n\n\n\nPython‚Äôs broader ecosystem makes it the go-to language in domains like AI, bioinformatics, data engineering, and computational biology.\n\nNote: Packages like rpy2 and reticulate make it possible to use both R and Python in the same project, but those are beyond the scope of this course.\nA primer on reticulate is available here: https://www.r-bloggers.com/2022/04/getting-started-with-python-using-r-and-reticulate/"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#programming-styles-r-vs-python",
    "href": "session4_newMLDemo/session4v2_slides.html#programming-styles-r-vs-python",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Programming Styles: R vs Python",
    "text": "Programming Styles: R vs Python\n\n In the first session, we talked briefly about functional vs object-oriented programming:\n\n\nFunctional programming: focuses on functions as the primary unit of code  Object-oriented programming: uses objects with attached attributes(data) and methods(behaviors) \n\n\nR leans heavily on the functional paradigm ‚Äî you pass data into functions and get back results, in most cases without altering the original data. Functions and pipes (%&gt;%) dominate most workflows.\nIn Python, everything is an object, even basic things like lists, strings, and dataframes. A lot of ‚Äòfunctions‚Äô are instead written as object-associated methods. Some of these methods modify the objects in-place by altering their attributes. Understanding how this works is key to using Python effectively!\n\n\nYou‚Äôve already seen this object-oriented style in Sessions 2 and 3 ‚Äî you create objects like lists or dataframes, then call methods on them like .append() or .sort_values(). In python, instead of piping, we sometimes chain methods together."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#modeling-in-python",
    "href": "session4_newMLDemo/session4v2_slides.html#modeling-in-python",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Modeling in Python",
    "text": "Modeling in Python\n\nPython absolutely uses functions‚Äîjust like R! They‚Äôre helpful for data transformation, wrangling, and automation tasks like looping and parallelization. \nBut when it comes to modeling, libraries are designed around classes: blueprints for creating objects that store data (attributes) and define behaviors (methods). \n\nscikit-learn is great for getting started‚Äîeverything follows a simple, consistent OOP interface. Its API is also consistant with other modeling packages, like xgboost and scvi-tools.\nscikit-survival is built on top of scikit-learn. https://scikit-survival.readthedocs.io/en/stable/user_guide/00-introduction.html is a good tutorial for it.\nPyTorch and TensorFlow are essential if you go deeper into neural networks or custom models‚Äîyou‚Äôll define your own model classes with attributes and methods, but the basic structure is similar to scikit-learn.\n\nstatsmodels is an alternative to scikit-learn for statistical analyses and has R-like syntax and outputs. It‚Äôs a bit more complex than scikit-learn and a bit less consistant with other packages in the python ecosystem. https://wesmckinney.com/book/modeling is a good tutorial for statsmodels.\n\n\nüí° To work effectively in Python, especially for tasks involving modeling or model training, it helps to think in terms of objects and classes, not just functions."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#why-does-oop-matter-in-python-modeling",
    "href": "session4_newMLDemo/session4v2_slides.html#why-does-oop-matter-in-python-modeling",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Why Does OOP Matter in Python Modeling?",
    "text": "Why Does OOP Matter in Python Modeling?\n\nIn Python modeling frameworks:\n\n\n\nModels are instances of classes\n\n\nYou call methods like .fit(), .predict(), .score()\n\n\nInternal model details like coefficients or layers are stored as attributes\n\n\n\nThis makes model behavior consistent between model classes and even libraries. It also simplifies creating/using pre-trained models: both the architecture and learned weights are bundled into a single object with expected built-in methods like .predict() or .fine_tune().\n\n\nInstead of having a separate results object, like in R, you would retrieve your results by accessing an attribute or using a method that is attached to the model object itself.\n\n\n We‚Äôll focus on scikit-learn in this session, but these ideas carry over to other libraries like xgboost, statsmodels, and PyTorch."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#key-oop-principles-recap",
    "href": "session4_newMLDemo/session4v2_slides.html#key-oop-principles-recap",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Key OOP Principles (Recap)",
    "text": "Key OOP Principles (Recap)\n\nIn OOP, code is structured around objects (as opposed to functions). This paradigm builds off the following principles:\n\n\nEncapsulation: Bundling data and methods together in a single unit.\n\nA StandardScaler object stores mean and variance data and has .fit() and .transform() methods\n\n\n\n\n\nInheritance: Creating new classes based on existing ones.\n\nsklearn.LinearRegression inherits attributes and methods from a general regression model class.\n\n\n\n\n\n\nAbstraction: Hiding implementation details and exposing only essential functionality.\n\ne.g., .fit() works the same way from the outside, regardless of model complexity\n\n\n\n\n\n\nPolymorphism: Objects of different types can be treated the same way if they implement the same methods.\n\nPython‚Äôs duck typing:\n\nü¶Ü ‚ÄúIf it walks like a duck and quacks like a duck, then it must be a duck.‚Äù ü¶Ü\n\nex: If different objects all have a .summarize() method, we can loop over them and call .summarize() without needing to check their class. As long as the method exists, Python will know what to do.\nThis lets us easily create pipelines that can work for many types of models.\n\n\n\n\nWe won‚Äôt cover pipelines here, but they are worth looking into!"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#classes-and-objects",
    "href": "session4_newMLDemo/session4v2_slides.html#classes-and-objects",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Classes and Objects",
    "text": "Classes and Objects\n\nClasses are blueprints for creating objects. Each object contains:\n\n\n\nAttributes (data): model coefficients, class labels\n\n\nMethods (behaviors): .fit(), .predict()\n\n\nüëâ To Get the class of an object, use:\n\ntype(object) # Returns the type of the object\n\nüëâ To check if an object is an instance of a particular class, use:\n\nisinstance(object, class)  # Returns True if `object` is an instance of `class`.\n\n\n\nKnowing what class an object belongs to helps us understand what methods and attributes it provides."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#base-classes",
    "href": "session4_newMLDemo/session4v2_slides.html#base-classes",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Base Classes",
    "text": "Base Classes\n\nA base class (or parent class) serves as a template for creating objects. Other classes can inherit from it to reuse its properties and methods.\nClasses are defined using the class keyword, and their structure is specified using an __init__() method for initialization.\n\nFor example, we can define a class called Dog and give it attributes that store data about a given dog and methods that represent behaviors an object of the Dog class can perform. We can also edit the special or ‚Äúdunder‚Äù methods (short for double underscore) that define how objects behave in certain contexts.\n\nclass Dog: ## begin class definition\n    def __init__(self, name, breed): ## define init method\n        self.name = name ## add attributes\n        self.breed = breed\n\n    def speak(self): ## add methods\n        return f\"{self.name} says woof!\"\n\n    def __str__(self): # __str__(self) tells python what to display when an object is printed\n        return f\"Our dog {self.name}\"\n\n    def __repr__(self): # add representation to display when dog is called in console\n        return f\"Dog(name={self.name!r}, breed={self.breed!r})\""
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#creating-a-dog",
    "href": "session4_newMLDemo/session4v2_slides.html#creating-a-dog",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Creating a dog",
    "text": "Creating a dog\n\nCreating an instance of the Dog class lets us model a particular dog:\n\nbuddy = Dog(\"Buddy\", \"Golden Retriever\")\nprint(f\"Buddy is an object of class {type(buddy)}\")\n\n\n\nBuddy is an object of class &lt;class '__main__.Dog'&gt;\n\n\n\n\nWe set the value of the attributes [name and breed], which are then stored as part of the buddy object\n\n\nWe can use any methods defined in the Dog class on buddy\n\n\n\n## if we want to see what kind of dog our dog is\n## we can call buddy's attributes\nprint(f\"Our dog {buddy.name} is a {buddy.breed}.\")\n\n## we can also call any Dog methods\nprint(buddy.speak())  \n\n## including special methods\nbuddy ## displays what was in the __repr__() method\n\n\n\nOur dog Buddy is a Golden Retriever.\nBuddy says woof!\n\n\nDog(name='Buddy', breed='Golden Retriever')\n\n\nNote: For python methods, the self argument is assumed to be passed and therefore we do not put anything in the parentheses when calling .speak(). For attributes, we do not put () at all."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#derived-child-classes",
    "href": "session4_newMLDemo/session4v2_slides.html#derived-child-classes",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Derived (Child) Classes",
    "text": "Derived (Child) Classes\n\nDerived/child classes build on base classes using the principle of inheritence. \nNow that we have a Dog class, we can build on it to create a specialized GuardDog class.\n\nclass GuardDog(Dog):  # GuardDog inherits from Dog\n    def __init__(self, name, breed, training_level): ## in addition to name and breed, we can \n        # define a training level. \n        # Call the parent (Dog) class's __init__ method\n        super().__init__(name, breed)\n        self.training_level = training_level  # New attribute for GuardDog that stores the \n        # training level for the dog\n\n    def guard(self): ## checks if the training level is &gt; 5 and if not says train more\n        if self.training_level &gt; 5:\n            return f\"{self.name} is guarding the house!\"\n        else:\n            return f\"{self.name} needs more training before guarding.\"\n    \n    def train(self): # modifies the training_level attribute to increase the dog's training level\n        self.training_level = self.training_level + 1\n        return f\"Training {self.name}. {self.name}'s training level is now {self.training_level}\"\n\n# Creating an instance of GuardDog\nrex = GuardDog(\"Rex\", \"German Shepherd\", training_level= 5)"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#mixins",
    "href": "session4_newMLDemo/session4v2_slides.html#mixins",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Mixins",
    "text": "Mixins\n\nA mixin is a special kind of class designed to add functionality to another class. Unlike base classes, mixins aren‚Äôt used alone.\n\nFor example, scikit-learn uses mixins like:\n- sklearn.base.ClassifierMixin (adds classifier-specific methods)\n- sklearn.base.RegressorMixin (adds regression-specific methods)\nwhich it adds to the BaseEstimator class to add functionality.  \nTo finish up our dog example, we are going to define a mixin class that adds learning tricks to the base Dog class and use it to create a new class called SmartDog."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#duck-typing",
    "href": "session4_newMLDemo/session4v2_slides.html#duck-typing",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Duck Typing",
    "text": "Duck Typing\n\n\nü¶Ü ‚ÄúIf it quacks like a duck and walks like a duck, it‚Äôs a duck.‚Äù ü¶Ü\n\nPython‚Äôs duck typing makes our lives a lot easier, and is one of the main benefits of methods over functions:\n\n\nRepurposing old code - methods by the same name work the same for different model types\n\n\nNot necessary to check types before using methods - methods are assumed to work on the object they‚Äôre attached to\n\n\n\nWe can demonstrate this by defining two new base classes that are different than Dog but also have a speak() method.\n\n\nclass Human:\n    def __init__(self, name):\n        self.name = name\n\n    def speak(self):\n        return f\"{self.name} says hello!\"\n\nclass Parrot:\n    def __init__(self, name):\n        self.name = name\n\n    def speak(self):\n        return f\"{self.name} says squawk!\""
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#duck-typing-in-action",
    "href": "session4_newMLDemo/session4v2_slides.html#duck-typing-in-action",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Duck Typing in Action",
    "text": "Duck Typing in Action\n\nEven though Dog, Human and Parrot are entirely different classes‚Ä¶\n\n\ndef call_speaker(obj):\n    print(obj.speak())\n\ncall_speaker(Dog(\"Fido\", \"Labrador\"))\ncall_speaker(Human(\"Alice\"))\ncall_speaker(Parrot(\"Polly\"))\n\n\n\nFido says woof!\nAlice says hello!\nPolly says squawk!\n\n\n\nThey all implement .speak(), so Python treats them the same!\nIn the context of our work, this would allow us to make a pipeline using models from different libraries that have the same methods.\n\nWhile our dog example was very simple, this is the same way that model classes work in python!"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#example-oop-in-machine-learning-and-modeling",
    "href": "session4_newMLDemo/session4v2_slides.html#example-oop-in-machine-learning-and-modeling",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Example: OOP in Machine Learning and Modeling",
    "text": "Example: OOP in Machine Learning and Modeling\n\nMachine learning models in Python are implemented as classes.\n\n\n\nWhen you create a model, you‚Äôre instantiating an object of a predefined class (e.g., LogisticRegression()).\n\n\nThat model has attributes (parameters, coefficients) and methods (like .fit() and .predict()).\n\n\nFor example LogisticRegression is a model class that inherits from SparseCoefMixin and BaseEstimator.\nclass LogisticRegression(LinearClassifierMixin, SparseCoefMixin, BaseEstimator):\nTo perform logistic regression, we create an instance of the LogisticRegression class.\n## Example: \nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()  # Creating an instance of the LogisticRegression class\nmodel.fit(X_train, y_train)   # Calling a method to train the model\npredictions = model.predict(X_test)  # Calling a method to make predictions\ncoefs = model.coef_ # Access model coefficients using attribute"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#key-benefits-of-oop-in-machine-learning",
    "href": "session4_newMLDemo/session4v2_slides.html#key-benefits-of-oop-in-machine-learning",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Key Benefits of OOP in Machine Learning",
    "text": "Key Benefits of OOP in Machine Learning\n\n\nEncapsulation ‚Äì Models store parameters and methods inside a single object.\n\nInheritance ‚Äì New models can build on base models, reusing existing functionality.\n\nAbstraction ‚Äì .fit() should work as expected, regardless of complexity of underlying implimentation.\nPolymorphism (Duck Typing) ‚Äì Different models share the same method names (.fit(), .predict()), making them easy to use interchangeably, particularly in analysis pipelines.\n\nUnderstanding base classes and mixins is especially important when working with deep learning frameworks like PyTorch and TensorFlow, which require us to create our own model classes."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#mini-project-classifying-penguins-with-scikit-learn",
    "href": "session4_newMLDemo/session4v2_slides.html#mini-project-classifying-penguins-with-scikit-learn",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "üêß Mini Project: Classifying Penguins with scikit-learn",
    "text": "üêß Mini Project: Classifying Penguins with scikit-learn\n\nNow that you understand classes and data structures in Python, let‚Äôs apply that knowledge to classify penguin species using two features:\n\n\n\nbill_length_mm\n\n\nbill_depth_mm\n\n\nWe‚Äôll explore:\n\n\nUnsupervised learning with K-Means clustering (model doesn‚Äôt ‚Äòknow‚Äô y)\n\n\nSupervised learning with a k-NN classifier (model trained w/ y information)\n\n\nAll scikit-learn models are designed to have\n\n\nCommon Methods:\n\n\n\n.fit() ‚Äî Train the model\n\n\n.predict() ‚Äî Make predictions\n\n\n\nCommon Attributes:\n\n\n.classes_, .n_clusters_, etc.\n\n\n\n\nThis is true of the scikit-survival package too!"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#import-libraries",
    "href": "session4_newMLDemo/session4v2_slides.html#import-libraries",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Import Libraries",
    "text": "Import Libraries\n\nBefore any analysis, we must import the necessary libraries.\nFor large libraries like scikit-learn, PyTorch, or TensorFlow, we usually do not import the entire package. Instead, we selectively import the classes and functions we need.\n\n\nClasses\n- StandardScaler ‚Äî for feature scaling\n- KNeighborsClassifier ‚Äî for supervised k-NN classification\n- KMeans ‚Äî for unsupervised clustering\n\n\nüî§ Naming Tip:\n- CamelCase = Classes\n- snake_case = Functions\n\n\nFunctions\n- train_test_split() ‚Äî to split data into training and test sets\n- accuracy_score() ‚Äî to evaluate classification accuracy\n- classification_report() ‚Äî to print precision, recall, F1 (balance of precision and recall), Support (number of true instances per class) - adjusted_rand_score() ‚Äî to evaluate clustering performance"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#import-libraries-1",
    "href": "session4_newMLDemo/session4v2_slides.html#import-libraries-1",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Import Libraries",
    "text": "Import Libraries\n\n## imports\nimport pandas as pd\nimport numpy as np\n\nfrom plotnine import *\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom great_tables import GT\n\n## sklearn imports\n\n## import classes\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\n\n## import functions\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, adjusted_rand_score"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#data-preparation",
    "href": "session4_newMLDemo/session4v2_slides.html#data-preparation",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Data Preparation",
    "text": "Data Preparation\n\n# Load the Penguins dataset\npenguins = sns.load_dataset(\"penguins\").dropna()\n\n# Make a summary table for the penguins dataset, grouping by species. \nsummary_table = penguins.groupby(\"species\").agg({\n    \"bill_length_mm\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"bill_depth_mm\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"sex\": lambda x: x.value_counts().to_dict()  # Count of males and females\n})\n\n# Round numeric values to 1 decimal place (excluding the 'sex' column)\nfor col in summary_table.columns:\n    if summary_table[col].dtype in [float, int]:\n        summary_table[col] = summary_table[col].round(1)\n\n# Display the result\ndisplay(summary_table)\n\n\n\n\n\n\n\n\n\n\nbill_length_mm\nbill_depth_mm\nsex\n\n\n\nmean\nstd\nmin\nmax\nmean\nstd\nmin\nmax\n&lt;lambda&gt;\n\n\nspecies\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdelie\n38.8\n2.7\n32.1\n46.0\n18.3\n1.2\n15.5\n21.5\n{'Male': 73, 'Female': 73}\n\n\nChinstrap\n48.8\n3.3\n40.9\n58.0\n18.4\n1.1\n16.4\n20.8\n{'Female': 34, 'Male': 34}\n\n\nGentoo\n47.6\n3.1\n40.9\n59.6\n15.0\n1.0\n13.1\n17.3\n{'Male': 61, 'Female': 58}"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#data-visualization",
    "href": "session4_newMLDemo/session4v2_slides.html#data-visualization",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nTo do visualization, we can use either seaborn or plotnine. plotnine mirrors ggplot2 syntax from R and is great for layered grammar-of-graphics plots, while seaborn seaborn is more convienient if you want to put multiple plots on the same figure. \nPlotting with Plotnine vs Seaborn\n\n\nPlotnine (like ggplot2 in R) The biggest differences between plotnine and ggplot2 syntax are:\n\n\nWith plotnine the whole call is wrapped in () parentheses\n\n\nVariables are called with strings (\"\" are needed!)\n\n\nIf you don‚Äôt use from plotnine import *, you will need to import each individual function you plan to use!\n\n\n\nSeaborn (base matplotlib + enhancements)\n\n\nDesigned for quick, polished plots\n\n\nWorks well with pandas DataFrames or NumPy arrays\n\n\nIntegrates with matplotlib for customization\n\n\nGood for things like decision boundaries or heatmaps\n\n\nHarder to customize than plotnine plots"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#scatterplot-with-plotnine",
    "href": "session4_newMLDemo/session4v2_slides.html#scatterplot-with-plotnine",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Scatterplot with plotnine",
    "text": "Scatterplot with plotnine\n\nTo take a look at the distribution of our species by bill length and bill depth before clustering‚Ä¶\n\n\nplot1 = (ggplot(penguins, aes(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"species\"))\n + geom_point()\n + ggtitle(\"Penguin Species\")\n + theme_bw())\n\ndisplay(plot1)"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#scatterplot-with-seaborn",
    "href": "session4_newMLDemo/session4v2_slides.html#scatterplot-with-seaborn",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Scatterplot with seaborn",
    "text": "Scatterplot with seaborn\nWe can make a similar plot in seaborn. This time, let‚Äôs include sex by setting the point style\n\n# Create the figure and axes obects\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Create a plot \nsns.scatterplot(\n    data=penguins, x=\"bill_length_mm\", y=\"bill_depth_mm\",\n    hue=\"species\", ## hue = fill\n    style=\"sex\",  ## style = style of dots\n    palette=\"Set2\", ## sets color pallet\n    edgecolor=\"black\", s=300, ## line color and point size \n    ax=ax              ## Draw plot on ax      \n)\n\n# Use methods on ax to set title, labels\nax.set_title(\"Penguin Bill Length vs Depth by Species\")\nax.set_xlabel(\"Bill Length (mm)\")\nax.set_ylabel(\"Bill Depth (mm)\")\nax.legend(title=\"Species\")\n\n# Plot the figure\nfig.tight_layout()"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#scatterplot-with-seaborn-output",
    "href": "session4_newMLDemo/session4v2_slides.html#scatterplot-with-seaborn-output",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Scatterplot with seaborn",
    "text": "Scatterplot with seaborn"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#scaling-the-data---understanding-the-standard-scaler-class",
    "href": "session4_newMLDemo/session4v2_slides.html#scaling-the-data---understanding-the-standard-scaler-class",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Scaling the data - Understanding the Standard Scaler class",
    "text": "Scaling the data - Understanding the Standard Scaler class\n\nFor our clustering to work well, the predictors should be on the same scale. To achieve this, we use an instance of the StandardScaler class.\nclass sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)\n\n\nParameters are supplied by user\n- copy, with_mean, with_std \nAttributes contain the data of the object\n- scale_: scaling factor\n- mean_: mean value for each feature\n- var_: variance for each feature\n- n_features_in_: number of features seen during fit\n- n_samples_seen: number of samples processed for each feature \nMethods describe the behaviors of the object and/or modify its attributes\n- fit(X): computes mean and std used for scaling and ‚Äòfits‚Äô scaler to data X\n- transform(X): performs standardization by centering and scaling X with fitted scaler\n- fit_transform(X): does both"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#scaling-data",
    "href": "session4_newMLDemo/session4v2_slides.html#scaling-data",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Scaling Data",
    "text": "Scaling Data\n\n\n# Selecting features for clustering -&gt; let's just use bill length and bill depth.\nX = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]]\ny = penguins[\"species\"]\n\n# Standardizing the features for better clustering performance\nscaler = StandardScaler() ## create instance of StandardScaler\nX_scaled = scaler.fit_transform(X) \n\n\n\n\n\n\n\n\n\n\nOriginal vs Scaled Features\n\n\nFeature\nOriginal\nScaled\n\n\nOriginal Mean\nOriginal Std\nScaled Mean\nScaled Std\n\n\n\n\nmean\n44\n17\n0\n0\n\n\nstd\n5\n2\n1\n1\n\n\n\n\n\n\n        \n\n\n\n\nShow table code\n## Make X_scaled a pandas df\nX_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)\n\n# Compute summary statistics and round to 2 sig figs\noriginal_stats = X.agg([\"mean\", \"std\"])\nscaled_stats = X_scaled_df.agg([\"mean\", \"std\"])\n\n# Combine into a single table with renamed columns\nsummary_table = pd.concat([original_stats, scaled_stats], axis=1)\nsummary_table.columns = [\"Original Mean\", \"Original Std\", \"Scaled Mean\", \"Scaled Std\"]\nsummary_table.index.name = \"Feature\"\n\n# Display nicely with great_tables\n(\n    GT(summary_table.reset_index()).tab_header(\"Original vs Scaled Features\")\n    .fmt_number(n_sigfig = 2)\n    .tab_spanner(label=\"Original\", columns=[\"Original Mean\", \"Original Std\"])\n    .tab_spanner(label=\"Scaled\", columns=[\"Scaled Mean\", \"Scaled Std\"])\n    .tab_options(table_font_size = 20)\n)"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#understanding-the-kmeans-model-class",
    "href": "session4_newMLDemo/session4v2_slides.html#understanding-the-kmeans-model-class",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Understanding the KMeans model class",
    "text": "Understanding the KMeans model class\n\nclass sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, \ntol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')\nParameters: Set by user at time of instantiation\n- n_clusters, max_iter, algorithm \nAttributes: Store object data\n- cluster_centers_: stores coordinates of cluster centers\n- labels_: stores labels of each point - n_iter_: number of iterations run (will be changed during method run)\n- n_features_in and feature_names_in_: store info about features seen during fit \nMethods: Define object behaviors\n- fit(X): fits model to data X - predict(X): predicts closest cluster each sample in X belongs to\n- transform(X): transforms X to cluster-distance space"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#use-function-to-calculate-ari",
    "href": "session4_newMLDemo/session4v2_slides.html#use-function-to-calculate-ari",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Use function to calculate ARI",
    "text": "Use function to calculate ARI\n\nTo check how good our model is, we can use one of the functions included in the sklearn library.\nThe adjusted_rand_score() function evaluates how well the cluster groupings agree with the species groupings while adjusting for chance.\n\n# Calculate clustering performance using Adjusted Rand Index (ARI)\nkmeans_ari = adjusted_rand_score(penguins['species'], penguins[\"kmeans_cluster\"])\nprint(f\"k-Means Adjusted Rand Index: {kmeans_ari:.2f}\")\n\n\n\nk-Means Adjusted Rand Index: 0.82"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#creating-plots",
    "href": "session4_newMLDemo/session4v2_slides.html#creating-plots",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Creating Plots",
    "text": "Creating Plots\n\n# Prepare the figure with 2 subplots; the axes object will contain both plots\nfig2, axes = plt.subplots(1, 2, figsize=(16, 7)) ## 1 row 2 columns\n\n# Plot heatmap on the first axis\nsns.heatmap(data = heatmap_data, cmap=\"Blues\", linewidths=0.5, linecolor='white', annot=True, \nfmt='d', ax=axes[0])\naxes[0].set_title(\"Heatmap of KMeans Clustering by Species\")\naxes[0].set_xlabel(\"Species\")\naxes[0].set_ylabel(\"KMeans Cluster\")\n\n# Scatterplot with jitter\nsns.scatterplot(data=scatter_data, x=\"x_jittered\", y=\"kmeans_cluster\",\n    hue=\"species\", style=\"sex\", size=\"count\", sizes=(100, 500),\n    alpha=0.8, ax=axes[1], legend=\"brief\")\naxes[1].set_xticks(range(len(species_order)))\naxes[1].set_xticklabels(species_order)\naxes[1].set_title(\"Cluster Assignment by Species and Sex (Jittered)\")\naxes[1].set_ylabel(\"KMeans Cluster\")\naxes[1].set_xlabel(\"Species\")\naxes[1].set_yticks([0, 1, 2])\naxes[1].legend(bbox_to_anchor=(1.05, 0.5), loc='center left', borderaxespad=0.0, title=\"Legend\")\n\nfig2.tight_layout()"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#creating-plots-output",
    "href": "session4_newMLDemo/session4v2_slides.html#creating-plots-output",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Creating Plots",
    "text": "Creating Plots"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#project-2-knn-classification",
    "href": "session4_newMLDemo/session4v2_slides.html#project-2-knn-classification",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Project 2: KNN classification",
    "text": "Project 2: KNN classification\n\nFor our KNN classification, the model is supervised (meaning it is dependent on the outcome ‚Äòy‚Äô data). This time, we need to split our data into a training and test set. \n\n\nThe function train_test_split() from scikit-learn is helpful here!\n\n# Splitting dataset into training and testing sets (still using scaled X!)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n\n\n\n\nUnlike R functions, which return a single object (often a list when multiple outputs are needed), Python functions can return multiple values as a tuple‚Äîletting you unpack them directly into separate variables."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#understanding-kneighborsclassifier-class",
    "href": "session4_newMLDemo/session4v2_slides.html#understanding-kneighborsclassifier-class",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Understanding KNeighborsClassifier class",
    "text": "Understanding KNeighborsClassifier class\nclass sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', \nalgorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n\nParameters: Set by user at time of instantiation\n- n_neigbors, weights, algorithm, etc. \nAttributes: Store object data\n- classes_: class labels known to the classifier\n- effective_metric_: distance metric used\n- effective_metric_params_: parameters for the metric function\n- n_features_in and feature_names_in_: store info about features seen during fit\n- n_samples_fit_: number of samples in fitted data \nMethods: Define object behaviors\n- .fit(X, y): fit knn classifier from training dataset (X and y)\n- .predict(X): predict class labels for provided data X\n- .predict_proba(X): return probability estimates for test data X\n- .score(X, y): return mean accuracy on given test data X and labels y"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#making-an-instance-of-kneighborsclassifier-and-fitting-to-training-data",
    "href": "session4_newMLDemo/session4v2_slides.html#making-an-instance-of-kneighborsclassifier-and-fitting-to-training-data",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Making an instance of KNeighborsClassifier and fitting to training data",
    "text": "Making an instance of KNeighborsClassifier and fitting to training data\n\nFor a supervised model, y_train is included in .fit()!\n\n\n## perform knn classification\n# Applying k-NN classification with 5 neighbors\nknn = KNeighborsClassifier(n_neighbors=5) ## make an instance of the KNeighborsClassifier class\n# and set the n_neighbors parameter to be 5. \n\n# Use the fit method to fit the model to the training data\nknn.fit(X_train, y_train)\nknn"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#once-the-model-is-fit",
    "href": "session4_newMLDemo/session4v2_slides.html#once-the-model-is-fit",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Once the model is fit‚Ä¶",
    "text": "Once the model is fit‚Ä¶\n-We can look at its attributes (ex: .classes_) which gives the class labels as known to the classifier\n\nprint(knn.classes_)\n\n\n\n['Adelie' 'Chinstrap' 'Gentoo']\n\n\n\n-And use fitted model to predict species for test data\n\n# Use the predict method on the test data to get the predictions for the test data\ny_pred = knn.predict(X_test)\n\n# Also can take a look at the prediction probabilities, \n# and use the .classes_ attribute to put the column labels in the right order\nprobs = pd.DataFrame(\n    knn.predict_proba(X_test),\n    columns = knn.classes_)\nprobs['y_pred'] = y_pred\n\nprint(\"Predicted probabilities: \\n\", probs.head())\n\n\n\nPredicted probabilities: \n    Adelie  Chinstrap  Gentoo     y_pred\n0     1.0        0.0     0.0     Adelie\n1     0.0        0.0     1.0     Gentoo\n2     1.0        0.0     0.0     Adelie\n3     0.0        0.6     0.4  Chinstrap\n4     1.0        0.0     0.0     Adelie"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#scatterplot-for-k-nn-classification-of-test-data",
    "href": "session4_newMLDemo/session4v2_slides.html#scatterplot-for-k-nn-classification-of-test-data",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Scatterplot for k-NN classification of test data",
    "text": "Scatterplot for k-NN classification of test data\n\n\nCreate dataframe of unscaled X_test, bill_length_mm, and bill_depth_mm.\nAdd to it the actual and predicted species labels\n\n\n## First unscale the test data\nX_test_unscaled = scaler.inverse_transform(X_test)\n\n## create dataframe \npenguins_test = pd.DataFrame(\n    X_test_unscaled,\n    columns=['bill_length_mm', 'bill_depth_mm']\n)\n\n## add actual and predicted species \npenguins_test['y_actual'] = y_test.values\npenguins_test['y_pred'] = y_pred\npenguins_test['correct'] = penguins_test['y_actual'] == penguins_test['y_pred']\n\nprint(\"Results: \\n\", penguins_test.head())\n\n\n\nResults: \n    bill_length_mm  bill_depth_mm   y_actual     y_pred  correct\n0            39.5           16.7     Adelie     Adelie     True\n1            46.9           14.6     Gentoo     Gentoo     True\n2            42.1           19.1     Adelie     Adelie     True\n3            49.8           17.3  Chinstrap  Chinstrap     True\n4            41.1           18.2     Adelie     Adelie     True"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#plotnine-scatterplot-for-k-nn-classification-of-test-data",
    "href": "session4_newMLDemo/session4v2_slides.html#plotnine-scatterplot-for-k-nn-classification-of-test-data",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Plotnine scatterplot for k-NN classification of test data",
    "text": "Plotnine scatterplot for k-NN classification of test data\nTo see how well our model did at classifying the remaining penguins‚Ä¶\n\n## Build the plot\nplot3 = (ggplot(penguins_test, aes(x=\"bill_length_mm\", y=\"bill_depth_mm\", \ncolor=\"y_actual\", fill = 'y_pred', shape = 'correct'))\n + geom_point(size=4, stroke=1.1)  # Stroke controls outline thickness\n + scale_shape_manual(values={True: 'o', False: '^'})  # Circle and triangle\n + ggtitle(\"k-NN Classification Results\")\n + theme_bw())\n\ndisplay(plot3)"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#plotnine-scatterplot-for-k-nn-classification-of-test-data-output",
    "href": "session4_newMLDemo/session4v2_slides.html#plotnine-scatterplot-for-k-nn-classification-of-test-data-output",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Plotnine scatterplot for k-NN classification of test data",
    "text": "Plotnine scatterplot for k-NN classification of test data"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#visualizing-decision-boundary-with-seaborn-and-matplotlib",
    "href": "session4_newMLDemo/session4v2_slides.html#visualizing-decision-boundary-with-seaborn-and-matplotlib",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Visualizing Decision Boundary with seaborn and matplotlib",
    "text": "Visualizing Decision Boundary with seaborn and matplotlib\n\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create and fit label encoder for y (just makes y numeric because it makes the scatter plot happy)\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Create the plot objects\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Create display object\ndisp = DecisionBoundaryDisplay.from_estimator(\n    knn,\n    X_test,\n    response_method = 'predict',\n    plot_method = 'pcolormesh',\n    xlabel = \"bill_length_scaled\",\n    ylabel = \"bill_depth_scaled\",\n    shading = 'auto',\n    alpha = 0.5,\n    ax = ax\n)\n\n# Use method from display object to create scatter plot\nscatter = disp.ax_.scatter(X_scaled[:,0], X_scaled[:,1], c=y_encoded, edgecolors = 'k')\ndisp.ax_.legend(scatter.legend_elements()[0], knn.classes_, loc = 'lower left', title = 'Species')\n_ = disp.ax_.set_title(\"Penguin Classification\")\n\nplt.show()"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#visualizing-decision-boundary-with-seaborn-and-matplotlib-output",
    "href": "session4_newMLDemo/session4v2_slides.html#visualizing-decision-boundary-with-seaborn-and-matplotlib-output",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Visualizing Decision Boundary with seaborn and matplotlib",
    "text": "Visualizing Decision Boundary with seaborn and matplotlib"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#evaluate-knn-performance",
    "href": "session4_newMLDemo/session4v2_slides.html#evaluate-knn-performance",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Evaluate KNN performance",
    "text": "Evaluate KNN performance\n\nTo check the performance of our KNN classifier, we can check the accuracy score and print a classification report.\n- accuracy_score and classification_report are both functions!\n- They are not unique to scikit-learn classes so it makes sense for them to be functions not methods\n\n\n## eval knn performance\nknn_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"k-NN Accuracy: {knn_accuracy:.2f}\")\nprint(\"Classification Report: \\n\", classification_report(y_test, y_pred))\n\n\n\nk-NN Accuracy: 0.94\nClassification Report: \n               precision    recall  f1-score   support\n\n      Adelie       0.98      0.98      0.98        48\n   Chinstrap       0.80      0.89      0.84        18\n      Gentoo       0.97      0.91      0.94        34\n\n    accuracy                           0.94       100\n   macro avg       0.92      0.93      0.92       100\nweighted avg       0.94      0.94      0.94       100"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#make-a-summary-table-of-metrics-for-both-models",
    "href": "session4_newMLDemo/session4v2_slides.html#make-a-summary-table-of-metrics-for-both-models",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Make a Summary Table of Metrics for Both Models",
    "text": "Make a Summary Table of Metrics for Both Models\n\nsummary_table = pd.DataFrame({\n    \"Metric\": [\"k-Means Adjusted Rand Index\", \"k-NN Accuracy\"],\n    \"Value\": [kmeans_ari, knn_accuracy]\n})\n(\n    GT(summary_table)\n    .tab_header(title = \"Model Results Summary\")\n    .fmt_number(columns = \"Value\", n_sigfig = 2)\n    .tab_options(table_font_size = 20)\n)\n\n\n\n\n\n\n\n\n\nModel Results Summary\n\n\nMetric\nValue\n\n\n\n\nk-Means Adjusted Rand Index\n0.82\n\n\nk-NN Accuracy\n0.94"
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#key-takeaways-from-this-session",
    "href": "session4_newMLDemo/session4v2_slides.html#key-takeaways-from-this-session",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Key Takeaways from This Session",
    "text": "Key Takeaways from This Session\n\n\n\n\n\nPython workflows rely on object-oriented structures in addition to functions: Understanding the OOP paradigm makes Python a lot easier!\n\n\nEverything is an object!\n\n\nDuck Typing: If an object has a method, that method can be called regardless of the object type. Caveat being, make sure the arguments (if any) in the method are specified correctly for all objects!\n\n\nPython packages use common methods that make it easy to change between model types without changing a lot of code."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#additional-insights",
    "href": "session4_newMLDemo/session4v2_slides.html#additional-insights",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Additional Insights",
    "text": "Additional Insights\n\n\n\nPredictable APIs enable seamless model switching: Swapping models like LogisticRegression ‚Üí RandomForestClassifier usually requires minimal code changes.\n\n\nscikit-learn prioritizes interoperability: Its consistent class design integrates with tools like Pipeline, GridSearchCV, and cross_val_score.\n\n\nClass attributes improve model transparency: Access attributes like .coef_, .classes_, and .feature_importances_ for model interpretation and debugging.\n\n\nCustom classes are central to deep learning: Frameworks like PyTorch and TensorFlow require you to define your own model classes by subclassing base models.\n\n\nMixins support modular design: Mixins (e.g., ClassifierMixin) let you add specific functionality without duplicating code."
  },
  {
    "objectID": "session4_newMLDemo/session4v2_slides.html#pre-reading-for-this-session",
    "href": "session4_newMLDemo/session4v2_slides.html#pre-reading-for-this-session",
    "title": "Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries",
    "section": "Pre-Reading for This Session",
    "text": "Pre-Reading for This Session\n\n\nScikit-learn Documentation\n\nIntroduction to OOP in Python (Real Python)\n\nPlotnine Reference\nSeaborn Reference"
  }
]