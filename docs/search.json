[
  {
    "objectID": "session2a/DataStructuresDemo.html#links",
    "href": "session2a/DataStructuresDemo.html#links",
    "title": "Session 2: Python Data Structures",
    "section": "Links",
    "text": "Links\n \n\nGuide to Python Data Structures\n\n  \n\nCancer Dataset"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#topic-1-lists",
    "href": "session2a/DataStructuresDemo.html#topic-1-lists",
    "title": "Session 2: Python Data Structures",
    "section": "Topic 1: Lists",
    "text": "Topic 1: Lists\nThe most important thing about lists\nIn Python, lists are mutable, meaning their elements can be changed after the list is created, allowing for modification such as adding, removing, or updating items. This flexibility makes lists powerful for handling dynamic collections of data.\n\nWe will learn how to do things such as:\n\nCreate lists\nModify lists\nSort lists\nLoop over elements of a list with a for-loop or using list comprehension\nSlice a list\nAppend to a list"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#lists",
    "href": "session2a/DataStructuresDemo.html#lists",
    "title": "Session 2: Python Data Structures",
    "section": "Lists",
    "text": "Lists\n\nTo create a list:\n\n\nmy_list = [1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\nprint(my_list)\n\n\n\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\n\n\n\nA list is simply a collection of objects. We can find the length of a list using the len() function:\n\n\nmy_list=[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\nlen(my_list)\n\n\n\n7"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#lists-continued",
    "href": "session2a/DataStructuresDemo.html#lists-continued",
    "title": "Session 2: Python Data Structures",
    "section": "Lists (continued)",
    "text": "Lists (continued)\n\nIn Python, lists are objects like all other data types, and the class for lists is named ‘list’ with a lowercase ‘L’.\nTo transform another Python object into a list, you can use the list() function, which is essentially the constructor of the list class. This function accepts a single argument: an iterable. So, you can use it to turn any iterable, such as a range, set, or tuple, into a list of concrete values.\nPython indices begin at 0. In addition, certain built-in python functions such as range will terminate at n-1 in the second argument.\n\n\nfirst_range=range(5)\nfirst_range_list=list(first_range)\nprint(first_range_list)\n\n\n\n[0, 1, 2, 3, 4]\n\n\n\nsecond_range=range(5,10)\nsecond_range_list=list(second_range)\nprint(second_range_list)\n\n\n\n[5, 6, 7, 8, 9]"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#accessing-python-list-elements",
    "href": "session2a/DataStructuresDemo.html#accessing-python-list-elements",
    "title": "Session 2: Python Data Structures",
    "section": "Accessing Python list elements",
    "text": "Accessing Python list elements\nTo access an individual list element, you need to know its position. Since python starts counting at 0, the first element is in position 0, and the second element is in position 1. You can also access nested elements within a list, or access the list in reverse.\n\nExamples using my_list from above\n\nprint(my_list)\n\n\n\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\n\n\n\nexample_1=my_list[0]\n\nexample_2=my_list[6][1]\n\nexample_3=my_list[-1]\n\nprint(example_1, example_2, example_3)\n\n\n\n1 z [4, 'z']"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#mutability-of-lists",
    "href": "session2a/DataStructuresDemo.html#mutability-of-lists",
    "title": "Session 2: Python Data Structures",
    "section": "Mutability of lists",
    "text": "Mutability of lists\nSince lists are mutable objects, we can directly change their elements.\n\nsome_list=[1,2,3]\nprint(some_list)\n\nsome_list[0]=\"hello\"\nprint(some_list)\n\n\n\n[1, 2, 3]\n['hello', 2, 3]"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#appending-an-element-to-a-list",
    "href": "session2a/DataStructuresDemo.html#appending-an-element-to-a-list",
    "title": "Session 2: Python Data Structures",
    "section": "Appending an element to a list",
    "text": "Appending an element to a list\nWhen calling append on a list, we append an object to the end of the list:\n\nprint(my_list)\n\nmy_list.append(5)\n\nprint(my_list)\n\n\n\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z']]\n[1, 2, 3.2, 'a', 'b', 'c', [4, 'z'], 5]"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#combining-lists",
    "href": "session2a/DataStructuresDemo.html#combining-lists",
    "title": "Session 2: Python Data Structures",
    "section": "Combining lists",
    "text": "Combining lists\nWe can combine lists with the “+” operator. This keeps the original lists intact\n\nlist_1=[1,2,3]\n\nlist_2=['a','b','c']\n\ncombined_lists=list_1+list_2\n\nprint(combined_lists)\n\n\n\n[1, 2, 3, 'a', 'b', 'c']\n\n\n\nAnother method is to extend one list onto another.\n\nlist_1=[1,2,3]\n\nlist_2=['a','b','c']\n\nlist_1.extend(list_2)\n\nprint(list_1)\n\n\n\n[1, 2, 3, 'a', 'b', 'c']"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#pop-method",
    "href": "session2a/DataStructuresDemo.html#pop-method",
    "title": "Session 2: Python Data Structures",
    "section": ".pop() method",
    "text": ".pop() method\nThe .pop() method removes and returns the last item by default unless you give it an index argument. If you’re familiar with stacks, this method as well as .append() can be used to create one!\n\nlist_1=[1,2,3]\n\nelement_1=list_1.pop()\nelement_2=list_1.pop(1)\n\nprint(element_1, element_2)\n\n\n\n3 2"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#deleting-items-by-index",
    "href": "session2a/DataStructuresDemo.html#deleting-items-by-index",
    "title": "Session 2: Python Data Structures",
    "section": "Deleting items by index",
    "text": "Deleting items by index\ndel removes an item without returning anything. In fact, you can delete any object, including the entire list, using del:\n\nlist_1=[1,2,3]\n\ndel list_1[0]\n\nprint(list_1)\n\n\n\n[2, 3]"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#deleting-items-by-value",
    "href": "session2a/DataStructuresDemo.html#deleting-items-by-value",
    "title": "Session 2: Python Data Structures",
    "section": "Deleting items by value",
    "text": "Deleting items by value\nThe .remove() method deletes a specific value from the list. This method will remove the first occurrence of the given object in a list.\n\nlist_1=[1,2,3]\n\nlist_1.remove(1)\n\nprint(list_1)\n\n\n\n[2, 3]"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#lists-vs.-sets-and-deleting-duplicates-from-a-list",
    "href": "session2a/DataStructuresDemo.html#lists-vs.-sets-and-deleting-duplicates-from-a-list",
    "title": "Session 2: Python Data Structures",
    "section": "Lists vs. sets, and deleting duplicates from a list",
    "text": "Lists vs. sets, and deleting duplicates from a list\nThe difference between a list and a set:\n\nA set is an unordered collection of distinct elements.\nA list is ordered and can contain repeats of an element.\nSets are denoted by curly brackets {}. We can use this knowledge to easily delete duplicates from a list, since there is no built-in method to do so.\n\n\nlist_1=[1,2,3,1,2]\nprint(list_1)\n\n\n\n[1, 2, 3, 1, 2]\n\n\n\nset_1=set(list_1)\nprint(set_1)\n\n\n\n{1, 2, 3}\n\n\n\nlist_2=list(set_1)\nprint (list_2)\n\n\n\n[1, 2, 3]"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#sorting-a-list",
    "href": "session2a/DataStructuresDemo.html#sorting-a-list",
    "title": "Session 2: Python Data Structures",
    "section": "Sorting a list",
    "text": "Sorting a list\nThere are two ways to sort a list in Python\n\n.sort() modifies the original list itself. Nothing is returned.\n.sorted() returns a new list, which is a sorted version of the original list.\n.reverse=True: Use this parameter to sort the list in reverse order.\n\n\nnumber_list_1=[3,5,2,1,6,19]\nnumber_list_1.sort()\nprint(number_list_1)\n\n\n\n[1, 2, 3, 5, 6, 19]\n\n\n\nnumber_list_2=sorted(number_list_1, reverse=True)\nprint(number_list_2)\n\n\n\n[19, 6, 5, 3, 2, 1]\n\n\n\nalphabet_list_1=['a','z','e','b']\nalphabet_list_1.sort()\nprint(alphabet_list_1)\n\n\n\n['a', 'b', 'e', 'z']\n\n\n\nalphabet_list_2=sorted(alphabet_list_1, reverse=True)\nprint(alphabet_list_2)\n\n\n\n['z', 'e', 'b', 'a']\n\n\n\nmixed_list_1=[1,5,3,'a','c','b']\ntry:\n    mixed_list_1.sort()\n    print(mixed_list_1)\nexcept TypeError:\n    print(\"Can't sort a list of mixed elements\")\n\n\n\nCan't sort a list of mixed elements"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#list-comprehension",
    "href": "session2a/DataStructuresDemo.html#list-comprehension",
    "title": "Session 2: Python Data Structures",
    "section": "List comprehension",
    "text": "List comprehension\nList comprehension offers a shorter syntax when you want to create a new list based on the values of an existing list (or other object)\n\n#Longer syntax with for loop\n\n\n#Example 1:\n\nsome_list=[1,2,3,'a', 'b', 'c']\nnew_list=[]\nfor item in some_list:\n    if type(item)==str:\n        new_list.append(item)\nprint(new_list)\n\n\n\n['a', 'b', 'c']\n\n\n\n#Example 2:\n\nlowercase_list=['joe', 'sarah', 'emily']\ncapital_list=[]\nfor item in lowercase_list:\n    capital_item=item.upper()\n    capital_list.append(capital_item)\nprint(capital_list)\n\n\n\n['JOE', 'SARAH', 'EMILY']\n\n\n\n#Example 3:\n\nsome_string=\"patrick\"\npatrick_list=[]\nfor letter in some_string:\n    if letter=='t' or letter=='a':\n        patrick_list.append(letter)\nprint(patrick_list)\n\n\n\n['a', 't']"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#shorter-syntax-with-list-comprehension",
    "href": "session2a/DataStructuresDemo.html#shorter-syntax-with-list-comprehension",
    "title": "Session 2: Python Data Structures",
    "section": "Shorter syntax with list comprehension",
    "text": "Shorter syntax with list comprehension\n\n#Example 1:\n\nsome_list=[1,2,3,'a', 'b', 'c']\nnew_list=[x for x in some_list if type(x)==str]\nprint(new_list)\n\n\n\n['a', 'b', 'c']\n\n\n\n#Example 2:\n\nlowercase_list=['joe', 'sarah', 'emily']\ncapital_list=[name.upper() for name in lowercase_list]\nprint(capital_list)\n\n\n\n['JOE', 'SARAH', 'EMILY']\n\n\n\n#Example 3:\n\nsome_string=\"patrick\"\npatrick_list=[x for x in some_string if x=='t' or x=='a']\nprint(patrick_list)\n\n\n\n['a', 't']"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#topic-2-tuples",
    "href": "session2a/DataStructuresDemo.html#topic-2-tuples",
    "title": "Session 2: Python Data Structures",
    "section": "Topic 2: Tuples",
    "text": "Topic 2: Tuples\n\nA tuple is similar to a list, but with one key difference. Tuples are immutable. This means that once you create a tuple, you cannot modify its elements.\nTuples are useful for storing data that should not be changed after creation, such as coordinates, days of the week, or fixed pairs.\nJust like lists, tuples are objects, and the class for tuples is tuple.\nTo transform another Python object into a tuple, you can use the tuple() constructor. It accepts a single iterable, such as a list, range, or string.\n\n\nTo create a tuple, you use parentheses () rather than square brackets [].\n\n# Creating a tuple\nmy_tuple = (10, 20, 30)\n\n# Accessing elements by index\nprint(\"First element:\", my_tuple[0])\nprint(\"Last element:\", my_tuple[-1])\n\n\n\nFirst element: 10\nLast element: 30"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#mutability-of-tuples",
    "href": "session2a/DataStructuresDemo.html#mutability-of-tuples",
    "title": "Session 2: Python Data Structures",
    "section": "Mutability of Tuples",
    "text": "Mutability of Tuples\nTuples are immutable, so you can’t modify their elements. Attempting to change a tuple will result in an error.\n\n# Trying to modify a tuple element (this will raise an error)\ntry:\n    my_tuple[1] = 99\nexcept TypeError:\n    print(\"Tuples are immutable and cannot be changed!\")\n\n\n\nTuples are immutable and cannot be changed!"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#functions-and-tuples",
    "href": "session2a/DataStructuresDemo.html#functions-and-tuples",
    "title": "Session 2: Python Data Structures",
    "section": "Functions and Tuples",
    "text": "Functions and Tuples\nFunctions can return multiple values as a tuple. This is useful for returning multiple results in a single function call.\n\n# Function that returns multiple values as a tuple\ndef min_max(nums):\n    return min(nums), max(nums)  # Returns a tuple of (min, max)\n\n# Calling the function and unpacking the tuple\n\nnumbers = [3, 7, 1, 5]\n\nour_tuple = min_max(numbers)\n\nmin_val, max_val = min_max(numbers) #Unpacking in the function call\n\nprint(our_tuple)\nprint(\"Min:\", min_val)\nprint(\"Max:\", max_val)\n\n\n\n(1, 7)\nMin: 1\nMax: 7"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#topic-3-strings",
    "href": "session2a/DataStructuresDemo.html#topic-3-strings",
    "title": "Session 2: Python Data Structures",
    "section": "Topic 3: Strings",
    "text": "Topic 3: Strings\nYou can use single or double quotes to define a string (but keep it consistent!)\n\nmy_string = \"Hello, World!\"\nprint(my_string)\n\n\n\nHello, World!\n\n\n\nYou can also create a multiline string using triple quotes:\n\nmulti_line_string = \"\"\"This is\na multiline\nstring.\"\"\"\nprint(multi_line_string)\n\n\n\nThis is\na multiline\nstring."
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#string-operations",
    "href": "session2a/DataStructuresDemo.html#string-operations",
    "title": "Session 2: Python Data Structures",
    "section": "String Operations",
    "text": "String Operations\nYou can find the length of a string using the len() funtion, just like with lists.\n\nmy_string = \"Hello, World!\"\nprint(len(my_string))\n\n\n\n13"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#accessing-characters-in-a-string",
    "href": "session2a/DataStructuresDemo.html#accessing-characters-in-a-string",
    "title": "Session 2: Python Data Structures",
    "section": "Accessing characters in a string",
    "text": "Accessing characters in a string\nStrings are indexed like lists, with the first character having index 0. You can access individual characters using their index.\n\nmy_string = \"Hello, World!\"\n\n# First character\nfirst_char = my_string[0]\n\n# Last character (using negative indexing)\nlast_char = my_string[-1]\n\n# Accessing a range of characters (slicing)\nsubstring = my_string[0:5]\n\nprint(first_char, last_char, substring)\n\n\n\nH ! Hello"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#mutability-of-strings",
    "href": "session2a/DataStructuresDemo.html#mutability-of-strings",
    "title": "Session 2: Python Data Structures",
    "section": "Mutability of Strings",
    "text": "Mutability of Strings\nStrings are immutable!\nUnlike lists, strings cannot be changed after creation. If you try to change an individual character, you’ll get an error.\n\nmy_string = \"Hello\"\ntry:\n    my_string[0] = \"h\"  # This will raise an error\nexcept TypeError:\n    print(\"Strings are immutable!\")\n\n\n\nStrings are immutable!"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#concatenating-strings",
    "href": "session2a/DataStructuresDemo.html#concatenating-strings",
    "title": "Session 2: Python Data Structures",
    "section": "Concatenating Strings",
    "text": "Concatenating Strings\nYou can concatenate (combine) strings using the + operator:\n\ngreeting = \"Hello\"\nname = \"Patrick\"\ncombined_string = greeting + \", \" + name + \"!\"\nprint(combined_string)\n\n\n\nHello, Patrick!"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#string-methods",
    "href": "session2a/DataStructuresDemo.html#string-methods",
    "title": "Session 2: Python Data Structures",
    "section": "String methods",
    "text": "String methods\nPython provides many built-in methods for manipulating strings. Some common ones are:\n\nupper() and lower() These methods convert a string to uppercase or lowercase.\n\nmy_string = \"Hello, World!\"\nprint(my_string.upper())\nprint(my_string.lower())\n\n\n\nHELLO, WORLD!\nhello, world!\n\n\n\n\nstrip() This method removes any leading or trailing whitespace from the string.\n\nmy_string = \"   Hello, World!   \"\nprint(my_string.strip())\n\n\n\nHello, World!\n\n\n\n\nreplace() You can replace parts of a string with another string.\n\nmy_string = \"Hello, World!\"\nnew_string = my_string.replace(\"World\", \"Patrick\")\nprint(new_string)\n\n\n\nHello, Patrick!\n\n\n\n\nThe split() method divides a string into a list of substrings based on a delimiter (default is whitespace).\n\nmy_string = \"Hello, World!\"\nwords = my_string.split()\nprint(words)\n\n\n\n['Hello,', 'World!']\n\n\n\nanother_string=\"Hello-World!\"\nmore_words=another_string.split(\"-\")\nprint(more_words)\n\n\n\n['Hello', 'World!']\n\n\n\n\nThe join() method takes an iterable (like a list) and concatenates its elements into a string with a specified separator between them.\n\nmy_list=['Hello,', 'my', 'name', 'is', 'Patrick']\n\nmy_string=' '.join(my_list)\n\nprint(my_string)\n\n\n\nHello, my name is Patrick"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#f-strings-python-3.6",
    "href": "session2a/DataStructuresDemo.html#f-strings-python-3.6",
    "title": "Session 2: Python Data Structures",
    "section": "f-strings (Python 3.6+)",
    "text": "f-strings (Python 3.6+)\nYou can insert variables directly into strings using f-strings.\n\nname = \"Patrick\"\nage = 30\nformatted_string = f\"My name is {name} and I am {age} years old.\"\nprint(formatted_string)\n\n\n\nMy name is Patrick and I am 30 years old.\n\n\n\nmy_string = \"Hello, World!\"\n\n# Extract all vowels from the string\nvowels = str([char for char in my_string if char.lower() in \"aeiou\"])\nprint(vowels)\n\n\n\n['e', 'o', 'o']"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#string-slicing",
    "href": "session2a/DataStructuresDemo.html#string-slicing",
    "title": "Session 2: Python Data Structures",
    "section": "String Slicing",
    "text": "String Slicing\nWe will slice a string using different combinations of start, end, and step to extract different parts of the string.\n\n#To slice a string, follow the string[start:end:step] format\n\n# Original string\nmy_string = \"Python is awesome!\"\nprint(f\"Original string: {my_string}\")\n\n\n\nOriginal string: Python is awesome!\n\n\n\n# Slice from index 0 to 6 (not inclusive), stepping by 1 (default)\n# This will extract \"Python\"\nsubstring_1 = my_string[0:6]\nprint(f\"Substring 1 (0:6): {substring_1}\")\n\n\n\nSubstring 1 (0:6): Python\n\n\n\n# Slice from index 7 to the end of the string, stepping by 1 (default)\n# This will extract \"is awesome!\"\nsubstring_2 = my_string[7:]\nprint(f\"Substring 2 (7:): {substring_2}\")\n\n\n\nSubstring 2 (7:): is awesome!\n\n\n\n# Slice the entire string but take every second character\n# This will extract \"Pto saeoe\"\nsubstring_3 = my_string[::2]\nprint(f\"Substring 3 (every second character): {substring_3}\")\n\n\n\nSubstring 3 (every second character): Pto saeoe\n\n\n\n# Slice from index 0 to 6, stepping by 2\n# This will extract \"Pto\"\nsubstring_4 = my_string[0:6:2]\nprint(f\"Substring 4 (0:6:2): {substring_4}\")\n\n\n\nSubstring 4 (0:6:2): Pto\n\n\n\n# Slice from index 11 to 6, stepping backward by -1\n# This will extract \"wa si\" (reverse slice)\nsubstring_5 = my_string[11:6:-1]\nprint(f\"Substring 5 (11:6:-1): {substring_5}\")\n\n\n\nSubstring 5 (11:6:-1): wa si"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#topic-4-dictionaries",
    "href": "session2a/DataStructuresDemo.html#topic-4-dictionaries",
    "title": "Session 2: Python Data Structures",
    "section": "Topic 4: Dictionaries",
    "text": "Topic 4: Dictionaries\n\nA dictionary is a collection in Python that stores data as key-value pairs.\nIt’s similar to a real-world dictionary where you look up a word (the key) to get its definition (the value).\nIn Python, dictionaries are mutable, meaning you can add, remove, and change items.\n\n\nTo create a dictionary, use curly braces {}, with each key-value pair separated by a colon (:), and pairs separated by commas.\n\n# Creating a dictionary\nmy_dictionary = {\n    'name': 'Alice',\n    'age': 25,\n    'city': 'New York'\n}\nprint(my_dictionary)\n\n\n\n{'name': 'Alice', 'age': 25, 'city': 'New York'}"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#dictionary-operations",
    "href": "session2a/DataStructuresDemo.html#dictionary-operations",
    "title": "Session 2: Python Data Structures",
    "section": "Dictionary Operations",
    "text": "Dictionary Operations\nAccessing dictionary values\n\nTo access a specific value in a dictionary, use the key in square brackets.\nYou can also use the .get() method, which returns None if the key does not exist, instead of raising an error.\n\n\nprint(my_dictionary['name'])      # Using key\nprint(my_dictionary.get('age'))   # Using .get() method\nprint(my_dictionary.get('gender', 'Not specified'))  # Providing a default value\n\n\n\nAlice\n25\nNot specified"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#adding-and-updating-dictionary-items",
    "href": "session2a/DataStructuresDemo.html#adding-and-updating-dictionary-items",
    "title": "Session 2: Python Data Structures",
    "section": "Adding and updating dictionary items",
    "text": "Adding and updating dictionary items\nDictionaries are mutable, so you can add new items or update existing ones using assignment.\n\nmy_dictionary['job'] = 'Engineer'        # Adding a new key-value pair\nmy_dictionary['age'] = 26                # Updating an existing value\nprint(my_dictionary)\n\n\n\n{'name': 'Alice', 'age': 26, 'city': 'New York', 'job': 'Engineer'}"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#dictionary-methods",
    "href": "session2a/DataStructuresDemo.html#dictionary-methods",
    "title": "Session 2: Python Data Structures",
    "section": "Dictionary Methods",
    "text": "Dictionary Methods\nPython dictionaries have several useful methods for managing data:\n\nkeys(): Returns a list of all the keys in the dictionary.\nvalues(): Returns a list of all values in the dictionary.\nitems(): Returns a list of key-value pairs as tuples.\n\n\n# Getting all keys\nprint(my_dictionary.keys())\n\n\n\ndict_keys(['name', 'age', 'city', 'job'])\n\n\n\n# Getting all values\nprint(my_dictionary.values())\n\n\n\ndict_values(['Alice', 26, 'New York', 'Engineer'])\n\n\n\n# Getting all key-value pairs\nprint(my_dictionary.items())\n\n\n\ndict_items([('name', 'Alice'), ('age', 26), ('city', 'New York'), ('job', 'Engineer')])"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#python-dictionaries-for-cancer-research-data",
    "href": "session2a/DataStructuresDemo.html#python-dictionaries-for-cancer-research-data",
    "title": "Session 2: Python Data Structures",
    "section": "Python Dictionaries for Cancer Research Data",
    "text": "Python Dictionaries for Cancer Research Data\nIn cancer research, dictionaries can be used to store patient data, genetic mutations, and statistical results as key-value pairs. This allows for easy lookup, organization, and analysis of data.\n\n#Let’s create a dictionary to store basic patient information, where each patient has a unique ID, and each ID maps to a dictionary containing information about the patient’s age, cancer type, and stage.\n\n# Dictionary of patients with nested dictionaries\npatient_data = {\n    'P001': {'age': 50, 'cancer_type': 'Lung Cancer', 'stage': 'II'},\n    'P002': {'age': 60, 'cancer_type': 'Breast Cancer', 'stage': 'I'},\n    'P003': {'age': 45, 'cancer_type': 'Melanoma', 'stage': 'III'}\n}\n\nprint(patient_data)\n\n\n\n{'P001': {'age': 50, 'cancer_type': 'Lung Cancer', 'stage': 'II'}, 'P002': {'age': 60, 'cancer_type': 'Breast Cancer', 'stage': 'I'}, 'P003': {'age': 45, 'cancer_type': 'Melanoma', 'stage': 'III'}}\n\n\n\nYou can access a patient’s information using their unique ID. To access nested data, chain the keys. For example, to retrieve the cancer type of a specific patient, you’d use the following:\n\n# Accessing specific information\n\npatient_id = 'P002'\ncancer_type = patient_data[patient_id]['cancer_type']\nprint(f\"Cancer type for {patient_id}: {cancer_type}\")\n\n\n\nCancer type for P002: Breast Cancer\n\n\n\n# Updating a patient’s stage\npatient_data['P003']['stage'] = 'IV'\nprint(f\"Updated stage for P003: {patient_data['P003']['stage']}\")\n\n\n\nUpdated stage for P003: IV"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#adding-and-removing-data",
    "href": "session2a/DataStructuresDemo.html#adding-and-removing-data",
    "title": "Session 2: Python Data Structures",
    "section": "Adding and Removing Data",
    "text": "Adding and Removing Data\nNew patient data can be added using assignment, and pop() or del can remove a patient’s data.\n\n# Adding a new patient\npatient_data['P004'] = {'age': 70, 'cancer_type': 'Prostate Cancer', 'stage': 'II'}\nprint(\"Added new patient:\", patient_data['P004'])\n\n\n\nAdded new patient: {'age': 70, 'cancer_type': 'Prostate Cancer', 'stage': 'II'}\n\n\n\n# Removing a patient\nremoved_patient = patient_data.pop('P001')\nprint(\"Removed patient:\", removed_patient)\n\n\n\nRemoved patient: {'age': 50, 'cancer_type': 'Lung Cancer', 'stage': 'II'}"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#dictionary-methods-1",
    "href": "session2a/DataStructuresDemo.html#dictionary-methods-1",
    "title": "Session 2: Python Data Structures",
    "section": "Dictionary Methods",
    "text": "Dictionary Methods\nDictionaries allow you to retrieve keys, values, or entire key-value pairs. Here’s how to use these methods to get an overview of the data.\nkeys(): Retrieves all patient IDs. values(): Retrieves all patient records. items(): Retrieves patient records as key-value pairs\n\nFurther Example\n\n# Getting all patient IDs\nprint(\"Patient IDs:\", patient_data.keys())\n\n# Getting all patient details\nprint(\"Patient Details:\", patient_data.values())\n\n# Looping through each patient's data\nfor patient_id, details in patient_data.items():\n    print(f\"Patient {patient_id} - Age: {details['age']}, Cancer Type: {details['cancer_type']}, Stage: {details['stage']}\")\n\n\n\nPatient IDs: dict_keys(['P002', 'P003', 'P004'])\nPatient Details: dict_values([{'age': 60, 'cancer_type': 'Breast Cancer', 'stage': 'I'}, {'age': 45, 'cancer_type': 'Melanoma', 'stage': 'IV'}, {'age': 70, 'cancer_type': 'Prostate Cancer', 'stage': 'II'}])\nPatient P002 - Age: 60, Cancer Type: Breast Cancer, Stage: I\nPatient P003 - Age: 45, Cancer Type: Melanoma, Stage: IV\nPatient P004 - Age: 70, Cancer Type: Prostate Cancer, Stage: II"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#topic-5-functions-vs-methods-in-python",
    "href": "session2a/DataStructuresDemo.html#topic-5-functions-vs-methods-in-python",
    "title": "Session 2: Python Data Structures",
    "section": "Topic 5: Functions vs Methods in Python",
    "text": "Topic 5: Functions vs Methods in Python\nWhat’s the difference?\n\n\n\n\n\n\n\n\nConcept\nFunction\nMethod\n\n\n\n\nDefinition\nA block of code that performs an action\nA function that is associated with an object\n\n\nCalled on\nStandalone / with parameters\nCalled on an object (e.g., a string or list)\n\n\nSyntax\nfunction(arg)\nobject.method()\n\n\nExample\nlen(\"hello\")\n\"hello\".upper()"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#function-example",
    "href": "session2a/DataStructuresDemo.html#function-example",
    "title": "Session 2: Python Data Structures",
    "section": "Function Example",
    "text": "Function Example\n\n# Define your own function\ndef greet(name):\n    return f\"Hello, {name}!\"\n\nprint(greet(\"Alice\"))  # Hello, Alice!\n\n\n\nHello, Alice!\n\n\n\n# Use a built-in function\nwords = [\"Python\", \"Data\", \"Science\"]\nprint(len(words))  # 3\n\n\n\n3"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#method-example",
    "href": "session2a/DataStructuresDemo.html#method-example",
    "title": "Session 2: Python Data Structures",
    "section": "Method Example",
    "text": "Method Example\n\n# 'upper()' is a method of string objects\nname = \"patrick\"\nprint(name.upper())  # Output: PATRICK\nprint(name) #Why doesn't name.upper() modify the original string? What would we have to do so that name becomes permanently uppercase?\n\n\n\nPATRICK\npatrick\n\n\n\n# 'append()' is a method of list objects\ncolors = [\"red\", \"blue\"]\ncolors.append(\"green\")\nprint(colors)  # ['red', 'blue', 'green']\n\n#Think back to the last example.  Why does colors permanently change when we use a method, but name did not?\n\n\n\n['red', 'blue', 'green']"
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#behind-the-scenes",
    "href": "session2a/DataStructuresDemo.html#behind-the-scenes",
    "title": "Session 2: Python Data Structures",
    "section": "Behind the Scenes",
    "text": "Behind the Scenes\n\n# This also works\nstr.upper(\"patrick\")  # Output: 'PATRICK'\n\n\n\n'PATRICK'\n\n\n\n# But this is more common\n\"patrick\".upper()  # Output: 'PATRICK'\n\n\n\n'PATRICK'\n\n\n\nBoth work — but “str.upper()” is a method being called directly from the class."
  },
  {
    "objectID": "session2a/DataStructuresDemo.html#you-try",
    "href": "session2a/DataStructuresDemo.html#you-try",
    "title": "Session 2: Python Data Structures",
    "section": "You Try!",
    "text": "You Try!\nNavigate to the follow-along file and try the practice problems!"
  },
  {
    "objectID": "session2b/DataStructuresDemo_2.html#links",
    "href": "session2b/DataStructuresDemo_2.html#links",
    "title": "Session 3: Pandas Data Structures",
    "section": "Links",
    "text": "Links\n \n\nGuide to Python Data Structures\n\n  \n\nCancer Dataset"
  },
  {
    "objectID": "session2b/DataStructuresDemo_2.html#topic-5-intro-to-pandas",
    "href": "session2b/DataStructuresDemo_2.html#topic-5-intro-to-pandas",
    "title": "Session 3: Pandas Data Structures",
    "section": "Topic 5: Intro to Pandas",
    "text": "Topic 5: Intro to Pandas\nPandas is a powerful open-source data analysis and manipulation library in Python. It provides data structures, primarily the DataFrame and Series, which are optimized for handling and analyzing large datasets efficiently.\nData Structures:\nSeries: A one-dimensional labeled array, suitable for handling single columns or rows of data.\nDataFrame: A two-dimensional table with labeled axes (rows and columns), much like a spreadsheet or SQL table, allowing you to work with data in rows and columns simultaneously.\nData Manipulation:\nPandas has functions for merging, reshaping, and aggregating datasets, which helps streamline data cleaning and preparation.\n\nIt can handle missing data, making it easy to filter or fill gaps in datasets.\nData Analysis:\nIt provides extensive functionality for descriptive statistics, grouping data, and handling time series.\nIntegrates well with other libraries, making it easy to move data between libraries like NumPy for numerical computations and Matplotlib or Seaborn for visualization.\nCreating folders for project housekeeping\n\n###Example Folder Structure\n'''\nproject_name/\n    data/\n        raw/\n        processed/\n    scripts/\n    results/\n    logs/\n'''\n\n#To Create folders\n\nimport os\n\n#Defining working directory\nbase_dir = \"G:\\\\dir_demo\"\n\n#Defining Project folder\nproject_name = os.path.join(base_dir, \"my_project\")\n\n# Define the subdirectories\nsubdirs = [\n    \"data/raw\",\n    \"data/processed\",\n    \"scripts\",\n    \"results\",\n    \"logs\",\n]\n\n# Create directories\nfor subdir in subdirs:\n    path = os.path.join(project_name, subdir)\n    os.makedirs(path, exist_ok=True)  #ensures no error if the folder already exists\n    print(f\"Created directory: {path}\")\n\nCreated directory: G:\\dir_demo\\my_project\\data/raw\nCreated directory: G:\\dir_demo\\my_project\\data/processed\nCreated directory: G:\\dir_demo\\my_project\\scripts\nCreated directory: G:\\dir_demo\\my_project\\results\nCreated directory: G:\\dir_demo\\my_project\\logs\n\n\nLoading the Dataset\n\nimport os\nimport pandas as pd\n\n# Load the dataset\ncancer_data = pd.read_csv(os.path.join('example_data', 'Cancer_Data.csv'))\n\nprint (type(cancer_data))\n\n\n# Display the first few rows of the dataset\ncancer_data.head()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\n...\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\nUnnamed: 32\n\n\n\n\n0\n842302\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\nNaN\n\n\n1\n842517\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\nNaN\n\n\n2\n84300903\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\nNaN\n\n\n3\n84348301\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n...\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\nNaN\n\n\n4\n84358402\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n...\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\nNaN\n\n\n\n\n5 rows × 33 columns\n\n\n\nViewing Basic Information a. Checking the Dataset’s Shape\n.shape returns a tuple with (number of rows, number of columns), which provides a basic overview of the dataset size.\n\n# Display the shape of the dataset\nprint(\"Dataset Shape:\", cancer_data.shape)\n\nDataset Shape: (569, 33)\n\n\n\nSummarizing Column Information\n\n.info() lists all columns, their data types, and counts of non-null values, helping identify any columns that may have missing data.\n\n# Display column names, data types, and non-null counts\ncancer_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 33 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   id                       569 non-null    int64  \n 1   diagnosis                569 non-null    object \n 2   radius_mean              569 non-null    float64\n 3   texture_mean             569 non-null    float64\n 4   perimeter_mean           569 non-null    float64\n 5   area_mean                569 non-null    float64\n 6   smoothness_mean          569 non-null    float64\n 7   compactness_mean         569 non-null    float64\n 8   concavity_mean           569 non-null    float64\n 9   concave points_mean      569 non-null    float64\n 10  symmetry_mean            569 non-null    float64\n 11  fractal_dimension_mean   569 non-null    float64\n 12  radius_se                569 non-null    float64\n 13  texture_se               569 non-null    float64\n 14  perimeter_se             569 non-null    float64\n 15  area_se                  569 non-null    float64\n 16  smoothness_se            569 non-null    float64\n 17  compactness_se           569 non-null    float64\n 18  concavity_se             569 non-null    float64\n 19  concave points_se        569 non-null    float64\n 20  symmetry_se              569 non-null    float64\n 21  fractal_dimension_se     569 non-null    float64\n 22  radius_worst             569 non-null    float64\n 23  texture_worst            569 non-null    float64\n 24  perimeter_worst          569 non-null    float64\n 25  area_worst               569 non-null    float64\n 26  smoothness_worst         569 non-null    float64\n 27  compactness_worst        569 non-null    float64\n 28  concavity_worst          569 non-null    float64\n 29  concave points_worst     569 non-null    float64\n 30  symmetry_worst           569 non-null    float64\n 31  fractal_dimension_worst  569 non-null    float64\n 32  Unnamed: 32              0 non-null      float64\ndtypes: float64(31), int64(1), object(1)\nmemory usage: 146.8+ KB\n\n\n\nViewing Column Names\n\n.columns lists column headers, while .tolist() converts it into a standard Python list for easier viewing.\n\n# Display column names\nprint(\"Column Names:\", cancer_data.columns.tolist())\n\nColumn Names: ['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32']\n\n\nSummary Statistics\n.describe() generates essential statistics (mean, std, min, max, percentiles) for numeric columns, useful for identifying data distributions.\n\n# Generate summary statistics for numeric columns\ncancer_data.describe()\n\n\n\n\n\n\n\n\nid\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\n...\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\nUnnamed: 32\n\n\n\n\ncount\n5.690000e+02\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n...\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n0.0\n\n\nmean\n3.037183e+07\n14.127292\n19.289649\n91.969033\n654.889104\n0.096360\n0.104341\n0.088799\n0.048919\n0.181162\n...\n25.677223\n107.261213\n880.583128\n0.132369\n0.254265\n0.272188\n0.114606\n0.290076\n0.083946\nNaN\n\n\nstd\n1.250206e+08\n3.524049\n4.301036\n24.298981\n351.914129\n0.014064\n0.052813\n0.079720\n0.038803\n0.027414\n...\n6.146258\n33.602542\n569.356993\n0.022832\n0.157336\n0.208624\n0.065732\n0.061867\n0.018061\nNaN\n\n\nmin\n8.670000e+03\n6.981000\n9.710000\n43.790000\n143.500000\n0.052630\n0.019380\n0.000000\n0.000000\n0.106000\n...\n12.020000\n50.410000\n185.200000\n0.071170\n0.027290\n0.000000\n0.000000\n0.156500\n0.055040\nNaN\n\n\n25%\n8.692180e+05\n11.700000\n16.170000\n75.170000\n420.300000\n0.086370\n0.064920\n0.029560\n0.020310\n0.161900\n...\n21.080000\n84.110000\n515.300000\n0.116600\n0.147200\n0.114500\n0.064930\n0.250400\n0.071460\nNaN\n\n\n50%\n9.060240e+05\n13.370000\n18.840000\n86.240000\n551.100000\n0.095870\n0.092630\n0.061540\n0.033500\n0.179200\n...\n25.410000\n97.660000\n686.500000\n0.131300\n0.211900\n0.226700\n0.099930\n0.282200\n0.080040\nNaN\n\n\n75%\n8.813129e+06\n15.780000\n21.800000\n104.100000\n782.700000\n0.105300\n0.130400\n0.130700\n0.074000\n0.195700\n...\n29.720000\n125.400000\n1084.000000\n0.146000\n0.339100\n0.382900\n0.161400\n0.317900\n0.092080\nNaN\n\n\nmax\n9.113205e+08\n28.110000\n39.280000\n188.500000\n2501.000000\n0.163400\n0.345400\n0.426800\n0.201200\n0.304000\n...\n49.540000\n251.200000\n4254.000000\n0.222600\n1.058000\n1.252000\n0.291000\n0.663800\n0.207500\nNaN\n\n\n\n\n8 rows × 32 columns\n\n\n\nUsing value_counts() on a Single Column\nThis method is straightforward if you want to check the frequency distribution of one specific categorical column. Returns a pandas series object\n\n# Count occurrences of each unique value in the 'diagnosis' column\ndiagnosis_counts = cancer_data['diagnosis'].value_counts()\nprint(\"Diagnosis Counts:\\n\", diagnosis_counts)\n\nDiagnosis Counts:\n diagnosis\nB    357\nM    212\nName: count, dtype: int64\n\n\nTo see summary statistics grouped by a categorical variable in pandas, you can use the groupby() method along with describe() or specific aggregation functions like mean(), sum(), etc.\n\n# Group by 'diagnosis' and get summary statistics for each group\ngrouped_summary = cancer_data.groupby('diagnosis').mean()\nprint(grouped_summary)\n\n\n#Group by 'diagnosis' and get summary statistics for only one variable\ngrouped_radius_mean = cancer_data.groupby('diagnosis')['radius_mean'].mean()\nprint(grouped_radius_mean)\n\n                     id  radius_mean  texture_mean  perimeter_mean  \\\ndiagnosis                                                            \nB          2.654382e+07    12.146524     17.914762       78.075406   \nM          3.681805e+07    17.462830     21.604906      115.365377   \n\n            area_mean  smoothness_mean  compactness_mean  concavity_mean  \\\ndiagnosis                                                                  \nB          462.790196         0.092478          0.080085        0.046058   \nM          978.376415         0.102898          0.145188        0.160775   \n\n           concave points_mean  symmetry_mean  ...  texture_worst  \\\ndiagnosis                                      ...                  \nB                     0.025717       0.174186  ...      23.515070   \nM                     0.087990       0.192909  ...      29.318208   \n\n           perimeter_worst   area_worst  smoothness_worst  compactness_worst  \\\ndiagnosis                                                                      \nB                87.005938   558.899440          0.124959           0.182673   \nM               141.370330  1422.286321          0.144845           0.374824   \n\n           concavity_worst  concave points_worst  symmetry_worst  \\\ndiagnosis                                                          \nB                 0.166238              0.074444        0.270246   \nM                 0.450606              0.182237        0.323468   \n\n           fractal_dimension_worst  Unnamed: 32  \ndiagnosis                                        \nB                         0.079442          NaN  \nM                         0.091530          NaN  \n\n[2 rows x 32 columns]\ndiagnosis\nB    12.146524\nM    17.462830\nName: radius_mean, dtype: float64\n\n\nRenaming Columns To make column names more readable or consistent, you can use rename() to change specific names. Here’s how to rename columns like radius_mean to Radius Mean.\n\n# Rename specific columns for readability\n\nnew_columns={\n    'radius_mean': 'Radius Mean',\n    'texture_mean': 'Texture Mean',\n    'perimeter_mean': 'Perimeter Mean'\n}\n\ncancer_data = cancer_data.rename(columns=new_columns)\n\n# Display the new column names to verify the changes\nprint(\"\\nUpdated Column Names:\", cancer_data.columns.tolist())\n\n\nUpdated Column Names: ['id', 'diagnosis', 'Radius Mean', 'Texture Mean', 'Perimeter Mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean', 'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se', 'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se', 'fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst', 'area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst', 'concave points_worst', 'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32']\n\n\n#How does Python handle missingness?"
  },
  {
    "objectID": "session2b/DataStructuresDemo_2.html#finding-missing-values",
    "href": "session2b/DataStructuresDemo_2.html#finding-missing-values",
    "title": "Session 3: Pandas Data Structures",
    "section": "Finding Missing Values",
    "text": "Finding Missing Values\n\ndf = pd.DataFrame({\"A\": [1, np.nan, 3, None, pd.NA]})\nprint(df.isna())  # Identifies missing values\nfor x in df.iloc[:, 0]:\n    print(type(x))\n\n       A\n0  False\n1   True\n2  False\n3   True\n4   True\n&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n&lt;class 'int'&gt;\n&lt;class 'NoneType'&gt;\n&lt;class 'pandas._libs.missing.NAType'&gt;"
  },
  {
    "objectID": "session2b/DataStructuresDemo_2.html#filling-missing-values",
    "href": "session2b/DataStructuresDemo_2.html#filling-missing-values",
    "title": "Session 3: Pandas Data Structures",
    "section": "Filling Missing Values",
    "text": "Filling Missing Values\n\ndf[\"A\"]=df[\"A\"].fillna(pd.NA).astype('Float64')  # Replaces missing values with pd.NA, and then changes the column type to 'FLoat64, pandas' nullable float datatype.\nfor value in df.iloc[:,0]:\n        print(value)\n        print(type(value))\n\nprint(df[\"A\"].dtype)\n\n1.0\n&lt;class 'numpy.float64'&gt;\n&lt;NA&gt;\n&lt;class 'pandas._libs.missing.NAType'&gt;\n3.0\n&lt;class 'numpy.float64'&gt;\n&lt;NA&gt;\n&lt;class 'pandas._libs.missing.NAType'&gt;\n&lt;NA&gt;\n&lt;class 'pandas._libs.missing.NAType'&gt;\nFloat64\n\n\n\ndf[\"A\"]=df[\"A\"].astype('float64')  #typecasting as lowercase \"float\" changes all pd.NA back to np.nan because \"float64\" (lowercase) is not supported by pd.NA\nfor value in df.iloc[:,0]:\n        print(value)\n        print(type(value))\n\nprint(df[\"A\"].dtype)\n\n1.0\n&lt;class 'float'&gt;\nnan\n&lt;class 'float'&gt;\n3.0\n&lt;class 'float'&gt;\nnan\n&lt;class 'float'&gt;\nnan\n&lt;class 'float'&gt;\nfloat64"
  },
  {
    "objectID": "session2b/DataStructuresDemo_2.html#dropping-missing-values",
    "href": "session2b/DataStructuresDemo_2.html#dropping-missing-values",
    "title": "Session 3: Pandas Data Structures",
    "section": "Dropping Missing Values",
    "text": "Dropping Missing Values\n\ndf.dropna()  # Removes rows with missing values\n\n\n\n\n\n\n\n\nA\n\n\n\n\n0\n1.0\n\n\n2\n3.0\n\n\n\n\n\n\n\n\nfor value in df.iloc[:,0]:\n    if np.isnan(value):\n        print(value+1)\n\nnan\nnan\nnan"
  },
  {
    "objectID": "session2b/DataStructuresDemo_2.html#pandas-example-method-chaining",
    "href": "session2b/DataStructuresDemo_2.html#pandas-example-method-chaining",
    "title": "Session 3: Pandas Data Structures",
    "section": "Pandas Example: Method Chaining",
    "text": "Pandas Example: Method Chaining\n\nimport pandas as pd\n\n# Sample DataFrame\ndf = pd.DataFrame({\n    \"name\": [\" Alice \", \"BOB\", \"Charlie\", None],\n    \"score\": [85, 92, None, 74]\n})\n\n# Clean the data using method chaining\nclean_df = (\n    df\n    .dropna()                # Method: drop rows with any NaNs\n    .assign(                 # Method: add or update columns\n        name_clean=lambda d: d[\"name\"].str.strip().str.title()\n    )\n    .sort_values(\"score\", ascending=False)  # Method: sort by score\n)\n\nprint(clean_df)\n\n      name  score name_clean\n1      BOB   92.0        Bob\n0   Alice    85.0      Alice\n\n\nOutput:\n     name  score name_clean\n1     BOB   92.0        Bob\n0   Alice   85.0      Alice"
  },
  {
    "objectID": "session2b/DataStructuresDemo_2.html#why-.str.strip-and-not-just-.strip",
    "href": "session2b/DataStructuresDemo_2.html#why-.str.strip-and-not-just-.strip",
    "title": "Session 3: Pandas Data Structures",
    "section": "Why .str.strip() and not just .strip()?",
    "text": "Why .str.strip() and not just .strip()?\n\n# This works:\ndf[\"name\"].str.strip()\n\n# This does NOT:\ntry:\n    df[\"name\"].strip()  # ❌ AttributeError\nexcept AttributeError: \n    print(\"AttributeError: .strip is used for single strings, not a Series of strings\")\n\nAttributeError: .strip is used for single strings, not a Series of strings\n\n\n🧠 Why?\n\ndf[\"name\"] is a Series — not a string.\n.strip() is a string method that works on single strings.\n.str is the accessor that tells pandas: “apply this string method to each element in the Series.”\n\n✅ Rule of Thumb:\n\n\n\n\n\n\n\n\nYou have…\nUse…\nWhy?\n\n\n\n\nA single string\n\"hello\".strip()\nIt’s just Python\n\n\nA Series of strings\ndf[\"col\"].str.strip()\nIt’s pandas, operating on many strings"
  },
  {
    "objectID": "sessions.html",
    "href": "sessions.html",
    "title": "Sessions",
    "section": "",
    "text": "Session links\nTutorial: Get Started with Python \nGo to Session2a: Intro to Python Data Structures \nGo to Session2b: Intro to Pandas \nGo to ML Demo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Python Workshops: Intro to Python",
    "section": "",
    "text": "Welcome to the Intro to Python Workshops\nOur Intro to Python Workshops are geared towards R and SAS users who want to explore using Python. Python is a versatile language and is especially powerful for tasks such as machine learning, big data processing, and broader data science applications. We will cover installing and setting up python with anaconda, basic python datastructures, and finally a demo logistic regression model with a cancer dataset. We will upload session videos, python demos and other things here.\nWe value your feedback and aim to continually improve these workshops. If you have any suggestions, questions, or ideas, please feel free to share them on our  GitHub Discussions page. We look forward to hearing from you!\n\n\nLinks\n \n\nInstall Anaconda\n\n \n\nInstall VS Code\n\n \n\nInstall Quarto\n\n\n \n\nDownload environment YML file\n\n \n\nDownload Follow Along Files\n\n\n \n\nDownload Intro to Pandas Dataset\n\n\n\n\nSessions\nTutorial: Get Started with Python \nGo to Session2a: Intro to Python Data Structures \nGo to Session2b: Intro to Pandas \nGo to ML Demo"
  },
  {
    "objectID": "FAQ.html",
    "href": "FAQ.html",
    "title": "FAQ",
    "section": "",
    "text": "Question: Is it possible (or recommended) to create a virtual environment in a specific project folder? Are there advantages or disadvantages to keeping the virtual environment in the default location versus a network drive (e.g., H drive)?\nAnswer: It is definitely possible to create an environment in a specific project folder/on the H drive (conda create --prefix /path/to/your/project_folder/env_name)! Note that, to activate the environment, you need to activate it by the full path, not by a name–e.g., conda activate /path/to/your/project_folder/env_name.\n\nAdvantages:\n\nBackup and Portability: Keeping the environment with the project makes it easier to back up as a single unit and enables access on multiple devices without recreating the environment each time.\n\nDisadvantages:\n\nPerformance: Access speed may be slower on a network drive, and network or permissions issues could make the environment temporarily inaccessible. If you typically work off a network drive, storing your environments within the project folder can be beneficial for organization and consistency.\n\n\n\n\n\n\n\n\nActivating environments with custom folder locations\n\n\n\nYou need to activate the conda environment stored outside of the default location by the path, not by a name.\nconda activate /path/to/your/project_folder/env_name\nYou may also create a symbolic link in the default envs folder to point to your custom environment location.\n\nOn macOS:\nln -s /path/to/your/project_folder/env_name /path/to/conda/envs/&lt;custom_env_name&gt;\nOn Windows:\nmklink /J \"C:\\path\\to\\conda\\envs\\&lt;custom_env_name&gt;\" \"H:\\path\\to\\your\\project_folder\\env_name\"\n\nNow, you will be able to activate the environment by name only.\nconda activate &lt;custom_env_name&gt;\n\n\n\nQuestion: Will creating a virtual environment for each project lead to memory or storage issues? What is the best practice for closing out a project environment?\nAnswer: In general, having a virtual environment for each active project shouldn’t cause memory issues. However, it’s good practice to clean up environments when a project concludes.\n\nHere’s a recommended process:\n\nActivating the environment you want to export.\nExporting the environment to a .yaml file with conda env export &gt; environment.yaml.\nAfter the .yaml file is created (make sure to check before deleting the environment!), you can delete the environment with conda env remove --name env_name (or conda env remove --prefix path/to/env_name if using a specific path).\n\nTip: By exporting the environment before deletion, you can always recreate the environment later if needed by using:\nconda env create --prefix /path/to/your/project_folder/env_name -f environment.yaml\nconda activate /path/to/your/project_folder/env_name"
  },
  {
    "objectID": "FAQ.html#managing-virtual-environments-with-conda",
    "href": "FAQ.html#managing-virtual-environments-with-conda",
    "title": "FAQ",
    "section": "",
    "text": "Question: Is it possible (or recommended) to create a virtual environment in a specific project folder? Are there advantages or disadvantages to keeping the virtual environment in the default location versus a network drive (e.g., H drive)?\nAnswer: It is definitely possible to create an environment in a specific project folder/on the H drive (conda create --prefix /path/to/your/project_folder/env_name)! Note that, to activate the environment, you need to activate it by the full path, not by a name–e.g., conda activate /path/to/your/project_folder/env_name.\n\nAdvantages:\n\nBackup and Portability: Keeping the environment with the project makes it easier to back up as a single unit and enables access on multiple devices without recreating the environment each time.\n\nDisadvantages:\n\nPerformance: Access speed may be slower on a network drive, and network or permissions issues could make the environment temporarily inaccessible. If you typically work off a network drive, storing your environments within the project folder can be beneficial for organization and consistency.\n\n\n\n\n\n\n\n\nActivating environments with custom folder locations\n\n\n\nYou need to activate the conda environment stored outside of the default location by the path, not by a name.\nconda activate /path/to/your/project_folder/env_name\nYou may also create a symbolic link in the default envs folder to point to your custom environment location.\n\nOn macOS:\nln -s /path/to/your/project_folder/env_name /path/to/conda/envs/&lt;custom_env_name&gt;\nOn Windows:\nmklink /J \"C:\\path\\to\\conda\\envs\\&lt;custom_env_name&gt;\" \"H:\\path\\to\\your\\project_folder\\env_name\"\n\nNow, you will be able to activate the environment by name only.\nconda activate &lt;custom_env_name&gt;\n\n\n\nQuestion: Will creating a virtual environment for each project lead to memory or storage issues? What is the best practice for closing out a project environment?\nAnswer: In general, having a virtual environment for each active project shouldn’t cause memory issues. However, it’s good practice to clean up environments when a project concludes.\n\nHere’s a recommended process:\n\nActivating the environment you want to export.\nExporting the environment to a .yaml file with conda env export &gt; environment.yaml.\nAfter the .yaml file is created (make sure to check before deleting the environment!), you can delete the environment with conda env remove --name env_name (or conda env remove --prefix path/to/env_name if using a specific path).\n\nTip: By exporting the environment before deletion, you can always recreate the environment later if needed by using:\nconda env create --prefix /path/to/your/project_folder/env_name -f environment.yaml\nconda activate /path/to/your/project_folder/env_name"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "link.html",
    "href": "link.html",
    "title": "Links",
    "section": "",
    "text": "Links for downloading and installing mentioned software/files.\n \n\nInstall Anaconda\n\n\n \n\nDownload environment YML file\n\n\n \n\nDownload Follow Along File\n\n\n \n\nDownload Dataset for Intro to Pandas"
  },
  {
    "objectID": "session1/session1.html",
    "href": "session1/session1.html",
    "title": "Session 1. Python Installation",
    "section": "",
    "text": "Welcome to the first session of the Introduction to Python Workshop series!\nThis guide will you walk through the steps to install Python to your computer and work interactively with it in Visual Studio Code. After the session, you will acquire basic knowledge of the following:"
  },
  {
    "objectID": "session1/session1.html#anaconda",
    "href": "session1/session1.html#anaconda",
    "title": "Session 1. Python Installation",
    "section": "Anaconda",
    "text": "Anaconda\n\n\n\n\n\n\nAnaconda is an open-source distribution of Python, designed for scientific computing, data science, machine learning, and AI development.\n\nThe Anaconda distribution comes with the desktop Anaconda Navigator application, the latest version of Python, and ~150 pre-downloaded libraries.\nIt also comes with conda, a cross-platform package and environment manager. Conda is the virtual environment manager that helps install packages from the Anaconda repositories (also called channels). It supports more than just Python packages but numerous other programming languages like R, Java, etc.\n\n\n\n\n\n\nDifference Between Conda and Pip\n\n\n\nLong story short: Pip is for Python libraries only, while conda can install packages for any software (including python).\nIt is generally recommended that you only use conda install when in a conda environment, as anything installed via pip won’t be recognized by conda and vice versa. Using the two interchangeably might overwrite or break packages and mess up the environment.\nWhat if the Python package is unavailable through conda?\nIn these scenarios where the package you need is not built under conda, it makes sense to use pip to install packages within the conda environment. The best practice is to install everything with conda first, then use pip if needed.\nCheck out this blog for more information on using pip in a conda environment.\n\n\n\nConda, Miniconda, and Anaconda\n\n\n\nDifference between conda, miniconda, and Anaconda\n\n\nIn this tutorial, we are going to install the full Anaconda distribution and learn to use the features of its Anaconda Navigator desktop application. This allows us to manage packages and environments without necessarily needing to know the conda terminal commands."
  },
  {
    "objectID": "session1/session1.html#visual-studio-code",
    "href": "session1/session1.html#visual-studio-code",
    "title": "Session 1. Python Installation",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\n\n\nVisual Studio Code (VS Code) is one of the most popular open-source code editors with many features.\n\n\nMulti-Language Programming. You can code in almost any major programming languages in VS Code including Python, R, C/C++, JavaScript, etc.\nBuild-In Git Source Control. VS Code automatically recognizes and uses the computer’s Git installation to allow project version control. You can easily track changes, stage, and commit changes to your working branch.\nVariety of Project Development Support. You can add extra features such as language packs, debugging tools, Git/Github features, and remote server connector by installing extensions from the Extension Marketplace.\n\nGo to 1.4 Work with Jupyter/Quarto in VS Code for guide on how to get started with VS Code."
  },
  {
    "objectID": "session1/session1.html#quarto",
    "href": "session1/session1.html#quarto",
    "title": "Session 1. Python Installation",
    "section": "Quarto",
    "text": "Quarto\n\n\nQuarto is an open-source scientific and technical publishing system.\n\nThink of Quarto .qmd as similar to R Markdown .Rmd files. Both combine executable code chunks with text components and figures and allow generating outputs as PDF, HTML, Docx, and even slideshow presentations.\nBut Quarto has more:\n\nCompatibility with multiple IDEs. You can not only work with Quarto .qmd in RStudio, but also VS Code, Jupyter Lab, etc.\nMutli-lingual support. Unlike R Markdown which is dependent on R, Quarto does not require R. It supports embedded Python, JavaScript, and Julia executable code by simply specifying the language name in the braces on top of a code chunk (e.g., ```{python}).\nMulti-engine support. Don’t worry if you are an R Markdown or Jupyter Notebook user! Qurato also works with .Rmd and .ipynb files and will automatically deploy either the knitr or jupyter engine depending on the file type that you are rendering.\n\nIn summary, Quarto is easy for R users to transition into due to similarities with the R Markdown. It also includes more functionality and flexibility, making it a great tool for learning Python."
  },
  {
    "objectID": "session1/session1.html#what-is-a-virtual-environment",
    "href": "session1/session1.html#what-is-a-virtual-environment",
    "title": "Session 1. Python Installation",
    "section": "What is a virtual environment?",
    "text": "What is a virtual environment?\nEnvironments are isolated, independent installations of a programming language and groups of packages that don’t interfere with each other.\nFor example, you may have a Python version 3.8 installed on your computer as the System Python. Meanwhile, you can install as many virtual environments as you want with the same or different Python versions and set of packages.\nYou can switch between environments for different projects, create environment files and share them with others."
  },
  {
    "objectID": "session1/session1.html#why-virtual-environments",
    "href": "session1/session1.html#why-virtual-environments",
    "title": "Session 1. Python Installation",
    "section": "Why virtual environments?",
    "text": "Why virtual environments?\nYou may find the flexibility of environments useful in many cases.\n\nAvoid Conflicts. When you need libraries that are not compatibles with your system settings, such as an older Python version or conflicting dependencies. Creating a virtual environment can resolve the conflicts and changing it won’t affect your other environments.\nSharing Environment Setting. You can also share your environment and the list of dependencies with someone with a copy of the environment.yaml file.\nEasy Management. When your work is temporary or that you simply want to experiment things without having to worry about things breaking, you can work within a virtual environemnt and later delete it when needed."
  },
  {
    "objectID": "session1/session1.html#creating-a-virtual-environment-with-gui",
    "href": "session1/session1.html#creating-a-virtual-environment-with-gui",
    "title": "Session 1. Python Installation",
    "section": "1.3.1 Creating a Virtual Environment (with GUI)",
    "text": "1.3.1 Creating a Virtual Environment (with GUI)\nOne option to create an environment in through the Anaconda Navigator graphical user interface (GUI). This approach is straightforward and does not require command line skills.\n\nOpen the Anaconda Navigator application.\n\nThe Home page shows tabs for software available to be installed or launched in the Navigator (E.g., RStudio, VS Code, JupyterLab, and Jupyter Notebook). The first dropdown menu at the top allows filtering applications shown below.\nFrom the second dropdown menu, you may switch to other conda environments that you have created. Then you may launch applications from within the environment you selected.\nCreate the first virtual environment.\nSelect the Environments tab on the left. Click Create as shown below:\n\nCustomizing environment name and Python version.\nIn the environment Name field, type a descriptive name for your environment. Then choose the Python version you want (default is the latest version). For example, create an environment with Python 3.10 and name it python310.\nClick Create. Navigator creates the new environment and activates it.\nNow you have two environments: the default environment base (root) and the one you just created.\nInstalling packages.\n\nStay in the virtual environment you just created. Open the dropdown filter and select Not Installed. Type the name of the package you want to install into the upper right search box. E.g., seaborn.\nSelect the checkbox and click Apply. The selected package will be installed.\nNow you can see the package name listed under the Installed category.\nInstalling packages available outside of the default channel.\nBy default, the Navigator shows packages under the defaults channel. There are many more channels available in Anaconda.org and Anaconda.cloud with a wider range of packages to install.\nFor example, if we want to install a package from the conda-forge channel, we will first need to add it to Navigator.\n\nClick Channels\n\nClick Add….\nType conda-forge in the text box and press Enter.\nClick Update channels.\n\nNow your package search will also include packages on the conda-forge channel.\nRemoving an environment.\nOn the Environments page, select the environment you want to remove. Click Remove.\n\nThe entire environment, including the packages installed to it, will be deleted."
  },
  {
    "objectID": "session1/session1.html#creating-a-virutal-environment-with-command-line",
    "href": "session1/session1.html#creating-a-virutal-environment-with-command-line",
    "title": "Session 1. Python Installation",
    "section": "1.3.2 Creating a Virutal Environment (with Command Line)",
    "text": "1.3.2 Creating a Virutal Environment (with Command Line)\nAlternatively, you can use the conda command line interface (CLI) to create virtual environments.\n\n\n\n\n\n\nTip\n\n\n\nFor users comfortable with command line, this approach is generally more recommended than Anaconda Navigator, as it is faster, more robust to broken environments (reported by some who used the GUI), and offers greater flexibility and functionality for environment management.\n\n\n\nCreate the virtual environment.\nconda create --name &lt;env-name&gt;\nReplace &lt;env-name&gt; with the name you want to give your environment.\nNote: you can use -n (shorthand) and --name interchangeably.\nYou can also specify the Python version and packages you want to install to your environment.\nconda create -n &lt;env-name&gt; python=3.10 scipy=0.17.3 pandas matplotlib\nOr:\nconda create -n &lt;env-name&gt; python=3.10\nconda install -n &lt;env-name&gt; scipy pandas matplotlib\nYou can also specify channel other than the defaults channel (for multiple channels, pass the argument multiple times):\nconda install -n &lt;env-name&gt; scipy --channel conda-forge --channel bioconda\nNow, activate your environment.\nconda activate &lt;env-name&gt;\nYou can also verify that your installation was successful by looking up the list of all current environments on your computer.\nconda env list\nThe default location for the installed conda environments (except for the base conda environment) is ..\\anaconda3\\envs\\&lt;env-name&gt;\nDeactivate the conda environment.\nSimply use conda deactivate.\n\n\n\n\n\n\nNote: Avoid activating on top of another virtual environment!\n\n\n\nBe careful when activating environments. Remember to always deactivate the current environment before going into another one because environments can be stacked. This can lead to chaos in the packages in both environments. So make sure that you see (base) at the beginning of the terminal prompt line when you are about to activate an environment.\n\n\nRemoving an environment.\n\nRemove by environment name:\nconda env remove -n &lt;env-name&gt;\nRemove by environment folder path:\nconda env remove --prefix &lt;/path/to/your/env&gt;"
  },
  {
    "objectID": "session1/session1.html#creating-an-environment-from-an-environment.yml-file",
    "href": "session1/session1.html#creating-an-environment-from-an-environment.yml-file",
    "title": "Session 1. Python Installation",
    "section": "1.3.3 Creating an Environment from an environment.yml File",
    "text": "1.3.3 Creating an Environment from an environment.yml File\nWe can also create a virtual environment from a .yml file.\nconda env create -f environment.yml\nSimilarly, after installation, activate the new environment:\nconda activate &lt;env-name&gt;\nThis way, we can easily recreate an environment that is shared by others or share our environment settings with others.\nFor example, a simple environment file that has information about the environment name, channels, and dependencies:\nname: python310\nchannels:\n  - defaults\ndependencies:\n  - python==3.10\n  - pandas\n  - numpy\nDownload the YML file for this Python workshop series here. This file includes the required channel and dependencies for completing the workshop sessions."
  },
  {
    "objectID": "session1/session1.html#prerequisites",
    "href": "session1/session1.html#prerequisites",
    "title": "Session 1. Python Installation",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nInstall Everything\n\nInstall VS Code: Download and install VS Code\nInstall Quarto: Install from the official website\nInstall Conda: Go to previous section 1.2 Install Anaconda."
  },
  {
    "objectID": "session1/session1.html#setting-up-vs-code",
    "href": "session1/session1.html#setting-up-vs-code",
    "title": "Session 1. Python Installation",
    "section": "Setting up VS Code",
    "text": "Setting up VS Code\n\nOpen VS Code.\nInstall Extensions from Extension Marketplace. Click Extensions from the left toolbar or click Ctrl+Shift+X (or Cmd+Shift+X on Mac).\n\nPython: To support Python language, debugging, documentations, etc.\nQuarto: To support Quarto document editing and previewing.\nJupyter: To support rendering Python documents from Jupyter Notebooks or Quarto files.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou still need to install conda (Python) and Quarto to your computer to enable these extensions.\n\n\nCreate a Conda Virtual Environment. You may also use existing ones you created.\n\nRequired packages/dependencies (to install, use conda install &lt;package-name&gt; –channel conda-forge):\n\njupyter\nipykernel\npyyaml\n\n\n\n\nNote: we recommend installing an environment from this YML file. See [1.3.3 Creating an Environment from an environment.yml File] for detailed instructions.\n\nConfigure the Environment in VS Code.\n\nOpen Command Palette by pressing Ctrl+Shift+P (or Cmd+Shift+X on Mac).\nType “Python: Select Interpreter”.\nChoose the Conda python environment you created. If you don’t see its name pop up, choose Enter interpreter path… and manually type in the location of your conda virtual environment python executable.\nE.g., C:\\Users\\&lt;username&gt;\\AppData\\Local\\anaconda3\\envs\\&lt;env-name&gt;\\python.exe for Windows or Users/&lt;username&gt;/anaconda/envs/&lt;env-name&gt;/python for macOS.\n\n\n\n\n\n\nFind your conda Python executable path\n\n\n\nYou can search for the conda Python interpreter location on your computer. Here is an example of how to do it using command line.\nFor both Windows and macOS, open the Anaconda Prompt or terminal and activate the conda Python environment with conda activate &lt;env-name&gt;.\nThen, locate your Python executable by typing the following:\n\nWindows: where python\nmacOS: which python\n\n\n\n\n\nNow you are ready to go. Let’s start with creating a new Quarto file."
  },
  {
    "objectID": "session1/session1.html#create-a-quarto-file",
    "href": "session1/session1.html#create-a-quarto-file",
    "title": "Session 1. Python Installation",
    "section": "Create a Quarto File",
    "text": "Create a Quarto File\nGo to File &gt; New File and select Quarto Document. This will generate an empty .qmd file with the following YAML metadata. You can change the title and file output format as needed.\n---\ntitle: \"First Day\"\nformat: html\n---\nIn Quarto files, you can mix Markdown with code blocks just like in R Markdown. For example:\n# Heading 1\nThis is a demo Quarto file. Here is the text section.\n\n## Heading 2\nThis is a subsection.\nTo add an executable code block, specify the language name inside two curly braces (e.g., {python}, {r}, etc.). For example, let’s print something:\n```{python}\nprint(\"This is some Python output.\")\n```\n\n\nThis is some Python output.\n\n\n\nFile Previewing\nTo preview the html output, click on the Quarto Preview button at the top right (or press Ctrl+Shift+K).\nAlternatively, run quarto preview in your terminal to view the output.\nquarto preview &lt;yourfilename&gt;.qmd\nThis is an example of the html output preview showing next to the source .qmd file.\n\n\n\nFile Rendering/ Exporting to other Formats\nTo save the output, use Quarto: Render Document from the command search bar (Ctrl+Shift+P).\n\n\n\n\n\nThen select the desired output format. For example, Render HTML.\n\nSimilarly, you can use command line quarto render in the terminal to save the output.\nquarto render &lt;yourfilename&gt;.qmd\nNow you should see a &lt;yourfilename&gt;.html file generated in your working directory."
  },
  {
    "objectID": "session3/session3.html",
    "href": "session3/session3.html",
    "title": "Python Logistic Regression Demo",
    "section": "",
    "text": "Cancer Dataset\n\n  \n\nDownload Follow Along File"
  },
  {
    "objectID": "session3/session3.html#links",
    "href": "session3/session3.html#links",
    "title": "Python Logistic Regression Demo",
    "section": "",
    "text": "Cancer Dataset\n\n  \n\nDownload Follow Along File"
  },
  {
    "objectID": "session3/session3.html#getting-started",
    "href": "session3/session3.html#getting-started",
    "title": "Python Logistic Regression Demo",
    "section": "Getting Started",
    "text": "Getting Started\nBefore doing anything else, we should first activate the conda environment we want to use. If you created the ‘python-intro-env’ environment, please use that.\n\n\nRefresher: How to activate conda environment\n\n\n\n\nFrom terminal, type:\n\n&gt; conda activate ENVNAME\n\n\n\n\nWhen in VS code, you might get a popup message like the one below, confirming that the environment was activated:\n\nSelected conda environment was successfully activated, even though “(ENVNAME)” indicator may not be present in the terminal prompt.\n\nor\nIn Anaconda Navagator, click on the Environments tab on the left and select the environment you want to activate. Just selecting the environment should activate it.\n\n\n\nRefresher: How to install packages\n\nTo install packages, we can either use the “anaconda” dashboard, or we can use the command line. Make sure your environment is active before installing packages or the packages will not be available in your environment.\nTo install from the command line, we open a terminal and type:\n\n&gt; conda install {package}\n\nor\n\n&gt; pip install {package}\n\nWhen working with conda environments, it’s best practice to install everything with conda and only use pip for packages that are not available through conda!\n\n\nIf you are using the ‘python-intro-env’ environment, you may need to install the ‘statsmodels’ package if you have not already installed it.\n\nStatsmodels can be installed via conda install so:\n\n&gt; conda install statsmodels\n\n\nChecking installed packages\nIf we want to make sure we have the packages we’ll need installed in the environment before we try to import them, we can either check on anaconda or use the terminal:\n\n&gt; conda list\n\n\n\n\nOtherwise, we will get an error message if we try to import packages that are not installed.\nWe can also check for a specific package, like pandas, with `conda list {package}. See example below:\n\n&gt; conda list pandas\n # packages in environment at C:\\…\\anaconda3\\envs\\python-intro-env: # Name Version Build Channel  pandas 2.2.2 py310h5da7b33_0"
  },
  {
    "objectID": "session3/session3.html#step-1-import-packages",
    "href": "session3/session3.html#step-1-import-packages",
    "title": "Python Logistic Regression Demo",
    "section": "Step 1: Import Packages",
    "text": "Step 1: Import Packages\nSimilar to library() in R, we’ll use import in Python. Unlike R, however, python lets you set what ‘nickname’ you want to use for each package. There are some standard conventions for these import statements (like pandas typically being imported as pd) and following them helps make your code more readable.\nFill in the blanks to import the necessary packages:\n\nimport pandas as ___\nimport numpy as ___\nimport seaborn as ___\nimport matplotlib.pyplot as ___\n\nimport statsmodels.api as __\nimport statsmodels.formula.api as ___\n\n# Import from sklearn\nfrom sklearn.model_selection import ___\nfrom sklearn.preprocessing import ____\nfrom sklearn.decomposition import ___\n\n\nfrom sklearn.linear_model import ___\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\n# Import display from IPython to allow display of plots in notebook\nfrom IPython.display import display\n\nThe answers can be found under the drop down below.\n\n\n\nClick to reveal answers\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n## import from sklearn (scikit-learn)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\n# Import display from IPython to allow display of plots in notebook\nfrom IPython.display import display"
  },
  {
    "objectID": "session3/session3.html#step-2-read-in-data-and-perform-data-cleaning",
    "href": "session3/session3.html#step-2-read-in-data-and-perform-data-cleaning",
    "title": "Python Logistic Regression Demo",
    "section": "Step 2: Read in Data and Perform Data Cleaning",
    "text": "Step 2: Read in Data and Perform Data Cleaning\nWe can use the read_csv() function from the pandas package to read in the dataset.\n\ndata = pd.read_csv(\"__________\")\n\n\n\n\nClick to reveal answers\n\n\ndata = pd.read_csv(\"example_data/Cancer_Data.csv\")\n\n\n\nWe can use the .info() function to show some basic information about the dataset like:\n* the number of rows\n* number of columns\n* column labels\n* column type\n* number of non-null values in each column\n\ndata._______()\n\n\n\n\nClick to reveal answers\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 33 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   id                       569 non-null    int64  \n 1   diagnosis                569 non-null    object \n 2   radius_mean              569 non-null    float64\n 3   texture_mean             569 non-null    float64\n 4   perimeter_mean           569 non-null    float64\n 5   area_mean                569 non-null    float64\n 6   smoothness_mean          569 non-null    float64\n 7   compactness_mean         569 non-null    float64\n 8   concavity_mean           569 non-null    float64\n 9   concave points_mean      569 non-null    float64\n 10  symmetry_mean            569 non-null    float64\n 11  fractal_dimension_mean   569 non-null    float64\n 12  radius_se                569 non-null    float64\n 13  texture_se               569 non-null    float64\n 14  perimeter_se             569 non-null    float64\n 15  area_se                  569 non-null    float64\n 16  smoothness_se            569 non-null    float64\n 17  compactness_se           569 non-null    float64\n 18  concavity_se             569 non-null    float64\n 19  concave points_se        569 non-null    float64\n 20  symmetry_se              569 non-null    float64\n 21  fractal_dimension_se     569 non-null    float64\n 22  radius_worst             569 non-null    float64\n 23  texture_worst            569 non-null    float64\n 24  perimeter_worst          569 non-null    float64\n 25  area_worst               569 non-null    float64\n 26  smoothness_worst         569 non-null    float64\n 27  compactness_worst        569 non-null    float64\n 28  concavity_worst          569 non-null    float64\n 29  concave points_worst     569 non-null    float64\n 30  symmetry_worst           569 non-null    float64\n 31  fractal_dimension_worst  569 non-null    float64\n 32  Unnamed: 32              0 non-null      float64\ndtypes: float64(31), int64(1), object(1)\nmemory usage: 146.8+ KB\n\n\n\n\nFrom the info, we can see that the column types make sense and most of the columns have no missing values.\nWe do have this extra column called “Unnamed: 32” with 0 non-null values… so let’s drop it (remove it from the dataframe). We can also replace spaces in column names with “_“, which will be useful later.\n\ndata.drop(columns=\"Unnamed: 32\", inplace=______)\ndata.columns = data.columns.str.replace(\"?\", \"?\")\n# Check that the column was removed and column names were changed.\nprint(data.info())\n\n\n\n\nClick to reveal answers\n\n\n## `inplace` means that we modify the original dataframe\ndata.drop(columns=\"Unnamed: 32\", inplace=True)\ndata.columns = data.columns.str.replace(\" \", \"_\")\n## check that the column was removed\nprint(data.info())\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 32 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   id                       569 non-null    int64  \n 1   diagnosis                569 non-null    object \n 2   radius_mean              569 non-null    float64\n 3   texture_mean             569 non-null    float64\n 4   perimeter_mean           569 non-null    float64\n 5   area_mean                569 non-null    float64\n 6   smoothness_mean          569 non-null    float64\n 7   compactness_mean         569 non-null    float64\n 8   concavity_mean           569 non-null    float64\n 9   concave_points_mean      569 non-null    float64\n 10  symmetry_mean            569 non-null    float64\n 11  fractal_dimension_mean   569 non-null    float64\n 12  radius_se                569 non-null    float64\n 13  texture_se               569 non-null    float64\n 14  perimeter_se             569 non-null    float64\n 15  area_se                  569 non-null    float64\n 16  smoothness_se            569 non-null    float64\n 17  compactness_se           569 non-null    float64\n 18  concavity_se             569 non-null    float64\n 19  concave_points_se        569 non-null    float64\n 20  symmetry_se              569 non-null    float64\n 21  fractal_dimension_se     569 non-null    float64\n 22  radius_worst             569 non-null    float64\n 23  texture_worst            569 non-null    float64\n 24  perimeter_worst          569 non-null    float64\n 25  area_worst               569 non-null    float64\n 26  smoothness_worst         569 non-null    float64\n 27  compactness_worst        569 non-null    float64\n 28  concavity_worst          569 non-null    float64\n 29  concave_points_worst     569 non-null    float64\n 30  symmetry_worst           569 non-null    float64\n 31  fractal_dimension_worst  569 non-null    float64\ndtypes: float64(30), int64(1), object(1)\nmemory usage: 142.4+ KB\nNone\n\n\n\n\nThe column was successfully removed!\nNow, we can use .head(5) to show the first 5 rows of the dataset (rows 0-4). Remember that the first row is “0” not “1”!\n\ndata.head(5)\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave_points_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave_points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n842302\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n842517\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n84300903\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n84348301\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n84358402\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 32 columns\n\n\n\n\n\nRecoding a Variable\nFor our logistic regression, the diagnosis column, which is our outcome of interest, should be 0, 1 not B, M. To fix this, we can use a dictionary and .map().\nWe could also use a lambda function like we did in Session 3, but dictionaries can be more convenient if there are more than 2 values to be recoded.\n\n## define a dictionary\ny_recode = {\"B\": ___, \"M\": ___}\n\n## use .map to locate the keys in the column and replace with values\ndata[\"diagnosis\"] = data[\"diagnosis\"].map(________)\n\ndata.head(5)\n\n\n\n\nClick to reveal answers\n\n\n## define a dictionary\ny_recode = {\"B\": 0, \"M\": 1}\n\n## use .map() to locate the keys in the column and replace with values\n## B becomes 0, M becomes 1\ndata[\"diagnosis\"] = data[\"diagnosis\"].map(y_recode)\n\ndata.head(5)\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave_points_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave_points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n842302\n1\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n842517\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n84300903\n1\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n84348301\n1\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n84358402\n1\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 32 columns"
  },
  {
    "objectID": "session3/session3.html#step-3-exploratory-data-analysis",
    "href": "session3/session3.html#step-3-exploratory-data-analysis",
    "title": "Python Logistic Regression Demo",
    "section": "Step 3: Exploratory Data Analysis",
    "text": "Step 3: Exploratory Data Analysis\nNow that our data is cleaned and we have our outcome in numeric form, we can use .describe() to get summary statistics for each column of the dataset.\n\n___.___()\n\n\n\n\nClick to reveal answers\n\n\ndata.describe()\n\n\n\n\n\n\n\n\nid\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave_points_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave_points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\ncount\n5.690000e+02\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n...\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n569.000000\n\n\nmean\n3.037183e+07\n0.372583\n14.127292\n19.289649\n91.969033\n654.889104\n0.096360\n0.104341\n0.088799\n0.048919\n...\n16.269190\n25.677223\n107.261213\n880.583128\n0.132369\n0.254265\n0.272188\n0.114606\n0.290076\n0.083946\n\n\nstd\n1.250206e+08\n0.483918\n3.524049\n4.301036\n24.298981\n351.914129\n0.014064\n0.052813\n0.079720\n0.038803\n...\n4.833242\n6.146258\n33.602542\n569.356993\n0.022832\n0.157336\n0.208624\n0.065732\n0.061867\n0.018061\n\n\nmin\n8.670000e+03\n0.000000\n6.981000\n9.710000\n43.790000\n143.500000\n0.052630\n0.019380\n0.000000\n0.000000\n...\n7.930000\n12.020000\n50.410000\n185.200000\n0.071170\n0.027290\n0.000000\n0.000000\n0.156500\n0.055040\n\n\n25%\n8.692180e+05\n0.000000\n11.700000\n16.170000\n75.170000\n420.300000\n0.086370\n0.064920\n0.029560\n0.020310\n...\n13.010000\n21.080000\n84.110000\n515.300000\n0.116600\n0.147200\n0.114500\n0.064930\n0.250400\n0.071460\n\n\n50%\n9.060240e+05\n0.000000\n13.370000\n18.840000\n86.240000\n551.100000\n0.095870\n0.092630\n0.061540\n0.033500\n...\n14.970000\n25.410000\n97.660000\n686.500000\n0.131300\n0.211900\n0.226700\n0.099930\n0.282200\n0.080040\n\n\n75%\n8.813129e+06\n1.000000\n15.780000\n21.800000\n104.100000\n782.700000\n0.105300\n0.130400\n0.130700\n0.074000\n...\n18.790000\n29.720000\n125.400000\n1084.000000\n0.146000\n0.339100\n0.382900\n0.161400\n0.317900\n0.092080\n\n\nmax\n9.113205e+08\n1.000000\n28.110000\n39.280000\n188.500000\n2501.000000\n0.163400\n0.345400\n0.426800\n0.201200\n...\n36.040000\n49.540000\n251.200000\n4254.000000\n0.222600\n1.058000\n1.252000\n0.291000\n0.663800\n0.207500\n\n\n\n\n8 rows × 32 columns\n\n\n\n\n\nThe count column tells us the number of non-null (non-missing) values in a column.\n\n\nCreating Descriptive Plots\nCreating plots in python is similar to using ggplot in R, but there are some syntactic differences. The two most popular plotting packages in python are matplotlib and seaborn.\nMatplotlib is a low-level plotting package, and seaborn is built on top of it. Therefore, you can use many matplotlib methods with seaborn plot objects.\n\nBuilding a plot\nWhen building a plot in python, you start with a ‘figure’ object and an ‘axis’ object.\nData is plotted onto ‘axis’ objects. Axis objects sit on top of figure objects, which can be saved to variables and displayed later.\nThings like titles and legends are also associated with the ‘axis’ object, not the ‘figure’ object.\nYou can create figures using plt.figure(), with additional arguments for things like figure size. Then, you can add an axis object to the figure using fig.add_subplot().\n\nfig = plt.figure()\nax = fig.add_subplot()\n\n\n\nExample: Building a count plot of diagnoses using seaborn\nWe can look at the number of each diagnosis reflected in the dataset in a plot using seaborn.\nTo do this, we can first construct our ‘figure’ and ‘axis’ objects.\n\nfig = ___\nax = ___\n\n\n\n\nClick to reveal answers\n\n\nfig = plt.figure()\nax = fig.add_subplot()\n\n\n\n\n\n\n\n\n\n\nThen, we can create our ‘count plot’, which is similar to a barplot in ggplot, and assign it to the ‘axis’ object we just made.\nWe can also set the title of the axis object using the .set_title() method.\nIf we want to display the plot, we have to use display() if we are working with a quarto document or a jupyter notebook. If we are working with a regular python script, we use fig.show().\n\nsns.countplot(x=\"_________\", hue=\"_________\", data=______, ax=__)\nax.set_title(\"Distribution of Diagnoses\")\n\n___(___)\n\n\n\n\nClick to reveal answers\n\n\nsns.countplot(x=\"diagnosis\", hue=\"diagnosis\", data=data, ax=ax)\nax.set_title(\"Distribution of Diagnoses\")\n\ndisplay(fig)\n\n\n\n\n\n\n\n\n\n\n\n\nChanging plot attributes\nIf we want, we can change the colors of the plot. To make the plot a bit more useful, we can also change the y-scale from “count” to “percentage” and add labels so it is clear what “0” and “1” mean.\nTo help us pick colors, we can use sns.color_palette() which will display an image with the colors in the palette.\n\nsns.color_palette(\"colorblind\")\n\n\n\n\nTo change the colors of our plot, we can make a dictionary with the values of ‘diagnosis’ as keys and the hexcodes of the colors we want to use as values.\nWe can get the hex codes of colors from a seaborn palette using sns.color_palette().as_hex().\n\ncolor_hex = sns.color_palette(\"colorblind\")._____\n\nprint(\"The hexcodes for the 'colorblind' palette are:\\n\", ____)\n\n## if we want to make the columns green for benign and yellow for malignant\n\n## the \"-\" lets us index from the end of the list rather than the front. However, the '-1'th position is the last position (there is no '-0')\n\ncolors = {0: color_hex[__], 1: color_hex[__]}\n\n\n\n\nClick to reveal answers\n\n\ncolor_hex = sns.color_palette(\"colorblind\").as_hex()\n\nprint(\"The hexcodes for the 'colorblind' palette are:\\n\", color_hex)\n\n## if we want to make the columns green for benign and yellow for malignant\n\n## the \"-\" lets us index from the end of the list rather than the front.However, the '-1'th position is the last position (there is no '-0')\n\ncolors = {0: color_hex[2], 1: color_hex[-2]}\n\nThe hexcodes for the 'colorblind' palette are:\n ['#0173b2', '#de8f05', '#029e73', '#d55e00', '#cc78bc', '#ca9161', '#fbafe4', '#949494', '#ece133', '#56b4e9']\n\n\n\n\nWe then create the plot and tell seaborn to use ‘colors’ as the palette for the graph. We can also change the ‘stat’ to be “percent”, which can be more interpretable than raw counts.\nWe can also change the xtick labels to be “Benign” and “Malignant” instead of “0” and “1”.\nWe will also change the axis labels and set a title. Once we make these changes, we can show the finished plot.\n\nfig2 = plt.figure()\nax2 = fig2.add_subplot()\n\nsns.countplot(\n    x=\"___\", hue=\"___\", stat=\"___\", data=data, palette=colors, legend=False, ax=ax2\n)\n\n## change the xticklabels to benign and malignant\nax2.set_xticks([0, 1])\nax2.set_xticklabels([\"___\", \"\"])\n\n## change the axes labels and title\nax2.set(xlabel=\"___\", ylabel=\"___\", title=\"Distribution of Diagnoses\")\n\n## add legend\nax2.legend(title=\"Diagnosis\", loc=\"upper right\", labels=[\"Benign\", \"Malignant\"])\n\ndisplay(fig2)\n\n\n\n\nClick to reveal answers\n\n\nfig2 = plt.figure()\nax2 = fig2.add_subplot()\n\nsns.countplot(\n    x=\"diagnosis\",\n    hue=\"diagnosis\",\n    stat=\"percent\",\n    data=data,\n    palette=colors,\n    legend=False,\n    ax=ax2,\n)\n\n## change the xticklabels to benign and malignant\nax2.set_xticks([0, 1])\nax2.set_xticklabels([\"Benign\", \"Malignant\"])\n\n## change the axes labels and title\nax2.set(xlabel=\"Diagnosis\", ylabel=\"Percent\", title=\"Distribution of Diagnoses\")\n\n## add legend\nax2.legend(title=\"Diagnosis\", loc=\"upper right\", labels=[\"Benign\", \"Malignant\"])\n\n## show plot\ndisplay(fig2)\nplt.close(fig2)\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation Heatmap\nIf we wanted to, we could also make a correlation heatmap of our features using .corr() and sns.heatmap().\nFor this, all of our columns must be numeric, and we should remove the ‘id’ column as it is not useful for correlation. We use .select_dtypes() to select only the numeric columns from the dataset.\n\n## set figure size\nfig3 = plt.figure(figsize=(20, 20))\nax3 = fig3.add_subplot()\nnumeric_data = data.select_dtypes(include=___)\n\n## drop id column\nnumeric_data.drop(columns=___, inplace=___)\n\n\n## use corr function and seaborn heatmap to create correlation heatmap\n## 'fmt' allows us to choose the number display format for the heatmap\n\nsns.heatmap(numeric_data.___, annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax3)\n\n## set plot title and show plot\nax3.set_title(\"Feature Correlation Heatmap\")\n\ndisplay(fig3)\n\n\n\n\nClick to reveal answers\n\n\nfig3 = plt.figure(figsize=(20, 20))\nax3 = fig3.add_subplot()\n\nnumeric_data = data.select_dtypes(include=[np.number])\n\n## drop id column\nnumeric_data.drop(columns=\"id\", inplace=True)\n\n\n## use corr function and seaborn heatmap to create correlation heatmap\n## 'fmt' allows us to choose the number display format for the heatmap\n\nsns.heatmap(numeric_data.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax3)\n\n## set plot title and show plot\nax3.set_title(\"Feature Correlation Heatmap\")\n\ndisplay(fig3)\nplt.close(fig3)"
  },
  {
    "objectID": "session3/session3.html#step-4-creating-a-logistic-regression-model",
    "href": "session3/session3.html#step-4-creating-a-logistic-regression-model",
    "title": "Python Logistic Regression Demo",
    "section": "Step 4: Creating a Logistic Regression Model",
    "text": "Step 4: Creating a Logistic Regression Model\nHere we will explore two methods for creating a logistic regression model. The first, statsmodels, is more similar to R and is more user-friendly for statistical purposes. The second, scikit-learn, is more useful for machine learning and prediction models, but is a framework that is worth learning if you are going to use python often.\n\nMethod 1: Statsmodels\nThe statsmodels package is a python package for creating statistical models, conducting tests and performing data exploration. It is similar to packages used in R and creates an r-like model summary.\nIf we wanted to see if higher values of area_mean and texture_mean are associated with increased odds of malignancy, we can use smf.logit() to fit a logistic regression model.\n\nlogit = smf.logit(\"___ ~ ___ + ___\", data=data).fit()\n\nprint(logit.summary())\n\n\n\n\nClick to reveal answers\n\n\nlogit = smf.logit(\"diagnosis ~ area_mean + texture_mean\", data=data).fit()\n\nprint(logit.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.253932\n         Iterations 8\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:              diagnosis   No. Observations:                  569\nModel:                          Logit   Df Residuals:                      566\nMethod:                           MLE   Df Model:                            2\nDate:                Fri, 04 Apr 2025   Pseudo R-squ.:                  0.6154\nTime:                        17:22:26   Log-Likelihood:                -144.49\nconverged:                       True   LL-Null:                       -375.72\nCovariance Type:            nonrobust   LLR p-value:                3.776e-101\n================================================================================\n                   coef    std err          z      P&gt;|z|      [0.025      0.975]\n--------------------------------------------------------------------------------\nIntercept      -12.2437      1.151    -10.634      0.000     -14.500      -9.987\narea_mean        0.0120      0.001     10.172      0.000       0.010       0.014\ntexture_mean     0.2115      0.037      5.745      0.000       0.139       0.284\n================================================================================\n\n\nFrom the summary, we can see that the area_mean and texture_mean are both associated with increased odds of malignancy.\n\n\n\n\n\nAside: We can also use feature selection tools from the scikit-learn package to select what features to use.\n\nScikit learn requires the outcome and predictor variables to be split into two data frames.\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nX_raw = data.loc[:, \"radius_mean\"::]\n## set only the diagnosis column as \"y\"\ny = data.loc[:, \"diagnosis\"]\n\n# Select top k features based on ANOVA F-value between feature and target\nselector = SelectKBest(f_classif, k=5)  # Choose 'k' to specify number of features\nX_selected = selector.fit_transform(X_raw, y)\nselected_feature_names = X_raw.columns[selector.get_support()]\n\n## make model eqn\nformula = \"diagnosis ~\" + \"+\".join(selected_feature_names)\nsm_model = smf.logit(formula, data=data).fit()\n\nprint(sm_model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.109447\n         Iterations 10\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:              diagnosis   No. Observations:                  569\nModel:                          Logit   Df Residuals:                      563\nMethod:                           MLE   Df Model:                            5\nDate:                Fri, 04 Apr 2025   Pseudo R-squ.:                  0.8343\nTime:                        17:22:26   Log-Likelihood:                -62.275\nconverged:                       True   LL-Null:                       -375.72\nCovariance Type:            nonrobust   LLR p-value:                3.129e-133\n========================================================================================\n                           coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nIntercept              -18.7401      2.741     -6.836      0.000     -24.113     -13.367\nperimeter_mean          -0.2537      0.073     -3.465      0.001      -0.397      -0.110\nconcave_points_mean     66.7337     22.129      3.016      0.003      23.362     110.105\nradius_worst             1.8164      0.545      3.336      0.001       0.749       2.884\nperimeter_worst          0.0651      0.081      0.802      0.422      -0.094       0.224\nconcave_points_worst    17.8708     11.007      1.624      0.104      -3.703      39.445\n========================================================================================\n\nPossibly complete quasi-separation: A fraction 0.21 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified.\n\n\n\n\n\n\nMethod 2: Scikit-learn\nThe scikit-learn package is geared towards machine-learning and prediction-related tasks like classification, clustering and dimensionality reduction.\nFitting models with scikit-learn is a bit more complex than with statsmodels but is more along the lines of what most python projects will require.\nInstead of fitting a logistic regression model on the full dataset like we did with statsmodels, this time we are going to fit on a subset of our data and create a prediction model. We will test this prediction model on the remainder of the dataset.\n\n\nSplitting Training and Test Data\nTo fit a prediction model with sci-kit learn…\nWe first need to split the dataset into X (predictors/features) and y (outcomes). Then we use the train_test_split() function to split these datasets into a training dataset and a test dataset.\nWe use the .loc function and “:” to select all rows and any columns including and after “radius_mean”, and we assign these columns to x. This excludes the “diagnosis” and “id” columns.\nWe set y as simply the diagnosis column.\nWhen splitting our dataset, we can define ‘test_size’ which is the proportion of the data that will be set aside for testing the model. We can also set a random_state.\n\nUnlike R, Python allows for multi-argument returns from functions. This lets us assign each returned object to a different variable to be used later!\n\n\nX = data.loc[:, \"___\"::]\n\n## set only the diagnosis column as \"y\"\ny = data.loc[:, \"___\"]\n\n## here we assign each object returned from `train_test_split` to a different variable\n## we can use test_size to set the proportion of the dataset reserved for testing\nX_?, X_?, y_?, y_? = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nX_train.head(3)\n\n\n\n\nClick to reveal answers\n\n\nX = data.loc[:, \"radius_mean\"::]\n\n## set only the diagnosis column as \"y\"\ny = data.loc[:, \"diagnosis\"]\n\n## here we assign each object returned from `train_test_split` to a different variable\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nX_train.head(3)\n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave_points_mean\nsymmetry_mean\nfractal_dimension_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave_points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n68\n9.029\n17.33\n58.79\n250.5\n0.10660\n0.14130\n0.31300\n0.04375\n0.2111\n0.08046\n...\n10.31\n22.65\n65.50\n324.7\n0.14820\n0.4365\n1.2520\n0.17500\n0.4228\n0.1175\n\n\n181\n21.090\n26.57\n142.70\n1311.0\n0.11410\n0.28320\n0.24870\n0.14960\n0.2395\n0.07398\n...\n26.68\n33.48\n176.50\n2089.0\n0.14910\n0.7584\n0.6780\n0.29030\n0.4098\n0.1284\n\n\n63\n9.173\n13.86\n59.20\n260.9\n0.07721\n0.08751\n0.05988\n0.02180\n0.2341\n0.06963\n...\n10.01\n19.23\n65.59\n310.1\n0.09836\n0.1678\n0.1397\n0.05087\n0.3282\n0.0849\n\n\n\n\n3 rows × 30 columns\n\n\n\n\n\n\n\nScaling/Normalizing Data\nBecause all of our features have different scales, we need to standardize (normalize) our dataset. We can do this by creating an instance of the StandardScaler class called “scaler” and fitting that to the training data. We then use the same “scaler” to scale the test dataset.\n\n## standardize dataset\nscaler = ___()\n\n## fit the scaler to the _ data\nscaler.fit(___)\n\n## apply the scaler to the _ data and _ data\nX_train = scaler.transform(___)\nX_test = scaler.transform(___)\n\n\n\n\nClick to reveal answers\n\n\n## standardize dataset\nscaler = StandardScaler()\n\n## fit the scaler to the TRAINING data\nscaler.fit(X_train)\n\n## apply the scaler to BOTH the training and test data\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n\n\n\n\nAfter scaling the data, we can perform dimensional reduction with PCA\nPCA is often used for dimensional reduction with machine learning methods so we will demonstrate it here. We can set up the PCA transformer in the same way that we set the scaler above.\n\n## set up PCA transformer with the number of components you want and fit to training dataset\npca = PCA(n_components=__)\npca = pca.fit(___)\n\n## apply PCA transformer to training and test set\nX_train_pca = pca.transform(___)\nX_test_pca = pca.transform(___)\n\n\n\n\nClick to reveal answers\n\n\n## set up PCA transformer with the number of components you want and fit to training dataset\npca = PCA(n_components=10)\npca = pca.fit(X_train)\n\n## apply PCA transformer to training and test set\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\n\n\n\n\nWe can get an idea of how well our PCA factors represent our data\nTo do this, we can make a plot of the cumulative explained variance.\nIf we just want to make a quick plot that we do not plan on displaying multiple times, we can skip explicitly setting figure and axis objects.\nHere we use plt.plot() from matplotlib to create a plot of the cumulative explained variance. We can use plt.xlabel() and plt.ylabel() in the same code chunk to set the labels for this plot.\nIf we try to set the labels in a later chunk, we will get a blank plot.\n\n## we can look at the cumulative explained variance\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Cumulative Explained Variance\")\n\nText(0, 0.5, 'Cumulative Explained Variance')"
  },
  {
    "objectID": "session3/session3.html#step-5-model-setup",
    "href": "session3/session3.html#step-5-model-setup",
    "title": "Python Logistic Regression Demo",
    "section": "Step 5: Model Setup",
    "text": "Step 5: Model Setup\nNext we have to set up the model itself by creating an instance of the LogisticRegression model class.\n\nlr = ___\n\n\n\n\nClick to reveal answers\n\n\nlr = LogisticRegression()\n\n\n\nThen, we can fit this model to the training data.\n\n## fit to training data\nlr.___(X_train_pca, y_train)\n\n\n\n\nClick to reveal answers\n\n\n## fit to training data\nlr.fit(X_train_pca, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression()"
  },
  {
    "objectID": "session3/session3.html#step-6-look-at-results",
    "href": "session3/session3.html#step-6-look-at-results",
    "title": "Python Logistic Regression Demo",
    "section": "Step 6: Look At Results",
    "text": "Step 6: Look At Results\nOnce the model is fit, we can use it to predict the outcome (diagnosis) based on the features of the test data.\n\nStore Results in a Dataframe\nWe can use pd.DataFrame() to create an empty pandas dataframe that we can fill with our results.\n\n## use model to predict test data\n## set up dataframe to review results\nresults = pd.___\n\n## get predicted\nresults.loc[:, \"Predicted\"] = lr.___(___)\n\n## get true y values for test dataset\nresults.loc[:, \"Truth\"] = ___.___\n\n## get probability of being malignant\n## the output is one probability per outcome, we only want the second outcome (malignant)\nresults.loc[:, \"Probability: Malignant\"] = pd.DataFrame(lr.___(X_test_pca))[_]\n\nresults.head(5)\n\n\n\n\nClick to reveal answers\n\n\n## use model to predict test data\n## set up dataframe to review results\nresults = pd.DataFrame()\n\n## get predicted\nresults.loc[:, \"Predicted\"] = lr.predict(X_test_pca)\n\n## get true y values for test dataset\nresults.loc[:, \"Truth\"] = y_test.values\n\n## get probability of being malignant\n## the output is one probability per outcome, we only want the second outcome (malignant). The second outcome uses index 1\nresults.loc[:, \"Probability: Malignant\"] = pd.DataFrame(lr.predict_proba(X_test_pca))[1]\n\nresults.head(5)\n\n\n\n\n\n\n\n\nPredicted\nTruth\nProbability: Malignant\n\n\n\n\n0\n0\n0\n0.098966\n\n\n1\n1\n1\n0.999987\n\n\n2\n1\n1\n0.997340\n\n\n3\n0\n0\n0.000974\n\n\n4\n0\n0\n0.000056\n\n\n\n\n\n\n\n\n\nWe can also get a quantitative “accuracy score” that will give us an idea of how well our model predicts our outcomes.\n\naccuracy = accuracy_score(results[\"Truth\"], results[\"Predicted\"])\n\nprint(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n\nAccuracy: 98.25%\n\n\n\n\nCreate ROC curve\nAs a figure, we can create an ROC curve and use quarto chunk options to add a figure caption.\nLike we did for the cumulative variance plot, this time we will skip setting up named figure and axis objects. Instead, we will first create a ‘working figure’ of size 8x6 and add plots on top of that. Any ‘plt.plot()’ instances we create in this chunk will be overlayed on the working figure object.\nIf we were working in a python script rather than a quarto document, we would need to use plt.show() at the end to display the figure.\n\n## make a plot to vizualize the ROC curve\n\n## get false pos rate, true pos rate and thresholds\n## there are 3 outputs so we need 3 variables to catch them\n___, ___, ___ = roc_curve(results[\"Truth\"], results[\"Predicted\"])\n\n## get AUC data\nroc_auc = auc(___, ___)\n\n## set up plot\nplt.figure(figsize=(8, 6))\n\n## using matplotlib this time, create line plot with 2pt line weight\n## add \"ROC Curve (AUC = AUC)\" as label for orange line\n## .2f is for display formatting, lw is linewidth\nplt.plot(__, __, color=\"darkorange\", lw=2, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n\n## create another curve, this time blue with a dashed line labeled \"Random\"\n## as in random chance.\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\", label=\"Random\")\n\n## add xlabel, ylabel and title\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\n    \"Receiver Operating Characteristic (ROC) Curve\\nAccuracy: {:.2f}%\".format(\n        accuracy * 100\n    )\n)\n\n## add legend and show plot\nplt.legend(loc=\"lower right\")\n\n\n\n\nClick to reveal answers\n\n\n## make a plot to vizualize the ROC curve\n\n## get false pos rate, true pos rate and thresholds\nfpr, tpr, thresholds = roc_curve(results[\"Truth\"], results[\"Predicted\"])\n\n## get AUC data\nroc_auc = auc(fpr, tpr)\n\n## set up plot\nplt.figure(figsize=(8, 6))\n\n## using matplotlib this time, create line plot with 2pt line weight\n## add \"ROC Curve (AUC = AUC)\" as label for orange line\n## .2f is for display formatting, lw is linewidth\nplt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n\n## create another curve, this time blue with a dashed line labeled \"Random\"\n## as in random chance\nplt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\", label=\"Random\")\n\n## add xlabel, ylabel and title\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\n    \"Receiver Operating Characteristic (ROC) Curve\\nAccuracy: {:.2f}%\".format(\n        accuracy * 100\n    )\n)\n\n## add legend and show plot\nplt.legend(loc=\"lower right\")\n\n\n\n\nAn ROC curve for our logistic regression model\n\n\n\n\n\n\nCongratulations! You have successfully done logistic regression in Python!\n\n\n\nCreate a Statsmodels-like model and summary with scikit-learn and statsmodels\n\nIt is also possible to fit a model with scikit-learn, extract the coefficients, and use them to create a statsmodels model and summary.\nTypically, you would want to pick which package (sklearn or statsmodels) you want to use and stick with it, but this is an option if necessary. Note: I am showing Lasso here as well because statsmodels will fail if there are highly correlated features like with this dataset, however this same method can be used on a scikit-learn logistic regression model without Lasso penalties.\nThis time, we are going to fit on the full data.\nFirst, we can select features to use for model (statsmodels does not perform regularization and therefore will fail to converge when there are highly correlated features). Scikit-learn gives us multiple ways to do this. Let’s use LASSO.\n\n## scale X\nX_raw = X  ## scaled dfs lose column names\nX = scaler.transform(X_raw)\n\n## set up model for Lasso and fit it\nmodel = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=0.01)\nmodel.fit(X, y)\n\n# Get non-zero coefficient features\nselected_features = X_raw.columns[model.coef_[0] != 0]\nX_selected = X_raw[selected_features]\nprint(X_selected.columns)\n\nIndex(['concave_points_mean', 'perimeter_worst', 'concave_points_worst'], dtype='object')\n\n\nFit statsmodels model and get summary\n\n## Get coefficients\nintercept = model.intercept_[0]\ncoefficients = model.coef_[0][model.coef_[0] != 0]\n\n## make model eqn\nformula = \"diagnosis ~\" + \"+\".join(X_selected.columns)\nsm_model2 = smf.logit(formula, data=data).fit()\n\nsm_model2.params[:] = np.concatenate(\n    ([intercept], coefficients)\n)  # Set params from scikit-learn model\n\n# Display the summary\nprint(sm_model2.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.129410\n         Iterations 10\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:              diagnosis   No. Observations:                  569\nModel:                          Logit   Df Residuals:                      565\nMethod:                           MLE   Df Model:                            3\nDate:                Fri, 04 Apr 2025   Pseudo R-squ.:                  -42.68\nTime:                        17:22:26   Log-Likelihood:                -16411.\nconverged:                       True   LL-Null:                       -375.72\nCovariance Type:            nonrobust   LLR p-value:                     1.000\n========================================================================================\n                           coef    std err          z      P&gt;|z|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nIntercept                     0      2.441          0      1.000      -4.785       4.785\nconcave_points_mean      0.0898     16.311      0.006      0.996     -31.878      32.058\nperimeter_worst          0.5279      0.021     25.471      0.000       0.487       0.569\nconcave_points_worst     0.4867      8.505      0.057      0.954     -16.183      17.157\n========================================================================================\n\nPossibly complete quasi-separation: A fraction 0.37 of observations can be\nperfectly predicted. This might indicate that there is complete\nquasi-separation. In this case some parameters will not be identified.\n\n\n\n\n\n\nCitations\n\nIcons\nCsv icons created by rizal2109 - Flaticon Ipynb icons created by JunGSa - Flaticon Coding icons created by juicy_fish - Flaticon"
  }
]