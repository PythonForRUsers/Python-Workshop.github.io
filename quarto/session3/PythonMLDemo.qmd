---
title: "Python Machine Learning Demo"
author: "Python Group"
format: 
    html:
        output-file: "session3.html"
        toc: true
        code-copy: true
        code-line-numbers: true
        link-external-icon: false
        link-external-newwindow: true
---

## Links

<a href="https://www.kaggle.com/datasets/erdemtaha/cancer-data/data" class="link-block"> <img src="../icons/csv.png" alt="Dataset"/>

<p>Cancer Dataset</p>

</a> <a href="https://github.mskcc.org/Python-Workshop/Python-Workshop.github.io/tree/main/FollowAlong" class="link-block"> <img src="../icons/code.png" alt="File"/>

<p>Download Follow Along File</p>

</a>

## Getting Started

Before doing anything else, we should first activate the conda environment we want to use.

<details>

<summary>Refresher: How to activate conda environment</summary>

::: {style="sp"}
:::

From terminal, type:

::: terminal
\> conda activate ENVNAME
:::

::: {style="margin: 10px 0;"}
:::

When in VS code, you might get a popup message like the one below, confirming that the environment was activated:

::: {style="border-left: 4px solid #007bff; background-color: #f8f9fa; padding: 5px; margin: 5px 0;"}
Selected conda environment was successfully activated, even though "(ENVNAME)" indicator may not be present in the terminal prompt.
:::

</details>

If we want to make sure we have the packages we'll need installed in the environment before we try to import them, we can either check on anaconda or use the terminal:

::: terminal
\> conda list
:::

::: {style="sp"}
:::

Otherwise, we will get an error message if we try to import packages that are not installed.

<details>

<summary>Refresher: How to install packages</summary>

To install packages, we can either use the "anaconda" dashboard, or we can use the command line. Make sure your environment is active before installing packages or the packages will not be available in your environment.

To install from the command line, we open a terminal and type:

::: terminal
\> conda install {package}
:::

or

::: terminal
\> pip install {package}
:::

If a package is not available via conda it might be available via pip.

</details>

## Step 1: Import Packages

Similar to `library()` in R, weâ€™ll use `import` in Python. Fill in the blanks to import the necessary packages:

```{python}
# | eval: false
import ___ as pd
import ___ as np
import ___ as sns
import ___ as plt

# Import from sklearn
from sklearn.model_selection import __________
from sklearn.preprocessing import __________

from sklearn.linear_model import __________

from sklearn.metrics import accuracy_score, roc_curve, auc
```

::: ans
<details>

<summary>Click to reveal answers</summary>

```{python}
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

## import from sklearn (scikit-learn)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, roc_curve, auc
```

</details>
:::

## Step 2: Read in Data and Perform Data Cleaning

We can use the `read_csv()` function from the pandas package to read in the dataset.

```{python}
#| eval: false
data = pd.read_csv("__________")
```

::: ans
<details>

<summary>Click to reveal answers</summary>

```{python}
data = pd.read_csv("example_data/Cancer_Data.csv")
```

</details>
:::

We can use the `.info()` function to show some basic information about the dataset like:\
\* the number of rows\
\* number of columns\
\* column labels\
\* column type\
\* number of non-null values in each column

```{python}
#| eval: false
data._______()
```

::: ans
<details>

<summary>Click to reveal answers</summary>

```{python}
data.info()
```

</details>
:::

From the *info*, we can see that the column types make sense and most of the columns have no missing values.

We do have this extra column called "Unnamed: 32" with 0 non-null values... so let's drop it (remove it from the dataframe).

```{python}
#| eval: false
data.drop(columns="Unnamed: 32", inplace=______)

# Check that the column was removed
print(data.info())
```

::: ans
<details>

<summary>Click to reveal answers</summary>

```{python}
## `inplace` means that we modify the original dataframe
data.drop(columns="Unnamed: 32", inplace=True)

## check that the column was removed
print(data.info())

```

</details>
:::

The column was successfully removed!

Now, we can use `.head(5)` to show the first 5 rows of the dataset (rows 0-4). Remember that the first row is "0" not "1"!

```{python}
data.head(5)
```

### Recoding a Variable

For our logistic regression, the diagnosis column, which is our outcome of interest, should be 0, 1 not B, M. To fix this, we can use a *dictionary* and `.map()`. We could also use a lambda function as described in session 2b, but dictionaries can be more convenient for replacing multiple values.

```{python}
#| eval: false
## define a dictionary
y_recode = {"B": ___, "M": ___}

## use .map to locate the keys in the column and replace with values
data["diagnosis"] = data["diagnosis"].map(________)

data.head(5)
```

::: ans
<details>

<summary>Click to reveal answers</summary>

```{python}
## define a dictionary
y_recode = {"B": 0, "M": 1}

## use .map() to locate the keys in the column and replace with values
## B becomes 0, M becomes 1
data["diagnosis"] = data["diagnosis"].map(y_recode)

data.head(5)
```

</details>
:::

## Step 3: Exploratory Data Analysis

Now that our data is cleaned and we have our outcome in numeric form, we can use `.describe()` to get summary statistics for each column of the dataset.

```{python}
#| eval: false

___.___()
```

::: ans
<details>

<summary>Click to reveal answers</summary>

```{python}
data.describe()
```

</details>
:::

The count column tells us the number of non-null (non-missing) values in a column.

### Creating Descriptive Plots

We can also look at the number of each diagnosis reflected in the dataset in a plot using seaborn.

You can also save a plot to a variable (ex: 'p') if you want to display it later with `plt.show(p)`.

```{python}
#| eval: false
sns.countplot(x="_________", hue="_________", data=______)
plt.title("Distribution of Diagnoses")
_____
```

::: ans
<details>

<summary>Click to reveal answers</summary>

```{python}
#| dpi: 600
sns.countplot(x="diagnosis", hue="diagnosis", data=data)
plt.title("Distribution of Diagnoses")
plt.show()
```

</details>
:::

<details>

<summary>Click to reveal answers</summary>

```{python}
# | message: False
color_hex = sns.color_palette("colorblind").as_hex()

print("The hexcodes for the 'colorblind' palette are:\n", color_hex)

## if we want to make the columns green for benign and yellow for malignant

## the "-" lets us index from the end of the list rather than the front.However, the '-1'th position is the last position (there is no '-0')

colors = {0: color_hex[2], 1: color_hex[-2]}
```

</details>

</div>

We then create the plot and tell seaborn to use 'colors' as the palette for the graph. We can also change the 'stat' to be "percent", which can be more interpretable than raw counts.

We can also change the xtick labels to be "Benign" and "Malignant" instead of "0" and "1". Because we assigned the plot to the variable 'p', we can use `p.{}` to change attributes of plot 'p'.

We will also change the axis labels and set a title. Once we make these changes, we can show the finished plot.

```{python}
#| eval: false
p = sns.countplot(
    x="___",
    hue="___",
    stat="___",
    data=data,
    palette=colors,
    legend=False,
)

## change the xticklabels to benign and malignant
p.set_xticks([0, 1])
p.set_xticklabels(["___", ""])

## change the axes labels and title
p.set(xlabel="___", ylabel="___", title="Distribution of Diagnoses")

## add legend
plt.legend(title="Diagnosis", loc="upper right", labels=["Benign", "Malignant"])

## show plot
plt.show(p)
```

::: ans
<details>

<summary>Click to reveal answers</summary>

```{python}
p = sns.countplot(
    x="diagnosis",
    hue="diagnosis",
    stat="percent",
    data=data,
    palette=colors,
    legend=False,
)

## change the xticklabels to benign and malignant
p.set_xticks([0, 1])
p.set_xticklabels(["Benign", "Malignant"])

## change the axes labels and title
p.set(xlabel="Diagnosis", ylabel="Percent", title="Distribution of Diagnoses")

## add legend
plt.legend(title="Diagnosis", loc="upper right", labels=["Benign", "Malignant"])

## show plot
plt.show(p)
```

</details>
:::

If we wanted to, we could also make a correlation heatmap of our features using `.corr()` and `sns.heatmap()`.

For this, all of our columns must be numeric, and we should remove the 'id' column as it is not useful for correlation. We use `.select_dtypes()` to select only the numeric columns from the dataset.

```{python}
# | eval: false
numeric_data = data.select_dtypes(include=___)

## drop id column
numeric_data.drop(columns=___, inplace=___)

## set figure size
plt.figure(figsize=(20, 20))

## use corr function and seaborn heatmap to create correlation heatmap
## 'fmt' allows us to choose the number display format for the heatmap

sns.heatmap(numeric_data.___, annot=True, fmt=".2f", cmap="coolwarm")

## set plot title and show plot
plt.title("Feature Correlation Heatmap")

plt.___
```

::: ans
<details>

<summary>Click to reveal answers</summary>

```{python}
numeric_data = data.select_dtypes(include=[np.number])

## drop id column
numeric_data.drop(columns="id", inplace=True)

## set figure size
plt.figure(figsize=(20, 20))

## use corr function and seaborn heatmap to create correlation heatmap
## 'fmt' allows us to choose the number display format for the heatmap

sns.heatmap(numeric_data.corr(), annot=True, fmt=".2f", cmap="coolwarm")

## set plot title and show plot
plt.title("Feature Correlation Heatmap")

plt.show()
```

</details>
:::

## Step 4: Data Setup

### Splitting Training and Test Data

We first need to split the dataset into X (predictors/features) and y (outcomes). Then we use the `train_test_split()` function to split these datasets into a training dataset and a test dataset.

We use the .loc function and ":" to select all rows and any columns including and after "radius_mean", and we assign these columns to x. This excludes the "diagnosis" and "id" columns.

We set y as simply the diagnosis column.

When splitting our dataset, we can define 'test_size' which is the proportion of the data that will be set aside for testing the model. We can also set a random_state.

::: {style="border-left: 4px solid #007bff; background-color: #f8f9fa; padding: 5px; margin: 5px 0;"}
Unlike R, Python allows for multi-argument returns from functions. This lets us assign each returned object to a different variable to be used later!
:::

```{python}
#| eval: false
x = data.loc[:, "___"::]

## set only the diagnosis column as "y"
y = data.loc[:, "___"]

## here we assign each object returned from `train_test_split` to a different variable
## we can use test_size to set the proportion of the dataset reserved for testing
X_?, X_?, y_?, y_? = train_test_split(
    x, y, test_size=0.2, random_state=42
)

X_train.head(3)
```

::: ans
<details>

<summary>Click to reveal answers</summary>

```{python}
x = data.loc[:, "radius_mean"::]

## set only the diagnosis column as "y"
y = data.loc[:, "diagnosis"]

## here we assign each object returned from `train_test_split` to a different variable
X_train, X_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42
)

X_train.head(3)
```

</details>
:::

### Scaling/Normalizing Data

Because all of our features have different scales, we need to standardize (normalize) our dataset. We can do this by creating an instance of the `StandardScaler` class called "scaler" and fitting that to the training data. We then use the same "scaler" to scale the test dataset.

```{python}
#| eval: false
## standardize dataset
scaler = ___()

## fit the scaler to the _ data
scaler.fit(___)

## apply the scaler to the _ data and _ data
X_train = scaler.transform(___)
X_test = scaler.transform(___)
```

::: ans
<details>

<summary>Click to reveal answers</summary>

```{python}
## standardize dataset
scaler = StandardScaler()

## fit the scaler to the TRAINING data
scaler.fit(X_train)

## apply the scaler to BOTH the training and test data
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
```

</details>
:::

## Step 5: Model Setup

Next we have to set up the model itself by creating an instance of the `LogisticRegression` model class.

```{python}
#| eval: false
lr = ___
```

::: ans
<details>

<summary>Click to reveal answers</summary>

```{python}
lr = LogisticRegression()
```

</details>
:::

Then, we can fit this model to the training data.

```{python}
#| eval: false
## fit to training data
lr.___(X_train, y_train)
```

::: ans
<details>

<summary>Click to reveal answers</summary>

```{python}
## fit to training data
lr.fit(X_train, y_train)
```

</details>
:::

## Step 6: Look At Results

Once the model is fit, we can use it to predict the outcome (diagnosis) based on the features of the test data.

### Store Results in a Dataframe

We can use `pd.DataFrame()` to create an empty pandas dataframe that we can fill with our results.

```{python}
#| eval: false
## use model to predict test data
## set up dataframe to review results
results = pd.___

## get predicted
results.loc[:, 'Predicted']= lr.___(___)

## get true y values for test dataset
results.loc[:, 'Truth'] = ___.___

## get probability of being malignant
## the output is one probability per outcome, we only want the second outcome (malignant)
results.loc[:, 'Probability: Malignant'] = pd.DataFrame(lr.___(X_test))[_]

#results_recode = {0: "B", 1:"M"}
#results.replace({"Predicted": results_recode, 'Truth': results_recode}, inplace = True)

results.head(5)
```

::: ans
<details>

<summary>Click to reveal answers</summary>

```{python}
## use model to predict test data
## set up dataframe to review results
results = pd.DataFrame()

## get predicted
results.loc[:, 'Predicted']= lr.predict(X_test)

## get true y values for test dataset
results.loc[:, 'Truth'] = y_test.values

## get probability of being malignant
## the output is one probability per outcome, we only want the second outcome (malignant). The second outcome uses index 1
results.loc[:, 'Probability: Malignant'] = pd.DataFrame(lr.predict_proba(X_test))[1]

#results_recode = {0: "B", 1:"M"}
#results.replace({"Predicted": results_recode, 'Truth': results_recode}, inplace = True)

results.head(5)
```

</details>
:::

We can also get a quantitative "accuracy score" that will give us an idea of how well our model predicts our outcomes.

```{python}
accuracy = accuracy_score(results["Truth"], results["Predicted"])

print("Accuracy: {:.2f}%".format(accuracy * 100))
```

### Create ROC curve

As a figure, we can create an ROC curve and use quarto chunk options to add a figure caption.

```{python}
# | fig-cap: An ROC curve for our logistic regression model
# | eval: false

## make a plot to vizualize the ROC curve

## get false pos rate, true pos rate and thresholds
## there are 3 outputs so we need 3 variables to catch them
___, ___, ___ = roc_curve(results["Truth"], results["Predicted"])

## get AUC data
roc_auc = auc(___, ___)

## set up plot
plt.figure(figsize=(8, 6))

## using matplotlib this time, create line plot with 2pt line weight
## add "ROC Curve (AUC = AUC)" as label for orange line
## .2f is for display formatting, lw is linewidth
plt.plot(fpr, tpr, color="darkorange", lw=2, label=f"ROC Curve (AUC = {roc_auc:.2f})")

## create another curve, this time blue with a dashed line labeled "Random"
## as in random chance.
plt.plot(___, ___, color="navy", lw=2, linestyle="--", label="Random")

## add xlabel, ylabel and title
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title(
    "Receiver Operating Characteristic (ROC) Curve\nAccuracy: {:.2f}%".format(
        accuracy * 100
    )
)

## add legend and show plot
plt.legend(loc="lower right")
plt.show()

```

::: ans
<details>

<summary>Click to reveal answers</summary>

```{python}
# | fig-cap: An ROC curve for our logistic regression model

## make a plot to vizualize the ROC curve

## get false pos rate, true pos rate and thresholds
fpr, tpr, thresholds = roc_curve(results["Truth"], results["Predicted"])

## get AUC data
roc_auc = auc(fpr, tpr)

## set up plot
plt.figure(figsize=(8, 6))

## using matplotlib this time, create line plot with 2pt line weight
## add "ROC Curve (AUC = AUC)" as label for orange line
## .2f is for display formatting, lw is linewidth
plt.plot(fpr, tpr, color="darkorange", lw=2, label=f"ROC Curve (AUC = {roc_auc:.2f})")

## create another curve, this time blue with a dashed line labeled "Random"
## as in random chance
plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--", label="Random")

## add xlabel, ylabel and title
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title(
    "Receiver Operating Characteristic (ROC) Curve\nAccuracy: {:.2f}%".format(
        accuracy * 100
    )
)

## add legend and show plot
plt.legend(loc="lower right")
plt.show()

```

</details>
:::

Congratulations! You have successfully done logistic regression in Python!

<details>

<summary>Citations</summary>

Icons\
<a href="https://www.flaticon.com/free-icons/csv" title="csv icons">Csv icons created by rizal2109 - Flaticon</a> <a href="https://www.flaticon.com/free-icons/ipynb" title="ipynb icons">Ipynb icons created by JunGSa - Flaticon</a> <a href="https://www.flaticon.com/free-icons/coding" title="coding icons">Coding icons created by juicy_fish - Flaticon</a>

</details>