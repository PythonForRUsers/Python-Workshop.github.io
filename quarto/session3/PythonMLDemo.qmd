---
title: "Python Logistic Regression Demo"
author: "Python Group"
format: 
    html:
        output-file: "session3.html"
        toc: true
        code-copy: true
        code-line-numbers: true
        link-external-icon: false
        link-external-newwindow: true
---

## Links

<a href="https://www.kaggle.com/datasets/erdemtaha/cancer-data/data" class="link-block">
    <img src="../icons/csv.png" alt="Dataset">
    <p>Cancer Dataset</p>
</a> <a href="https://github.mskcc.org/Python-Workshop/Python-Workshop.github.io/tree/main/FollowAlong" class="link-block">
    <img src="../icons/code.png" alt="File">
    <p>Download Follow Along File</p>
</a>

# Welcome to session 4!

**In this session, we are going to walk through data cleaning, visualization and two different ways of building a logistic regression model.**

We are using the **same dataset** as in session 3! There is no need to re-download it!

Click on the 'Download Follow Along File' link above or visit the 'files' tab on the teams page to get the follow along file for this session. There is a jupyter notebook option and a quarto option. Please choose whichever you are comfortable with!

<hr style="border: none; border-top: 2px solid #007bff; width: 100%;">

## Getting Started

Before doing anything else, we should first activate the conda environment we want to use. If you created the 'python-intro-env' environment, please use that. 
<details>
<summary> Refresher: How to activate conda environment </summary>
<div style="sp"></div>
From terminal, type: 
  
<div class="terminal">
\> conda activate ENVNAME
</div>
  <div style="margin: 10px 0;"></div>
When in VS code, you might get a popup message like the one below, confirming that the environment was activated:  

<div style="border-left: 4px solid #007bff; background-color: #f8f9fa; padding: 5px; margin: 5px 0;">
Selected conda environment was successfully activated, even though "(ENVNAME)" indicator may not be present in the terminal prompt. 
</div>

or

In Anaconda Navagator, click on the **Environments** tab on the left and select the environment you want to activate. Just selecting the environment should activate it. 
</details>

<details>
<summary> Refresher: How to install packages </summary>

To install packages, we can either use the "anaconda" dashboard, or we can use the command line. Make sure your environment is active before installing packages or the packages will not be available in your environment. 

To install from the command line, we open a terminal and type: 

<div class="terminal">
\> conda install {package}
</div>

or

<div class="terminal">
\> pip install {package}
</div>

When working with conda environments, it's best practice to install everything with conda and only use pip for packages that are not available through conda!
</details>

<div style="border-left: 4px solid #007bff; background-color: #f8f9fa; padding: 5px; margin: 5px 0;">
If you are using the 'python-intro-env' environment, you may need to install the 'statsmodels' package if you have not already installed it. 
</div>

**Statsmodels can be installed via conda install so:**

<div style="background-color: #1e1e1e;
    border: 1px solid #f8f9fa;
    /* Optional border */
    color: #ffffff;
    padding: 5px;
    /* Top 10px, right 5px, bottom 5px, left 5px */
    font-family: monospace;
    border-radius: 5px;
    max-width: 800px;">
\> conda install statsmodels
</div>

### Checking installed packages
If we want to make sure we have the packages we'll need installed in the environment before we try to import them, we can either check on anaconda or use the terminal: 

<div style="background-color: #1e1e1e;
    border: 1px solid #f8f9fa;
    /* Optional border */
    color: #ffffff;
    padding: 5px;
    /* Top 10px, right 5px, bottom 5px, left 5px */
    font-family: monospace;
    border-radius: 5px;
    max-width: 800px;">
\> conda list
</div>
<div style="sp"></div>

Otherwise, we will get an error message if we try to import packages that are not installed. 

We can also check for a specific package, like pandas, with `conda list {package}. See example below:
<div style="background-color: #1e1e1e;
    border: 1px solid #f8f9fa;
    /* Optional border */
    color: #ffffff;
    padding: 5px;
    /* Top 10px, right 5px, bottom 5px, left 5px */
    font-family: monospace;
    border-radius: 5px;
    max-width: 800px;">
\> conda list pandas  
<br>
# packages in environment at C:\\...\\anaconda3\\envs\\python-intro-env: 
# Name                    Version                   Build  Channel
<br>
pandas                    2.2.2           py310h5da7b33_0
</div>

<hr style="border: none; border-top: 2px solid #007bff; width: 100%;margin-top: 20px; margin-bottom: 20px;">

# Now we are ready to get started!

Our goals for this session are:  
1. Practice the **pandas** skills we covered last session  
2. Get familiar with plotting in python using **seaborn** and **matplotlib**  
3. Understand how to create and evaluate models using **Statsmodels** and **scikit-learn**  

## Step 1: Import Packages

Similar to `library()` in R, weâ€™ll use `import` in Python. Unlike R, however, python lets you set what 'nickname' you want to use for each package. There are some standard conventions for these import statements (like pandas typically being imported as pd) and following them helps make your code more readable.

Fill in the blanks to import the necessary packages:

```{python}
# | eval: false
import pandas as ___
import numpy as ___
import seaborn as ___
import matplotlib.pyplot as ___

import statsmodels.api as __
import statsmodels.formula.api as ___

# Import from sklearn
from sklearn.model_selection import ___
from sklearn.preprocessing import ____
from sklearn.decomposition import ___


from sklearn.linear_model import ___

from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, roc_curve, auc
from sklearn.feature_selection import SelectKBest, f_classif

# Import display from IPython to allow display of plots in notebook
from IPython.display import display
```
The answers can be found under the drop down below.

<div class = "ans"> 
<details><summary> Click to reveal answers </summary>
```{python}
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

import statsmodels.api as sm
import statsmodels.formula.api as smf

## import from sklearn (scikit-learn)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

from sklearn.metrics import accuracy_score, roc_curve, auc
from sklearn.feature_selection import SelectKBest, f_classif

# Import display from IPython to allow display of plots in notebook
from IPython.display import display
```
</details>
</div>

<hr style="border: none; border-top: 2px solid #007bff; width: 100%;margin-top: 20px; margin-bottom: 20px;">

## Step 2: Read in Data and Perform Data Cleaning

We can use the `read_csv()` function from the pandas package to read in the dataset. 

```{python}
#| eval: false
data = pd.read_csv("__________")
```

<div class = "ans"> 
<details><summary> Click to reveal answers </summary>

```{python}
data = pd.read_csv("example_data/Cancer_Data.csv")
```
</details>
</div>

We can use the `.info()` function to show some basic information about the dataset like:  
* the number of rows  
* number of columns  
* column labels  
* column type  
* number of non-null values in each column

```{python}
#| eval: false
data._______()
```

<div class = "ans"> 
<details><summary> Click to reveal answers </summary>
```{python}
data.info()
```
</details>
</div>
From the *info*, we can see that the column types make sense and most of the columns have no missing values. 

We do have this extra column called "Unnamed: 32" with 0 non-null values...
so let's drop it (remove it from the dataframe). We can also replace spaces in column names with "_", which will be useful later. 

```{python}
# | eval: false
data.drop(columns="Unnamed: 32", inplace=______)
data.columns = data.columns.str.replace("?", "?")
# Check that the column was removed and column names were changed.
print(data.info())
```

<div class = "ans"> 
<details><summary> Click to reveal answers </summary>
```{python}
## `inplace` means that we modify the original dataframe
data.drop(columns="Unnamed: 32", inplace=True)
data.columns = data.columns.str.replace(" ", "_")
## check that the column was removed
print(data.info())

```
</details>
</div>

The column was successfully removed!

Now, we can use `.head(5)` to show the first 5 rows of the dataset (rows 0-4). Remember that the first row is "0" not "1"!

```{python}
data.head(5)
```

<hr style="border: none; border-top: 1px solid #f8f9fa; max-width: 950px;"> 

### Recoding a Variable

For our logistic regression, the diagnosis column, which is our outcome of interest, should be 0, 1 not B, M. To fix this, we can use a *dictionary* and `.map()`.   

We could also use a lambda function like we did in Session 3, but dictionaries can be more convenient if there are more than 2 values to be recoded. 

```{python}
#| eval: false
## define a dictionary
y_recode = {"B": ___, "M": ___}

## use .map to locate the keys in the column and replace with values
data["diagnosis"] = data["diagnosis"].map(________)

data.head(5)
```
<div class = "ans"> 
<details><summary> Click to reveal answers </summary>
```{python}
## define a dictionary
y_recode = {"B": 0, "M": 1}

## use .map() to locate the keys in the column and replace with values
## B becomes 0, M becomes 1
data["diagnosis"] = data["diagnosis"].map(y_recode)

data.head(5)
```

</details>
</div>

<hr style="border: none; border-top: 2px solid #007bff; width: 100%;margin-top: 20px; margin-bottom: 20px;">

## Step 3: Exploratory Data Analysis

Now that our data is cleaned and we have our outcome in numeric form, we can use `.describe()` to get summary statistics for each column of the dataset. 

```{python}
#| eval: false

___.___()
```

<div class = "ans"> 
<details><summary> Click to reveal answers </summary>
```{python}
data.describe()
```
</details>
</div>

The count column tells us the number of non-null (non-missing) values in a column. 

### Creating Descriptive Plots

Creating plots in python is similar to using ggplot in R, but there are some syntactic differences. The two most popular plotting packages in python are `matplotlib` and `seaborn`.

Matplotlib is a low-level plotting package, and seaborn is built on top of it. Therefore, you can use many matplotlib methods with seaborn plot objects.

#### Building a plot

When building a plot in python, you start with a 'figure' object and an 'axis' object. 

Data is plotted onto 'axis' objects. Axis objects sit on top of figure objects, which can be saved to variables and displayed later. 

Things like titles and legends are also associated with the 'axis' object, not the 'figure' object.

You can create figures using `plt.figure()`, with additional arguments for things like figure size. Then, you can add an axis object to the figure using fig.add_subplot().
```{python}
#| eval: false
fig = plt.figure()
ax = fig.add_subplot()
```

#### Example: Building a count plot of diagnoses using seaborn

We can look at the number of each diagnosis reflected in the dataset in a plot using `seaborn`. 

To do this, we can first construct our 'figure' and 'axis' objects. 

```{python}
# | dpi: 600
# | eval: false
fig = ___
ax = ___
```

<div class = "ans"> 
<details><summary> Click to reveal answers </summary>

```{python}
# | dpi: 600
fig = plt.figure()
ax = fig.add_subplot()
```

</details>
</div>

Then, we can create our 'count plot', which is similar to a barplot in ggplot, and assign it to the 'axis' object we just made. 

We can also set the title of the axis object using the `.set_title()` method. 

If we want to display the plot, we have to use `display()` if we are working with a quarto document or a jupyter notebook. If we are working with a regular python script, we use fig.show().

```{python}
# | eval: false
sns.countplot(x="_________", hue="_________", data=______, ax=__)
ax.set_title("Distribution of Diagnoses")

___(___)
```

<div class = "ans"> 
<details><summary> Click to reveal answers </summary>

```{python}
# | dpi: 600
sns.countplot(x="diagnosis", hue="diagnosis", data=data, ax=ax)
ax.set_title("Distribution of Diagnoses")

display(fig)
```

</details>
</div>

#### Changing plot attributes

If we want, we can change the colors of the plot. To make the plot a bit more useful, we can also change the y-scale from "count" to "percentage" and add labels so it is clear what "0" and "1" mean. 

To help us pick colors, we can use `sns.color_palette()` which will display an image with the colors in the palette. 

```{python}
sns.color_palette("colorblind")
```

To change the colors of our plot, we can make a dictionary with the values of 'diagnosis' as keys and the hexcodes of the colors we want to use as values. 

We can get the hex codes of colors from a seaborn palette using `sns.color_palette().as_hex()`.

```{python}
# | message: false
#| eval: false
color_hex = sns.color_palette("colorblind")._____

print("The hexcodes for the 'colorblind' palette are:\n", ____)

## if we want to make the columns green for benign and yellow for malignant

## the "-" lets us index from the end of the list rather than the front. However, the '-1'th position is the last position (there is no '-0')

colors = {0: color_hex[__], 1: color_hex[__]}
```

<div class = "ans"> 
<details><summary> Click to reveal answers </summary>

```{python}
# | message: False
color_hex = sns.color_palette("colorblind").as_hex()

print("The hexcodes for the 'colorblind' palette are:\n", color_hex)

## if we want to make the columns green for benign and yellow for malignant

## the "-" lets us index from the end of the list rather than the front.However, the '-1'th position is the last position (there is no '-0')

colors = {0: color_hex[2], 1: color_hex[-2]}
```

</details>
</div>

We then create the plot and tell seaborn to use 'colors' as the palette for the graph. We can also change the 'stat' to be "percent", which can be more interpretable than raw counts. 

We can also change the xtick labels to be "Benign" and "Malignant" instead of "0" and "1". 

We will also change the axis labels and set a title. Once we make these changes, we can show the finished plot. 

```{python}
# | eval: false
fig2 = plt.figure()
ax2 = fig2.add_subplot()

sns.countplot(
    x="___", hue="___", stat="___", data=data, palette=colors, legend=False, ax=ax2
)

## change the xticklabels to benign and malignant
ax2.set_xticks([0, 1])
ax2.set_xticklabels(["___", ""])

## change the axes labels and title
ax2.set(xlabel="___", ylabel="___", title="Distribution of Diagnoses")

## add legend
ax2.legend(title="Diagnosis", loc="upper right", labels=["Benign", "Malignant"])

display(fig2)
```

<div class = "ans"> 
<details><summary> Click to reveal answers </summary>

```{python}
fig2 = plt.figure()
ax2 = fig2.add_subplot()

sns.countplot(
    x="diagnosis",
    hue="diagnosis",
    stat="percent",
    data=data,
    palette=colors,
    legend=False,
    ax=ax2,
)

## change the xticklabels to benign and malignant
ax2.set_xticks([0, 1])
ax2.set_xticklabels(["Benign", "Malignant"])

## change the axes labels and title
ax2.set(xlabel="Diagnosis", ylabel="Percent", title="Distribution of Diagnoses")

## add legend
ax2.legend(title="Diagnosis", loc="upper right", labels=["Benign", "Malignant"])

## show plot
display(fig2)
plt.close(fig2)
```

</details>
</div>


#### Correlation Heatmap

If we wanted to, we could also make a correlation heatmap of our features using `.corr()` and `sns.heatmap()`. 

For this, all of our columns must be numeric, and we should remove the 'id' column as it is not useful for correlation. We use `.select_dtypes()` to select only the numeric columns from the dataset.

```{python}
# | eval: false
## set figure size
fig3 = plt.figure(figsize=(20, 20))
ax3 = fig3.add_subplot()
numeric_data = data.select_dtypes(include=___)

## drop id column
numeric_data.drop(columns=___, inplace=___)


## use corr function and seaborn heatmap to create correlation heatmap
## 'fmt' allows us to choose the number display format for the heatmap

sns.heatmap(numeric_data.___, annot=True, fmt=".2f", cmap="coolwarm", ax=ax3)

## set plot title and show plot
ax3.set_title("Feature Correlation Heatmap")

display(fig3)
```

<div class = "ans"> 
<details><summary> Click to reveal answers </summary>

```{python}
fig3 = plt.figure(figsize=(20, 20))
ax3 = fig3.add_subplot()

numeric_data = data.select_dtypes(include=[np.number])

## drop id column
numeric_data.drop(columns="id", inplace=True)


## use corr function and seaborn heatmap to create correlation heatmap
## 'fmt' allows us to choose the number display format for the heatmap

sns.heatmap(numeric_data.corr(), annot=True, fmt=".2f", cmap="coolwarm", ax=ax3)

## set plot title and show plot
ax3.set_title("Feature Correlation Heatmap")

display(fig3)
plt.close(fig3)
```
</details>
</div>

<hr style="border: none; border-top: 2px solid #007bff; width: 100%; margin-top: 20px; margin-bottom: 20px;">

## Step 4: Creating a Logistic Regression Model

Here we will explore two methods for creating a logistic regression model. The first, statsmodels, is more similar to R and is more user-friendly for statistical purposes. The second, scikit-learn, is more useful for machine learning and prediction models, but is a framework that is worth learning if you are going to use python often. 

### Method 1: Statsmodels

The <a href="https://www.statsmodels.org/stable/index.html">statsmodels package</a> is a python package for creating statistical models, conducting tests and performing data exploration. It is similar to packages used in R and creates an r-like model summary. 

If we wanted to see if higher values of area_mean and texture_mean are associated with increased odds of malignancy, we can use `smf.logit()` to fit a logistic regression model.

```{python}
# | eval: false
logit = smf.logit("___ ~ ___ + ___", data=data).fit()

print(logit.summary())
```

<div class = "ans"> 
<details><summary> Click to reveal answers </summary>
```{python}
logit = smf.logit("diagnosis ~ area_mean + texture_mean", data=data).fit()

print(logit.summary())
```
From the summary, we can see that the area_mean and texture_mean are both associated with increased odds of malignancy. 
</details>
</div>

<div>
<details><summary>**Aside:** We can also use feature selection tools from the scikit-learn package to select what features to use.</summary>

Scikit learn requires the outcome and predictor variables to be split into two data frames.

```{python}
from sklearn.feature_selection import SelectKBest, f_classif

X_raw = data.loc[:, "radius_mean"::]
## set only the diagnosis column as "y"
y = data.loc[:, "diagnosis"]

# Select top k features based on ANOVA F-value between feature and target
selector = SelectKBest(f_classif, k=5)  # Choose 'k' to specify number of features
X_selected = selector.fit_transform(X_raw, y)
selected_feature_names = X_raw.columns[selector.get_support()]

## make model eqn
formula = "diagnosis ~" + "+".join(selected_feature_names)
sm_model = smf.logit(formula, data=data).fit()

print(sm_model.summary())
```
</details>
</div>

### Method 2: Scikit-learn

The <a href="https://scikit-learn.org/stable/index.html">scikit-learn package</a> is geared towards machine-learning and prediction-related tasks like classification, clustering and dimensionality reduction. 

Fitting models with scikit-learn is a bit more complex than with statsmodels but is more along the lines of what most python projects will require. 

Instead of fitting a logistic regression model on the full dataset like we did with statsmodels, this time we are going to fit on a subset of our data and create a prediction model. We will test this prediction model on the remainder of the dataset. 

### Splitting Training and Test Data

To fit a prediction model with sci-kit learn...

We first need to split the dataset into X (predictors/features) and y (outcomes). Then we use the `train_test_split()` function to split these datasets into a training dataset and a test dataset. 

We use the .loc function and ":" to select all rows and any columns including and after "radius_mean", and we assign these columns to x. This excludes the "diagnosis" and "id" columns. 

We set y as simply the diagnosis column. 

When splitting our dataset, we can define 'test_size' which is the proportion of the data that will be set aside for testing the model. We can also set a random_state. 

<div style="border-left: 4px solid #007bff; background-color: #f8f9fa; padding: 5px; margin: 5px 0;">
Unlike R, Python allows for multi-argument returns from functions. This lets us assign each returned object to a different variable to be used later!
</div>

```{python}
#| eval: false
X = data.loc[:, "___"::]

## set only the diagnosis column as "y"
y = data.loc[:, "___"]

## here we assign each object returned from `train_test_split` to a different variable
## we can use test_size to set the proportion of the dataset reserved for testing
X_?, X_?, y_?, y_? = train_test_split(
    X, y, test_size=0.2, random_state=42
)

X_train.head(3)
```
<div class = "ans"> 
<details><summary> Click to reveal answers </summary>

```{python}
X = data.loc[:, "radius_mean"::]

## set only the diagnosis column as "y"
y = data.loc[:, "diagnosis"]

## here we assign each object returned from `train_test_split` to a different variable
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

X_train.head(3)
```
</details>
</div>

### Scaling/Normalizing Data

Because all of our features have different scales, we need to standardize (normalize) our dataset. We can do this by creating an instance of the `StandardScaler` class called "scaler" and fitting that to the training data. We then use the same "scaler" to scale the test dataset.

```{python}
#| eval: false
## standardize dataset
scaler = ___()

## fit the scaler to the _ data
scaler.fit(___)

## apply the scaler to the _ data and _ data
X_train = scaler.transform(___)
X_test = scaler.transform(___)
```
<div class = "ans"> 
<details><summary> Click to reveal answers </summary>

```{python}
## standardize dataset
scaler = StandardScaler()

## fit the scaler to the TRAINING data
scaler.fit(X_train)

## apply the scaler to BOTH the training and test data
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)
```
</details>
</div>

### After scaling the data, we can perform dimensional reduction with PCA

PCA is often used for dimensional reduction with machine learning methods so we will demonstrate it here. We can set up the PCA transformer in the same way that we set the scaler above. 

```{python}
# | eval: false
## set up PCA transformer with the number of components you want and fit to training dataset
pca = PCA(n_components=__)
pca = pca.fit(___)

## apply PCA transformer to training and test set
X_train_pca = pca.transform(___)
X_test_pca = pca.transform(___)
```

<div class = "ans"> 
<details><summary> Click to reveal answers </summary>

```{python}
## set up PCA transformer with the number of components you want and fit to training dataset
pca = PCA(n_components=10)
pca = pca.fit(X_train)

## apply PCA transformer to training and test set
X_train_pca = pca.transform(X_train)
X_test_pca = pca.transform(X_test)
```
</details>
</div>

#### We can get an idea of how well our PCA factors represent our data

To do this, we can make a plot of the cumulative explained variance. 

If we just want to make a quick plot that we do not plan on displaying multiple times, we can skip explicitly setting figure and axis objects.

Here we use `plt.plot()` from matplotlib to create a plot of the cumulative explained variance. We can use `plt.xlabel()` and `plt.ylabel()` **in the same code chunk** to set the labels for this plot.

If we try to set the labels in a later chunk, we will get a blank plot. 

```{python}
## we can look at the cumulative explained variance
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
```

## Step 5: Model Setup

Next we have to set up the model itself by creating an instance of the `LogisticRegression` model class. 

```{python}
#| eval: false
lr = ___
```
<div class = "ans"> 
<details><summary> Click to reveal answers </summary>
```{python}
lr = LogisticRegression()
```
</details>
</div>
Then, we can fit this model to the training data.

```{python}
#| eval: false
## fit to training data
lr.___(X_train_pca, y_train)
```

<div class = "ans"> 
<details><summary> Click to reveal answers </summary>
```{python}
## fit to training data
lr.fit(X_train_pca, y_train)
```
</details>
</div>

## Step 6: Look At Results

Once the model is fit, we can use it to predict the outcome (diagnosis) based on the features of the test data. 

### Store Results in a Dataframe

We can use `pd.DataFrame()` to create an empty pandas dataframe that we can fill with our results. 

```{python}
# | eval: false
## use model to predict test data
## set up dataframe to review results
results = pd.___

## get predicted
results.loc[:, "Predicted"] = lr.___(___)

## get true y values for test dataset
results.loc[:, "Truth"] = ___.___

## get probability of being malignant
## the output is one probability per outcome, we only want the second outcome (malignant)
results.loc[:, "Probability: Malignant"] = pd.DataFrame(lr.___(X_test_pca))[_]

results.head(5)
```

<div class = "ans"> 
<details><summary> Click to reveal answers </summary>

```{python}
## use model to predict test data
## set up dataframe to review results
results = pd.DataFrame()

## get predicted
results.loc[:, "Predicted"] = lr.predict(X_test_pca)

## get true y values for test dataset
results.loc[:, "Truth"] = y_test.values

## get probability of being malignant
## the output is one probability per outcome, we only want the second outcome (malignant). The second outcome uses index 1
results.loc[:, "Probability: Malignant"] = pd.DataFrame(lr.predict_proba(X_test_pca))[1]

results.head(5)
```
</details>
</div>

We can also get a quantitative "accuracy score" that will give us an idea of how well our model predicts our outcomes. 

```{python}
accuracy = accuracy_score(results["Truth"], results["Predicted"])

print("Accuracy: {:.2f}%".format(accuracy * 100))
```

### Create ROC curve

As a figure, we can create an ROC curve and use quarto chunk options to add a figure caption. 

Like we did for the cumulative variance plot, this time we will skip setting up named figure and axis objects. Instead, we will first create a 'working figure' of size 8x6 and add plots on top of that. Any 'plt.plot()' instances we create in this chunk will be overlayed on the working figure object. 

If we were working in a python script rather than a quarto document, we would need to use plt.show() at the end to display the figure. 

```{python}
# | fig-cap: An ROC curve for our logistic regression model
# | eval: false

## make a plot to vizualize the ROC curve

## get false pos rate, true pos rate and thresholds
## there are 3 outputs so we need 3 variables to catch them
___, ___, ___ = roc_curve(results["Truth"], results["Predicted"])

## get AUC data
roc_auc = auc(___, ___)

## set up plot
plt.figure(figsize=(8, 6))

## using matplotlib this time, create line plot with 2pt line weight
## add "ROC Curve (AUC = AUC)" as label for orange line
## .2f is for display formatting, lw is linewidth
plt.plot(__, __, color="darkorange", lw=2, label=f"ROC Curve (AUC = {roc_auc:.2f})")

## create another curve, this time blue with a dashed line labeled "Random"
## as in random chance.
plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--", label="Random")

## add xlabel, ylabel and title
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title(
    "Receiver Operating Characteristic (ROC) Curve\nAccuracy: {:.2f}%".format(
        accuracy * 100
    )
)

## add legend and show plot
plt.legend(loc="lower right")


```

<div class = "ans"> 
<details><summary> Click to reveal answers </summary>

```{python}
# | fig-cap: An ROC curve for our logistic regression model

## make a plot to vizualize the ROC curve

## get false pos rate, true pos rate and thresholds
fpr, tpr, thresholds = roc_curve(results["Truth"], results["Predicted"])

## get AUC data
roc_auc = auc(fpr, tpr)

## set up plot
plt.figure(figsize=(8, 6))

## using matplotlib this time, create line plot with 2pt line weight
## add "ROC Curve (AUC = AUC)" as label for orange line
## .2f is for display formatting, lw is linewidth
plt.plot(fpr, tpr, color="darkorange", lw=2, label=f"ROC Curve (AUC = {roc_auc:.2f})")

## create another curve, this time blue with a dashed line labeled "Random"
## as in random chance
plt.plot([0, 1], [0, 1], color="navy", lw=2, linestyle="--", label="Random")

## add xlabel, ylabel and title
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title(
    "Receiver Operating Characteristic (ROC) Curve\nAccuracy: {:.2f}%".format(
        accuracy * 100
    )
)

## add legend and show plot
plt.legend(loc="lower right")

```
</details>
</div>

Congratulations! You have successfully done logistic regression in Python!
<div>
<details><summary>Create a Statsmodels-like model and summary with scikit-learn and statsmodels</summary>

It is also possible to fit a model with scikit-learn, extract the coefficients, and use them to create a statsmodels model and summary. 

Typically, you would want to pick which package (sklearn or statsmodels) you want to use and stick with it, but this is an option if necessary. Note: I am showing Lasso here as well because statsmodels will fail if there are highly correlated features like with this dataset, however this same method can be used on a scikit-learn logistic regression model without Lasso penalties.

This time, we are going to fit on the full data. 

First, we can select features to use for model (statsmodels does not perform regularization and therefore will fail to converge when there are highly correlated features). Scikit-learn gives us multiple ways to do this. Let's use LASSO. 

```{python}
## scale X
X = scaler.transform(X_raw)

## set up model for Lasso and fit it
model = LogisticRegression(penalty="l1", solver="liblinear", C=0.01)
model.fit(X, y)

# Get non-zero coefficient features
selected_features = X_raw.columns[model.coef_[0] != 0]
X_selected = X_raw[selected_features]
print(X_selected.columns)
```

**Fit statsmodels model and get summary**

```{python}
## Get coefficients
intercept = model.intercept_[0]
coefficients = model.coef_[0][model.coef_[0] != 0]

## make model eqn
formula = "diagnosis ~" + "+".join(X_selected.columns)
sm_model2 = smf.logit(formula, data=data).fit()

sm_model2.params[:] = np.concatenate(
    ([intercept], coefficients)
)  # Set params from scikit-learn model

# Display the summary
print(sm_model2.summary())
```

</details>
</div>


<details>
<summary> Citations </summary>
Icons  
<a href="https://www.flaticon.com/free-icons/csv" title="csv icons">Csv icons created by rizal2109 - Flaticon</a>
<a href="https://www.flaticon.com/free-icons/ipynb" title="ipynb icons">Ipynb icons created by JunGSa - Flaticon</a>
<a href="https://www.flaticon.com/free-icons/coding" title="coding icons">Coding icons created by juicy_fish - Flaticon</a>  
</details>