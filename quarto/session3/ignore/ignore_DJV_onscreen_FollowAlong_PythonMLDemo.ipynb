{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Python Logistic Regression Demo\"\n",
    "author: \"Python Group\"\n",
    "---\n",
    "\n",
    "## Links\n",
    "\n",
    "<a href=\"https://www.kaggle.com/datasets/erdemtaha/cancer-data/data\" class=\"link-block\">\n",
    "    <img src=\"imgs/csv.png\" alt=\"Dataset\">\n",
    "    <p>Cancer Dataset</p>\n",
    "</a><a href=\"https://github.mskcc.org/Python-Workshop/Python-Workshop.github.io/tree/main/FollowAlong\" class=\"link-block\">\n",
    "    <img src=\"imgs/code.png\" alt=\"File\">\n",
    "    <p>Download Follow Along File</p>\n",
    "</a>\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../../style.css\">\n",
    "\n",
    "We are using the same dataset as in session 3! There is no need to re-download it!\n",
    "<hr style=\"border: none; border-top: 2px solid #007bff; width: 100%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "Before doing anything else, we should first activate the conda environment we want to use. **Please activate the 'python-intro-env' environment**. \n",
    "\n",
    "From terminal, type: \n",
    "  \n",
    "<div style=\"background-color: #1e1e1e;\n",
    "    border: 1px solid #f8f9fa;\n",
    "    /* Optional border */\n",
    "    color: #ffffff;\n",
    "    padding: 5px;\n",
    "    /* Top 10px, right 5px, bottom 5px, left 5px */\n",
    "    font-family: monospace;\n",
    "    border-radius: 5px;\n",
    "    max-width: 800px;\">\n",
    "\\> conda activate python-intro-env  \n",
    "</div>\n",
    "\n",
    "When in VS code, you might get a popup message like the one below, confirming that the environment was activated:  \n",
    "\n",
    "<div style=\"border-left: 4px solid #007bff; background-color: #f8f9fa; padding: 5px; margin: 5px 0; color: #007bff; max-width: 800px;border-radius: 5px;\">\n",
    "Selected conda environment was successfully activated, even though \"(python-intro-env)\" indicator may not be present in the terminal prompt. \n",
    "</div>\n",
    "\n",
    "or\n",
    "\n",
    "In **Anaconda Navagator**, click on the **Environments** tab on the left and select the environment you want to activate. Just selecting the environment should activate it. \n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"border: none; border-top: 1px solid #f8f9fa; max-width: 950px;\">\n",
    "\n",
    "\n",
    "## Refresher: How to install packages - Install statsmodels\n",
    "\n",
    "To install packages, we can either use the \"anaconda\" dashboard, or we can use the command line. Make sure your environment is active before installing packages or the packages will not be available in your environment. \n",
    "\n",
    "To install from the command line, we open a terminal and type: \n",
    "\n",
    "<div style=\"background-color: #1e1e1e;\n",
    "    border: 1px solid #f8f9fa;\n",
    "    /* Optional border */\n",
    "    color: #ffffff;\n",
    "    padding: 5px;\n",
    "    /* Top 10px, right 5px, bottom 5px, left 5px */\n",
    "    font-family: monospace;\n",
    "    border-radius: 5px;\n",
    "    max-width: 800px;\">\n",
    "\\> conda install {package}\n",
    "</div>\n",
    "\n",
    "or\n",
    "\n",
    "<div style=\"background-color: #1e1e1e;\n",
    "    border: 1px solid #f8f9fa;\n",
    "    /* Optional border */\n",
    "    color: #ffffff;\n",
    "    padding: 5px;\n",
    "    /* Top 10px, right 5px, bottom 5px, left 5px */\n",
    "    font-family: monospace;\n",
    "    border-radius: 5px;\n",
    "    max-width: 800px;\">\n",
    "\\> pip install {package}\n",
    "</div>\n",
    "\n",
    "When working with conda environments, it's best practice to install everything with conda and only use pip for packages that are not available through conda!\n",
    "</details>\n",
    "\n",
    "### Stats models can be installed via conda install so:\n",
    "\n",
    "<div style=\"background-color: #1e1e1e;\n",
    "    border: 1px solid #f8f9fa;\n",
    "    /* Optional border */\n",
    "    color: #ffffff;\n",
    "    padding: 5px;\n",
    "    /* Top 10px, right 5px, bottom 5px, left 5px */\n",
    "    font-family: monospace;\n",
    "    border-radius: 5px;\n",
    "    max-width: 800px;\">\n",
    "\\> conda install statsmodels\n",
    "</div>\n",
    "<br>\n",
    "<hr style=\"border: none; border-top: 1px solid #f8f9fa; max-width: 950px;margin-top: 20px; margin-bottom: 20px;\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to make sure we have the packages we'll need installed in the environment before we try to import them, we can either check on anaconda or use the terminal: \n",
    "\n",
    "<div style=\"background-color: #1e1e1e;\n",
    "    border: 1px solid #f8f9fa;\n",
    "    /* Optional border */\n",
    "    color: #ffffff;\n",
    "    padding: 5px;\n",
    "    /* Top 10px, right 5px, bottom 5px, left 5px */\n",
    "    font-family: monospace;\n",
    "    border-radius: 5px;\n",
    "    max-width: 800px;\">\n",
    "\\> conda list\n",
    "</div>\n",
    "<div style=\"sp\"></div>\n",
    "Otherwise, we will get an error message if we try to import packages that are not installed. \n",
    "<hr style=\"border: none; border-top: 2px solid #007bff; width: 100%;margin-top: 20px; margin-bottom: 20px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Packages\n",
    "\n",
    "Similar to `library()` in R, weâ€™ll use `import` in Python. Fill in the blanks to import the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as ___\n",
    "import numpy as ___\n",
    "import seaborn as ___\n",
    "import matplotlib.pyplot as ___\n",
    "\n",
    "import statsmodels.api as __\n",
    "import statsmodels.formula.api as ___\n",
    "\n",
    "# Import from sklearn\n",
    "from sklearn.model_selection import ___\n",
    "from sklearn.preprocessing import ____\n",
    "from sklearn.decomposition import ___\n",
    "\n",
    "\n",
    "from sklearn.linear_model import ___\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 2px solid #007bff; width: 100%;margin-top: 20px; margin-bottom: 20px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "</div>\n",
    "\n",
    "## Step 2: Read in Data and Perform Data Cleaning\n",
    "\n",
    "We can use the `read_csv()` function from the pandas package to read in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"example_data/Cancer_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 1px solid #f8f9fa; max-width: 950px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "</div>\n",
    "\n",
    "We can use the `.info()` function to show some basic information about the dataset like:  \n",
    "* the number of rows  \n",
    "* number of columns  \n",
    "* column labels  \n",
    "* column type  \n",
    "* number of non-null values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "From the **info**, we can see that the column types make sense and most of the columns have no missing values. \n",
    "\n",
    "We do have this extra column called \"Unnamed: 32\" with 0 non-null values...\n",
    "so let's drop it (remove it from the dataframe).   \n",
    "  \n",
    "We can also replace spaces in column names with \"_\", which will be useful later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=\"Unnamed: 32\", inplace=______)\n",
    "\n",
    "\n",
    "data.columns = data.columns.str.replace(\"?\", \"?\")\n",
    "\n",
    "\n",
    "# Check that the column was removed and column names were changed.\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "</div>\n",
    "\n",
    "The column was successfully removed!\n",
    "\n",
    "Now, we can use `.head(5)` to show the first 5 rows of the dataset (rows 0-4). Remember that the first row is \"0\" not \"1\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 1px solid #f8f9fa; max-width: 950px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoding a Variable\n",
    "\n",
    "For our logistic regression, the **diagnosis column**, which is our outcome of interest, should be 0, 1 not B, M. To fix this, we can use a *dictionary* and `.map()`. We could also use a lambda function like we did in Session 3, but dictionaries can be more convenient if there are more than 2 values to be recoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a dictionary\n",
    "y_recode = {\"B\": ___, \"M\": ___}\n",
    "\n",
    "## use .map to locate the keys in the column and replace with values\n",
    "data[\"diagnosis\"] = data[\"diagnosis\"].map(________)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 2px solid #007bff; width: 100%;margin-top: 20px; margin-bottom: 20px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "</div>\n",
    "\n",
    "## Step 3: Exploratory Data Analysis\n",
    "\n",
    "Now that our data is cleaned and we have our outcome in numeric form, we can use `.describe()` to get summary statistics for each column of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "___.___()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count column tells us the number of non-null (non-missing) values in a column. \n",
    "<hr style=\"border: none; border-top: 2px solid #007bff; width: 100%; margin-top: 20px; margin-bottom: 20px;\">\n",
    "\n",
    "\n",
    "### Creating Descriptive Plots\n",
    "\n",
    "We can also look at the number of each diagnosis reflected in the dataset in a plot using seaborn. \n",
    "\n",
    "You can also save a plot to a variable (ex: 'p') if you want to display it later with `plt.show(p)`.\n",
    "\n",
    "Note: Plotting in python is similar to plotting in R, but there are a few key differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"_________\", hue=\"_________\", data=______)\n",
    "plt.title(\"Distribution of Diagnoses\")\n",
    "___.___()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 1px solid #f8f9fa; max-width: 950px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "</div>\n",
    "\n",
    "If we want, we can change the colors of the plot. To make the plot a bit more useful, we can also change the y-scale from \"count\" to \"percentage\" and add labels so it is clear what \"0\" and \"1\" mean. \n",
    "\n",
    "To help us pick colors, we can use `sns.color_palette()` which will display an image with the colors in the palette. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.color_palette(\"colorblind\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To change the colors of our plot, we can make a dictionary with the values of 'diagnosis' as keys and the hexcodes of the colors we want to use as values. \n",
    "\n",
    "We can get the hex codes of colors from a seaborn palette using `sns.color_palette().as_hex()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "message": false
   },
   "outputs": [],
   "source": [
    "color_hex = sns.color_palette(\"colorblind\")._____\n",
    "\n",
    "print(\"The hexcodes for the 'colorblind' palette are:\\n\", ____)\n",
    "\n",
    "## if we want to make the columns green for benign and yellow for malignant\n",
    "\n",
    "## the \"-\" lets us index from the end of the list rather than the front. However, the '-1'th position is the last position (there is no '-0')\n",
    "\n",
    "colors = {0: color_hex[__], 1: color_hex[__]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "</div>\n",
    "\n",
    "We then create the plot and tell seaborn to use 'colors' as the palette for the graph. We can also change the 'stat' to be \"percent\", which can be more interpretable than raw counts. \n",
    "\n",
    "We can also change the xtick labels to be \"Benign\" and \"Malignant\" instead of \"0\" and \"1\". Because we assigned the plot to the variable 'p', we can use `p.{}` to change attributes of plot 'p'.\n",
    "\n",
    "We will also change the axis labels and set a title. Once we make these changes, we can show the finished plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.countplot(\n",
    "    x=\"___\",\n",
    "    hue=\"___\",\n",
    "    stat=\"___\",\n",
    "    data=data,\n",
    "    palette=colors,\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "## change the xticklabels to benign and malignant\n",
    "p.set_xticks([0, 1])\n",
    "p.set_xticklabels([\"___\", \"\"])\n",
    "\n",
    "## change the axes labels and title\n",
    "p.set(xlabel=\"___\", ylabel=\"___\", title=\"Distribution of Diagnoses\")\n",
    "\n",
    "## add legend\n",
    "plt.legend(title=\"Diagnosis\", loc=\"upper right\", labels=[\"Benign\", \"Malignant\"])\n",
    "\n",
    "## show plot\n",
    "plt.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "</div>\n",
    "\n",
    "If we wanted to, we could also make a correlation heatmap of our features using `.corr()` and `sns.heatmap()`. \n",
    "\n",
    "For this, all of our columns must be numeric, and we should remove the 'id' column as it is not useful for correlation. We use `.select_dtypes()` to select only the numeric columns from the dataset.\n",
    "\n",
    "<div style=\"border-left: 4px solid #007bff; background-color: #f8f9fa; padding: 5px; margin: 5px 0; color: #007bff; max-width: 950px;border-radius: 5px;\"><b>Note:</b> Python has slightly a different way of formatting strings/labels than R.   \n",
    "The '.2f' means format the number labels to 2 decimal places. If you use `sprintf()` in R, this is the equivalent of \"%.2f\". </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = data.select_dtypes(include=___)\n",
    "\n",
    "## drop id column\n",
    "numeric_data.drop(columns=___, inplace=___)\n",
    "\n",
    "## set figure size\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "## use corr function and seaborn heatmap to create correlation heatmap\n",
    "## 'fmt' allows us to choose the number display format for the heatmap\n",
    "\n",
    "sns.heatmap(numeric_data.___, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "\n",
    "## set plot title and show plot\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "\n",
    "plt.___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 2px solid #007bff; width: 100%; margin-top: 20px; margin-bottom: 20px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "</div>\n",
    "\n",
    "## Step 4: Creating a Logistic Regression Model\n",
    "\n",
    "Here we will explore two methods for creating a logistic regression model. The first, **statsmodels**, is more similar to R and is more user-friendly for statistical purposes. The second, scikit-learn, is more useful for machine learning and prediction models, but is a framework that is worth learning if you are going to use python often. \n",
    "\n",
    "### Method 1: Statsmodels\n",
    "\n",
    "The <a href=\"https://www.statsmodels.org/stable/index.html\">statsmodels package</a> is a python package for creating statistical models, conducting tests and performing data exploration. It is similar to packages used in R and creates an r-like model summary. According to its documentation, the statsmodels package has also been tested against and compared to equivalent R packages.  \n",
    "<hr style=\"border: none; border-top: 1px solid #f8f9fa; max-width: 950px;\">\n",
    "\n",
    "\n",
    "If we wanted to see if higher values of area_mean and texture_mean are associated with a higher probability of malignancy, we can use `smf.logit()` to fit a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = smf.logit(\"___ ~ ___ + ___\", data=data).fit()\n",
    "\n",
    "print(logit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary, we can see that the area_mean and texture_mean are both associated with an increased probability of malignancy. \n",
    "\n",
    "\n",
    "<div style=\"border-left: 4px solid #007bff; background-color: #f8f9fa; padding: 5px; margin: 5px 0; color: #007bff; max-width: 950px;border-radius: 5px;\"><b>Aside:</b> We can also use feature selection tools from the scikit-learn package to select what features to use. I will not cover this during the demo for the sake of time, but the tutorial is on the website for you! </div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 2px solid #007bff; width: 100%; margin-top: 20px; margin-bottom: 20px;\">\n",
    "\n",
    "\n",
    "### Method 2: Scikit-learn\n",
    "\n",
    "The <a href=\"https://scikit-learn.org/stable/index.html\">scikit-learn package</a> is geared towards machine-learning and prediction-related tasks like classification, clustering and dimensionality reduction. \n",
    "\n",
    "Fitting models with scikit-learn is a bit more complex than with statsmodels but is more along the lines of what most python projects will require. \n",
    "\n",
    "Instead of fitting a logistic regression model on the full dataset like we did with statsmodels, this time we are going to fit on a subset of our data and create a prediction model. We will test this prediction model on the remainder of the dataset. \n",
    "\n",
    "<hr style=\"border: none; border-top: 1px solid #f8f9fa; max-width: 950px;\">\n",
    "\n",
    "\n",
    "### Splitting Training and Test Data\n",
    "\n",
    "To fit a prediction model with sci-kit learn...\n",
    "\n",
    "We first need to split the dataset into X (predictors/features) and y (outcomes). Then we use the `train_test_split()` function to split these datasets into a training dataset and a test dataset. \n",
    "\n",
    "We use the .loc function and \":\" to select all rows and any columns including and after \"radius_mean\", and we assign these columns to x. This excludes the \"diagnosis\" and \"id\" columns. \n",
    "\n",
    "We set y as simply the diagnosis column. \n",
    "\n",
    "When splitting our dataset, we can define 'test_size' which is the proportion of the data that will be set aside for testing the model. We can also set a random_state. \n",
    "\n",
    "<div style=\"border-left: 4px solid #007bff; background-color: #f8f9fa; padding: 5px; margin: 5px 0; color: #007bff; max-width: 950px;border-radius: 5px;\">\n",
    "Unlike R, Python allows for multi-argument returns from functions. This lets us assign each returned object to a different variable to be used later! \n",
    "  \n",
    "If we don't want to have multiple separate variables, we can provide a single variable and the output of the function will be saved to that variable as a list or similar object.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.loc[:, \"___\"::]\n",
    "\n",
    "## set only the diagnosis column as \"y\"\n",
    "y = data.loc[:, \"___\"]\n",
    "\n",
    "## here we assign each object returned from `train_test_split` to a different variable\n",
    "## we can use test_size to set the proportion of the dataset reserved for testing\n",
    "X_?, X_?, y_?, y_? = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 1px solid #f8f9fa; max-width: 950px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "</div>\n",
    "\n",
    "### Scaling/Normalizing Data\n",
    "\n",
    "Because all of our features have different scales, we need to standardize (normalize) our dataset. We can do this by creating an instance of the `StandardScaler` class called \"scaler\" and fitting that to the training data. We then use the same \"scaler\" to scale the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## standardize dataset\n",
    "scaler = ___()\n",
    "\n",
    "## fit the scaler to the _ data\n",
    "scaler.fit(___)\n",
    "\n",
    "## apply the scaler to the _ data and _ data\n",
    "X_train = scaler.transform(___)\n",
    "X_test = scaler.transform(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After scaling the data, we can perform dimensional reduction with PCA\n",
    "\n",
    "PCA (principal components analysis) is often used for dimensional reduction with machine learning methods, so we will demonstrate it here. It also helps us avoid multicollinearity with a dataset like this where many of the predictors are correlated. We can set up the PCA transformer in the same way that we set the scaler above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up PCA transformer with the number of components you want and fit to training dataset\n",
    "pca = PCA(n_components=__)\n",
    "pca = pca.fit(___)\n",
    "\n",
    "## apply PCA transformer to training and test set\n",
    "X_train_pca = pca.transform(___)\n",
    "X_test_pca = pca.transform(___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we want to look at how well our PCA factors capture the overall variance of the data...\n",
    "\n",
    "We can plot the cumulative explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can look at the cumulative explained variance\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel(\"Number of Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 2px solid #007bff; width: 100%; margin-top: 20px; margin-bottom: 20px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Model Setup\n",
    "\n",
    "Next we have to set up the model itself by creating an instance of the `LogisticRegression` model class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, we can fit this model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fit to training data\n",
    "lr.___(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 2px solid #007bff; width: 100%; margin-top: 20px; margin-bottom: 20px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>\n",
    "</div>\n",
    "\n",
    "## Step 6: Look At Results\n",
    "\n",
    "Once the model is fit, we can use it to predict the outcome (diagnosis) based on the features of the test data. \n",
    "\n",
    "### Store Results in a Dataframe\n",
    "\n",
    "We can use `pd.DataFrame()` to create an empty pandas dataframe that we can fill with our results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use model to predict test data\n",
    "## set up dataframe to review results\n",
    "results = pd.___\n",
    "\n",
    "## get predicted\n",
    "results.loc[:, 'Predicted']= lr.___(___)\n",
    "\n",
    "## get true y values for test dataset\n",
    "results.loc[:, 'Truth'] = ___.___\n",
    "\n",
    "## get probability of being malignant\n",
    "## the output is one probability per outcome, we only want the second outcome (malignant)\n",
    "results.loc[:, 'Probability: Malignant'] = pd.DataFrame(lr.___(X_test_pca))[_]\n",
    "\n",
    "results.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 1px solid #f8f9fa; max-width: 950px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get a quantitative \"accuracy score\" that will give us an idea of how well our model predicts our outcomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(results[\"Truth\"], results[\"Predicted\"])\n",
    "\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 2px solid #007bff; width: 100%; margin-top: 20px; margin-bottom: 20px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ROC curve\n",
    "\n",
    "As a figure, we can create an ROC curve and use quarto chunk options to add a figure caption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | fig-cap: An ROC curve for our logistic regression model\n",
    "## make a plot to vizualize the ROC curve\n",
    "\n",
    "## get false pos rate, true pos rate and thresholds\n",
    "## there are 3 outputs so we need 3 variables to catch them\n",
    "___, ___, ___ = roc_curve(results[\"Truth\"], results[\"Predicted\"])\n",
    "\n",
    "## get AUC data\n",
    "roc_auc = auc(___, ___)\n",
    "\n",
    "## set up plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "## using matplotlib this time, create line plot with 2pt line weight\n",
    "## add \"ROC Curve (AUC = AUC)\" as label for orange line\n",
    "## .2f is for display formatting, lw is linewidth\n",
    "plt.plot(fpr, tpr, color=\"darkorange\", lw=2, label=f\"ROC Curve (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "## create another curve, this time blue with a dashed line labeled \"Random\"\n",
    "## as in random chance.\n",
    "plt.plot(___, ___, color=\"navy\", lw=2, linestyle=\"--\", label=\"Random\")\n",
    "\n",
    "## add xlabel, ylabel and title\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\n",
    "    \"Receiver Operating Characteristic (ROC) Curve\\nAccuracy: {:.2f}%\".format(\n",
    "        accuracy * 100\n",
    "    )\n",
    ")\n",
    "\n",
    "## add legend and show plot\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully done logistic regression in Python!\n",
    "<hr style=\"border: none; border-top: 2px solid #007bff; width: 100%; margin-top: 20px; margin-bottom: 20px;\">\n",
    "<br><br>\n",
    "\n",
    "## EXTRA - ONLY IF WE HAVE TIME\n",
    "\n",
    "<div>\n",
    "<details><summary>Create a Statsmodels-like model and summary with scikit-learn and statsmodels</summary>\n",
    "\n",
    "It is also possible to fit a model with scikit-learn, extract the coefficients, and use them to create a statsmodels model and summary. \n",
    "\n",
    "Typically, you would want to pick which package (sklearn or statsmodels) you want to use and stick with it, but this is an option if necessary. Note: I am showing Lasso here as well because statsmodels will fail if there are highly correlated features like with this dataset, however this same method can be used on a scikit-learn logistic regression model without Lasso penalties.\n",
    "\n",
    "This time, we are going to fit on the full data. \n",
    "\n",
    "First, we can select features to use for model (statsmodels does not perform regularization and therefore will fail to converge when there are highly correlated features). Scikit-learn gives us multiple ways to do this. Let's use LASSO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## scale X\n",
    "X = scaler.transform(X_raw)\n",
    "\n",
    "## set up model for Lasso and fit it\n",
    "model = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=0.01)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get non-zero coefficient features\n",
    "selected_features = X_raw.columns[model.coef_[0] != 0]\n",
    "X_selected = X_raw[selected_features]\n",
    "print(X_selected.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit statsmodels model and get summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get coefficients\n",
    "intercept = model.intercept_[0]\n",
    "coefficients = model.coef_[0][model.coef_[0] != 0]\n",
    "\n",
    "## make model eqn\n",
    "formula = \"diagnosis ~\" + \"+\".join(X_selected.columns)\n",
    "sm_model2 = smf.logit(formula, data=data).fit()\n",
    "\n",
    "sm_model2.params[:] = np.concatenate(\n",
    "    ([intercept], coefficients)\n",
    ")  # Set params from scikit-learn model\n",
    "\n",
    "# Display the summary\n",
    "print(sm_model2.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-intro-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
