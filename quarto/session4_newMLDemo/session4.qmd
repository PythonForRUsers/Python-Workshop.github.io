---
title: '**Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries**'
jupyter: python3
format: revealjs
---



### Session Overview

This session is divided into two parts:  
1. **A quick recap of Object-Oriented Programming (OOP)** and why it‚Äôs useful.  
2. **Applying OOP concepts to machine learning** by building models in scikit-learn.

---

## Introduction

Both R and python use objects, but not everything in R is object-oriented... If that sounds confusing that's because it is! 

**Functional programming:** focuses on functions as the primary unit of code

**Object-oriented programming:** uses objects with attached attributes(data) and methods(behaviors)

Functional and object-oriented programming are paradigms (styles) and these styles can be applied in both R and Python. However, Python libraries and workflows tend to rely more on object-oriented programming than those designed for R. 

R originated from another statistical programming language called S, which is not object-oriented, and R tends to lend itself better to functional programming than object-oriented programming in many cases. 

Both R and python use objects, but not everything in R is object-oriented... if that sounds confusing that's because it is! 

I want to quickly define two concepts here that will help us talk about this, functional and object-oriented programming. Functional programming focuses on functions as the primary unit of code. Object-oriented programming uses objects with attached attributes(data) and methods (behaviors)

## Why Python? üêç
 
- R built by statisticians for statisticians: 
    * Excels at statistical analysis and modeling
    * Beautiful data visualizations with fairly simple code 
- Python is a general purpose language (like C++, Java):
    * Excels at deep learning, image analysis, text analysis
    * But also: 
        - Automation
        - Software/Application development (including CLI [command line interface])
        - Web development

Many of the most popular Python libraries for modeling‚Äîsuch as scikit-learn, statsmodels, PyTorch, and TensorFlow‚Äîare built around the principles of object-oriented programming (OOP).  

This means that to work effectively in Python, especially for tasks involving modeling or model training, <span style="color: #007acc"><strong>it helps to think in terms of objects and classes, not just functions.</strong></span>


While R excels at statistical analysis and modeling, Python‚Äôs object-oriented nature makes it especially powerful for deep learning and AI. It is also more popular for things like image and text analysis. 

Additionally, python can be used for automation, software/app development and web development. 


## Functions vs Objects in Python

Python absolutely still uses functions (just like R), and they‚Äôre incredibly useful‚Äîparticularly for data transformations, wrangling, or tasks like parallel processing. 

But when it comes to modeling, the dominant paradigm is object-oriented.

Models in Python: 
- Typically instances of classes
- Come with built-in methods (like .fit() or .predict()) and attributes (like .coef_ or .score) that define their behavior and internal state

## Why This Matters for Machine Learning/ Modeling

If you‚Äôre doing machine learning, deep learning, or building custom models in Python, you're often working in domains where R doesn't offer as much built-in support. That‚Äôs where packages like:

- **scikit-learn** (machine learning)
- **PyTorch** and **TensorFlow** (deep learning and neural networks)

come in‚Äîand they all require an understanding of how to work with **objects and classes**.

- **Scikit-learn** provides a wide array of **ready-to-use model classes**, making it a great entry point.  
- **PyTorch** and **TensorFlow**, especially for custom neural network architectures, require you to **create your own classes** using inheritance from base classes and mixins.

I won‚Äôt go deep into PyTorch or TensorFlow here, but feel free to ask me later or explore tutorials online if you're curious!


## Statsmodels

- **Statsmodels** offers a more R-like interface for regression models but requires some additional setup for design matrices. (You can check out their excellent [documentation here](https://www.statsmodels.org/stable/gettingstarted.html).)

I won't cover it here, but it is worth looking up if you are interested. 

## What We'll Cover

To get comfortable with this way of thinking, we‚Äôll first do a **brief recap of object-oriented programming**‚Äîwhat classes are, how inheritance works, and how you can define your own classes in Python.

Then we‚Äôll shift focus to using **model classes in Python**, particularly with **scikit-learn**. Whether you‚Äôre using a prebuilt model class or writing your own from scratch, the workflow is often similar. You‚Äôll still need to define or use common methods like `.fit()`, `.predict()`, and `.score()`, and understand how the model stores internal data like coefficients and hyperparameters.


## **Part 1: Object-Oriented Programming**  

### **Recap: What Are Classes and Objects?**  
A **class** is a **blueprint** for creating objects. An **object** is an **instance** of a class that contains **data (attributes)** and **behaviors (methods)**.  

For example, in Python, we can define a class called Dog and give it attributes that store data about a given dog.  
We can also define methods that represent behaviors an object of the dog class can perform:  


```{python}
#| slideshow: {slide_type: subslide}
#| vscode: {languageId: python}
class Dog:
    def __init__(self, name, breed):
        self.name = name
        self.breed = breed

    def bark(self):
        return f"{self.name} says woof!"
```

Creating an instance (object) of the `Dog` class lets us model a particular dog:

```{python}
#| vscode: {languageId: python}

my_dog = Dog("Buddy", "Golden Retriever")
print(my_dog.bark())  
```

Here, `my_dog` is an **object** of the `Dog` class.   

It has **attributes** (`name`, `breed`) and **methods** (`bark()`).  

When we make an instance of the Dog class: 
 - We set the value of the attributes [`name` and `breed`], which are then stored as part of the `my_dog` object
 - We can use any methods defined in the Dog class on `my_dog`
 
**Note:** For python methods, the `self` argument is assumed to be passed and therefore we do not put anything in the parentheses when calling `.bark()`. 

## **How Does This Relate to Machine Learning and Modeling?**  
Machine learning models in Python are implemented as **classes**.  
- When you create a model, you‚Äôre **instantiating an object** of a predefined class (e.g., `LogisticRegression()`).  
- That model **inherits** attributes (parameters, coefficients) and methods (like `.fit()` and `.predict()`).  


For example, a **logistic regression model** in `scikit-learn` is an instance of the `LogisticRegression` class:
```python
## Example: 
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()  # Creating an instance of the LogisticRegression class
model.fit(X_train, y_train)   # Calling a method to train the model
predictions = model.predict(X_test)  # Calling a method to make predictions
```
Here, **`model`** is an **object** that has inherited attributes and methods from `LogisticRegression`.  

üëâ **To check if an object is an instance of a particular class**, use:
```python
isinstance(object, class)  # Returns True if `object` is an instance of `class`.
```
Knowing what class an object is helps us know what methods we can expect to have access to.

### **Key Benefits of OOP in Machine Learning**  

1. **Encapsulation** ‚Äì Models store parameters and methods inside a single object.  
2. **Inheritance** ‚Äì New models can build on base models, reusing existing functionality.  
3. **Abstraction**  ‚Äì `.fit()` should work as expected, regardless of complexity of underlying implimentation.
4. **Polymorphism** ‚Äì Different models share the same method names (`.fit()`, `.predict()`), making them easy to use interchangeably. 

## Session 4 - Object Oriented Programming and Machine Learning Modules

Now that we know what classes are and what data structures are, let's apply that!

For this session, I am going to split it into two parts. First, we are going to do a brief recap of object oriented programming (OOP) and an example of how classes are useful in programming. We will also discuss why OOP is useful for machine learning projects. Then, we are going to apply what we learned about object oriented programming to make two models with scikit-learn. 

One of the main types of projects that would be done in Python rather than R are projects involving machine learning and deep learning models. Python's object oriented nature lends itself well to these kinds of projects. 

If we recall from session 1, a class is a blueprint for creating objects and an instance of a class is an object with a given blueprint that contains specific data. Classes can have attributes which consist of data attributes (instance variables (unique to each instance) and class variables (shared by all instances of the class)) and methods (functions belonging to an object, shared by all instances of the class). 

While R does allow for the creation of classes, we tend to write more functions than classes in R. Python uses a more object-oriented style than R, and it is more convenient in python to create a class with assoiated methods (aka functions that modify the state of the class instance), because derived classes can inherit methods from base classes. Objects can contain arbitrary amounts and kinds of data. Additionally, most built-in operators with special syntax (aka arithmetic operators) can be redefined for class instances. Classes can also have their own iterators, which allow us to loop over the data contained in an instance of a class in some sensible way.

Inheritence is when new classes are created based off of base classes, allowing the new classes to inherit methods from the base classes. Understanding the base class an object has can help you understand what methods it will have as well. However, derived classes may override methods of their base classes. There are also classes called Mixins, which are similar to base classes in that they can be used to add functionality to a new class (https://scikit-learn.org/stable/api/sklearn.base.html), but mixin classes are not useful on their own. Mixins allow for reusable functionality without enforcing a rigid structure like a base class. If you are going to be creating models in pytorch or tensor flow, understanding base classes and mixins is very helpful. 

If you want to check the type of an instance, you can use `isinstance(obj, type)` to check if obj is of type 'type' or of some class derived from 'type'. 


## Projects
Most of the time, you will likely be doing things in python that there is not a lot of support for in R -> think machine learning/AI. 
Packages for these things require an understanding of object oriented programming (classes, class instances, methods, attributes.) For example, most packages come equiped with functions/methods you will need to use while using the packages. 

In addition to the object oriented programming, we will demonstrate how to make some useful plots and display tables in python similarly to how we would do it in R and maybe how to make a report?? idk haven't decided yet. 

I want to walk through a project and spend a bit of time on each thing you'd do + compare it to R. 

Some examples being: sci-kit learn, pytorch, tensorflow. 


## Part 1: Why object oriented programming matters in ML

Because classes can inherit from other classes (and other reasons), it makes sense for python to use classes. This is especially important in the context of creating models becausae when we want to make a model or classifier in python we are making an instance of a class. For example, if we wanted to make a logistic regression model in python we would make an instance of the LogisticRegression() class from sci-kit learn. Once we make an instance of a class, that instance has access to all methods associated with the class it came from. For example, it has methods like 'fit' that fit the model to a dataset and 'predict' that uses the model to predict the outcome based on the supplied values of predictor variables. 

Object oriented programming is important in ML because:
* ML libraries like scikit-learn, pytorch and tensorflow are built around objects (models and datastructures are objects)
* Because the models are instances of classes, they are self-contained with their parameters and methods. They can also inherit from a base model class, making it easier to customize models and understand what methods a model will have. You can also write your own derivative model classes using one of the pre-built model classes as a base class. 
* Finally, because models are often built off of a base model class, they will have methods by the same expected name that work across different models (ex: `.fit()`, `.predict()`, `.transform()`)

When we want to make an instance of a class, we 'call' or instantiate the class object, creating an instance of the class object. 
`x = MyClass()` creates an instance `x` of the class `MyClass()`. 


## Example: Understanding Classes - Definition, Inheritance, Mixins

Before we get into the machine learning demo projects, I want to quickly demonstrate how classes work and how we can leverage inheritance when making our own classes. Even though this example is very simple, the same method applies to making your own classes for machine learning and neural network models.


## Base Classes
A **base class** (or parent class) serves as a template for creating objects. Other classes can inherit from it to reuse its properties and methods.

For example, the class we created earlier, `Dog`, could be a base class. 

```{python}
#| vscode: {languageId: python}
class Dog: ## class definition
    def __init__(self, name, breed): ## sets up the initialization for an instance of class Dog. Allows us to assign name and breed when we instantiate dog. 
        self.name = name ## attributes
        self.breed = breed

    def bark(self): ## method
        return f"{self.name} says Woof!"

my_dog = Dog("Fido", "Labrador") ## create a dog of name 'Fido' and breed 'Labrador'

print(my_dog.bark())

## if we want to see what kind of dog our dog is
print(f"Our dog {my_dog.name} is a {my_dog.breed}.")
```

### Derived (Child) Classes
Now that we have a `Dog` class, let‚Äôs define a new class called `GuardDog`. This class will inherit all the properties and methods from `Dog`, while also adding its own unique attributes and behaviors.

This is the power of inheritance‚Äîwe don‚Äôt have to rewrite everything from scratch. Instead, we can extend the existing functionality of `Dog` to create a more specialized class.

```{python}
#| slideshow: {slide_type: slide}
#| vscode: {languageId: python}
class GuardDog(Dog):  # GuardDog inherits from Dog
    def __init__(self, name, breed, training_level): ## in addition to name and breed, we can define a training level. 
        # Call the parent (Dog) class's __init__ method
        super().__init__(name, breed)
        self.training_level = training_level  # New attribute for GuardDog that stores the training level for the dog

    def guard(self): ## checks if the training level is > 5 and if not says train more
        if self.training_level > 5:
            return f"{self.name} is guarding the house!"
        else:
            return f"{self.name} needs more training before guarding."
    
    def train(self): ## modifies the training_level attribute to increase the dog's training level
        self.training_level = self.training_level + 1
        return f"Training {self.name}. {self.name}'s training level is now {self.training_level}"

# Creating an instance of GuardDog
my_guard_dog = GuardDog("Rex", "German Shepherd", training_level= 5)
```

```{python}
#| slideshow: {slide_type: slide}
#| vscode: {languageId: python}
# Using methods from the base class
print(my_guard_dog.bark())  # Inherited from Dog -> Output: "Rex says Woof!"

# Using a method from the derived class
print(f"{my_guard_dog.name}'s training level is {my_guard_dog.training_level}.")
print(my_guard_dog.guard()) 

## if we want to train Rex and increase his training level, 
print(my_guard_dog.train())

## now check if he can guard 
print(my_guard_dog.guard()) 
```

As we see with Rex, child classes inherit all attributes (`.name` and `.breed`) and methods (`.bark()`) from parent classes. They can also have new methods (`.train()`).

### Mixins
A **mixin** is a special kind of class designed to add **functionality** to another class. Unlike base classes, mixins aren‚Äôt used alone.  

For example, scikit-learn uses mixins like:  
- `sklearn.base.ClassifierMixin` (adds classifier-specific methods)  
- `sklearn.base.RegressorMixin` (adds regression-specific methods)  

which it adds to the `BaseEstimator` class to add functionality.

To finish up our dog example, we are going to define a mixin class that adds a functionality to the base `Dog()` class which allows us to teach a dog tricks. 

```{python}
#| slideshow: {slide_type: slide}
#| vscode: {languageId: python}
class TrickMixin: ## mixin that will let us teach a dog tricks
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)  # Ensures proper initialization in multiple inheritance
        self.tricks = []  # Store learned tricks

    def learn_trick(self, trick):
        """Teaches the dog a new trick."""
        if trick not in self.tricks:
            self.tricks.append(trick)
            return f"{self.name} learned a new trick: {trick}!"
        return f"{self.name} already knows {trick}!"

    def perform_tricks(self):
        """Returns a list of tricks the dog knows."""
        if self.tricks:
            return f"{self.name} can perform: {', '.join(self.tricks)}."
        return f"{self.name} hasn't learned any tricks yet."

## note: the TrickMixin class is not a standalone class! it does not let us create a dog on its own!!!
```

Using this Trick mixin, we can then create a new class of dog (SmartDog) using both `Dog`and `TrickMixin` as base classes. 

```{python}
#| slideshow: {slide_type: slide}
#| vscode: {languageId: python}
class SmartDog(Dog, TrickMixin):
    def __init__(self, name, breed):
        super().__init__(name, breed)  # Initialize Dog class
        TrickMixin.__init__(self)  # Initialize TrickMixin separately

# Creating a SmartDog that can learn tricks
my_smart_dog = SmartDog("Buddy", "Border Collie")

# a SmartDog object can use methods from both parent object `Dog` and mixin `TrickMixin`.
print(my_smart_dog.bark())  # "Buddy says Woof!"
print(my_smart_dog.learn_trick("Sit"))  # "Buddy learned a new trick: Sit!"
print(my_smart_dog.learn_trick("Roll Over"))  # "Buddy learned a new trick: Roll Over!"
print(my_smart_dog.learn_trick("Sit"))  # "Buddy already knows Sit!"
print(my_smart_dog.perform_tricks())  # "Buddy can perform: Sit, Roll Over."
```

While our dog example was very simple, this is the same way that model classes work in python. 

## **OOP In ML Recap**   

Understanding **base classes** and **mixins** is especially important when working with deep learning frameworks like **PyTorch and TensorFlow**, as they allow for easy customization of models.  

By using **object-oriented programming**, Python makes it easy to **structure machine learning workflows** in a reusable and scalable way. The fact that all ML models in scikit-learn follow the **same structure** (with `.fit()`, `.predict()`, `.score()`, etc.) makes it easier to switch between models and automate processes. Additionally, the model classes in the statsmodels package have many of the same methods (`.fit()`, `.predict()`, `.score()`). 

## Part B - Demo Projects

### **üêß Mini Project: Classifying Penguins with scikit-learn**

Now that you understand **classes** and **data structures** in Python, let‚Äôs apply that knowledge!

In this project, we‚Äôll try to classify **penguin species** using two features:  
- `bill_length_mm`  
- `bill_depth_mm`  

We‚Äôll explore:
- **Unsupervised learning** with **K-Means** clustering (model doesn't 'know' y)
- **Supervised learning** with a **k-NN classifier** (model trained w/ y information)


## **üîç Modeling with scikit-learn Classes**

We'll use models from **scikit-learn**, which are built using object-oriented design.  
Each model is an **instance of a class** (inheriting from `BaseEstimator`) with:

**Common Methods:**
- `.fit()` ‚Äî Train the model  
- `.predict()` ‚Äî Make predictions

**Common Attributes:**
- `.get_params()`, `.classes_`, `.n_clusters_`, etc.

> We're using `scikit-learn` here for its simplicity, but the concepts apply to more advanced frameworks like **PyTorch** and **TensorFlow** too!

## General Modeling Workflow
0. Prepare Workspace
- Import libraries/modules
    * functions
    * classes
1. Data Preparation
- Load data (pandas)
- Clean data (pandas, numpy)
- Transform/Scale (sklearn.preprocessing)
- Split into training and test sets (if needed)
2. Initialize the model
- Create an instance of the model class (`KMeans`, `KNeighborsClassifier`)
    * Set parameters during instantiation
3. Fit the model
- Call `.fit(X_train, y_train)` or `.fit(X)`
4. Make predictions (optional)
- `.predict(X_test)`
- `.predict_proba()` for probabilities
5. Evaluate model performance
- Check predictions vs true values
- Visualize results

# Step 0: Import libraries

Before any analyses, we must first import the necessary libraries. 

For large libraries like scikit-learn (or PyTorch, TensorFlow), we usually do not import the entire library. Instead, we import particular classes and functions that we plan to use. Typically, things in camel case (first word lowercase, subsequent words in capital letters) are classes and things in snake case (underscores between words) are functions. 

In this case, we need:
- **Classes:**
    - `StandardScaler`
    - `KNeighborsClassifier`
    - `KMeans`

- **Functions**
    - `train_test_split()`
    - `accuracy_score()`
    - `classification_report()`
    - `adjusted_rand_score()`

```{python}
#| slideshow: {slide_type: slide}
#| vscode: {languageId: python}
import pandas as pd
import numpy as np

from plotnine import *
import seaborn as sns
import matplotlib.pyplot as plt

from great_tables import GT
from tabulate import tabulate

## import classes
from sklearn.preprocessing import StandardScaler 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans

## import functions
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, adjusted_rand_score

```

## Step 1: Data Preparation

```{python}
#| vscode: {languageId: python}
# Load the Penguins dataset
penguins = sns.load_dataset("penguins").dropna()

# Make a summary table for the penguins dataset, grouping by species. 
summary_table = penguins.groupby("species").agg({
    "bill_length_mm": ["mean", "std", "min", "max"],
    "bill_depth_mm": ["mean", "std", "min", "max"],
    "flipper_length_mm": ["mean", "std", "min", "max"],
    "body_mass_g": ["mean", "std", "min", "max"],
    "sex": lambda x: x.value_counts().to_dict()  # Count of males and females -> can include a dictionary in a table...
})

# Round numeric values to 1 decimal place (excluding the 'sex' column)
for col in summary_table.columns:
    if summary_table[col].dtype in [float, int]:
        summary_table[col] = summary_table[col].round(1)

# Display the result
print(summary_table)
```

2. Framing the Question

Goal:
We want to determine if the natural clusters of penguins, based on bill length/bill depth ratio are a good predictor of species.

To do this, we will:
    1. Cluster the dataset using k-Means (https://scikit-learn.org/stable/modules/clustering.html#k-means) and see if clusters match species labels.
    2. Classify species using k-NN (https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification) and evaluate its predictive accuracy.
    3. Compare both approaches and discuss their effectiveness.


Part 1: Clustering using K-means

In this case we want to cluster the dataset using the features listed and see if the resulting clusters correspond to the species labels. Because we are just clustering the dataset and not doing any prediction, we do not need to split the data into a training and test set. However, we do need to scale the features to allow for comparison and not over-weight any particular feature. 


## Scaling the data - Understanding the Standard Scaler class

For our clustering to work well, the predictors should be on the same scale.

To achieve this, we use an instance of the `StandardScaler` class. 


### Standard Scaler

```python
class sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)
```
*Parameters* are supplied by user  
- copy, with_mean, with_std

*Attributes* contain the `data` of the object  
- `scale_`: scaling factor
- `mean_`: mean value for each feature
- `var_`: variance for each feature
- `n_features_in_`: number of features seen nduring fit
- `n_samples_seen`: number of samples processed for each feature

*Methods* describe the `behaviors` of the object and/or `modify` its attributes  
- `fit(X)` -> compute mean and std used for scaling -> fit scaler to data X
    * updates the attributes of the scaler object
- `transform(X)` -> perform standardization by centering and scaling with fitted scaler

## Data Preparation

```{python}
#| vscode: {languageId: python}
## Data Preparation
# Selecting features for clustering -> let's just use bill length and bill depth.
X = penguins[["bill_length_mm", "bill_depth_mm"]]
y = penguins["species"]

# Standardizing the features for better clustering performance
scaler = StandardScaler() ## create instance
X_scaled = scaler.fit_transform(X) ## same as calling scaler.fit(X) then X_scaled = scaler.transform(X)
```

## Understanding the KMeans model class

```python
class sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')

```
*Parameters*: Set by user at time of instantiation
- n_clusters, max_iter, algorithm  
*Attributes*  
- `cluster_centers_`: stores coordinates of cluster centers  
- `labels_`: stores labels of each point 
- `n_iter_`: number of iterations run (will be changed during method run)  
- `n_features_in` and `feature_names_in_`: store info about features seen during fit  
*Methods*    
- `fit(X)` -> same as usage as `train()` -> fit model to data X  
- `predict(X)` -> predict closest cluster each sample in X belongs to  
- `transform(X)` -> transform X to cluster-distance space  

## Step 2: Create model

```{python}
#| vscode: {languageId: python}
## Perform K-means clustering 
# Applying k-Means clustering with 3 clusters (since we have 3 species)
kmeans = KMeans(n_clusters=3, random_state=42) ## make an instance of the K means class and perform clustering. 
```

## Step 3: Fit model to data

```{python}
#| vscode: {languageId: python}
## the fit
penguins["kmeans_cluster"] = kmeans.fit_predict(X_scaled)

## now that we fit the model, we can check some of the attributes
print("Coordinates of cluster centers:", kmeans.cluster_centers_)
```

## Step 5: Visualize and Evaluate

To do visualization, we can use either seaborn or plotnine. Plotnine is nice because it is just a python port of ggplot!

```{python}
#| slideshow: {slide_type: slide}
#| vscode: {languageId: python}
# Plotnine scatterplot of species by bill length/depth
plot1 = (ggplot(penguins, aes(x="bill_length_mm", y="bill_depth_mm", color="species"))
 + geom_point()
 + ggtitle("Penguin Species")
 + theme_bw())

display(plot1)
```

```{python}
#| vscode: {languageId: python}
# Plotnine scatterplot of k-Means clusters
plot2 = (ggplot(penguins, aes(x="bill_length_mm", y="bill_depth_mm", color="factor(kmeans_cluster)"))
 + geom_point()
 + ggtitle("K-Means Clustering Results")
 + theme_bw())

display(plot2)
```

```{python}
#| vscode: {languageId: python}
# Calculate clustering performance using Adjusted Rand Index (ARI) -> maybe change this or describe what it means. 
kmeans_ari = adjusted_rand_score(penguins['species'], penguins["kmeans_cluster"])
print(f"k-Means Adjusted Rand Index: {kmeans_ari:.2f}")
```

### We can also use methods on our data structure to create new data
- use this to plot cluster agreement with species label

```{python}
#| slideshow: {slide_type: fragment}
#| vscode: {languageId: python}
# Count occurrences of each species-cluster-sex combination( .size gives the count as index, use reset_index to get count column. )
scatter_data = penguins.groupby(["species", "kmeans_cluster", "sex"]).size().reset_index(name="count")

# Create a heatmap of the cluster assignments by species
heatmap_plot = (
    ggplot(scatter_data, aes(x="species", y="kmeans_cluster", fill="count"))
    + geom_tile(color="white")  # Add white grid lines for separation
    + scale_fill_gradient(low="lightblue", high="darkblue")  # Heatmap colors
    + labs(
        title="Heatmap of KMeans Clustering by Species",
        x="Species",
        y="KMeans Cluster",
        fill="Count"
    )
    + theme_bw()
)

# Display the plot
display(heatmap_plot)
```

```{python}
#| slideshow: {slide_type: slide}
#| vscode: {languageId: python}
## if we want to add sex to see if that is why we have imperfect clusters, can use a scatterplot
# Define the scatter plot using plotnine -> syntax like R ggplot except for parentheses and must use strings instead of just column names
# Create the scatter plot
scatter_plot = (
    ggplot(scatter_data, aes(x="species", y="kmeans_cluster", color="species", shape="sex", size="count"))
    + geom_point(alpha=0.7, position=position_dodge(width=0.5))  # Horizontal separation
    + scale_size(range=(2, 10))  # Adjust point sizes
    + scale_y_continuous(breaks=[0, 1, 2])  # Set y-axis ticks to only 0, 1, 2
    + theme_bw()
    + labs(
        title="KMeans Clustering vs Species (Size = Count)",
        x="Species",
        y="KMeans Cluster"
    )
)
#Display the plot
display(scatter_plot)
```

## Project 2 -> KNN classification

For our KNN classification, the model is supervised (meaning it is dependent on the outcome 'y' data) and therefore we need to split into a training and test set. 

The **function** train_test_split() from scikit-learn is helpful here! Our classifier object has built in methods for fitting models and predicting.


```{python}
#| slideshow: {slide_type: fragment}
#| vscode: {languageId: python}
# Splitting dataset into training and testing sets (still using scaled X!)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
```

## Understanding KNeighborsClassifier class
*PARAMETERS*  
*ATTRIBUTES*  
*METHODS*  
- `.fit(X, y)` -> fit knn classifier from training dataset (X and y)
- `.predict(X)` -> predict class labels for provided data X
- `.predict_proba(X)` -> return probability estimates for test data X
- `.score(X, y)` -> return mean accuracy on given test data X and labels y

```{python}
#| slideshow: {slide_type: fragment}
#| vscode: {languageId: python}
## perform knn classification
# Applying k-NN classification with 5 neighbors
knn = KNeighborsClassifier(n_neighbors=5) ## make an instance of the KNeighborsClassifier class and set the n_neighbors parameter to be 5. 

# Use the fit method to fit the model to the training data
knn.fit(X_train, y_train)
```

```{python}
#| vscode: {languageId: python}
## once the model is fit, we can look at its attributes, like '.classes_' which gives the class labels as known to the classifier

knn.classes_

# Use the predict method on the test data to get the predictions for the test data
y_pred = knn.predict(X_test)

# Also can take a look at the prediction probabilities, and use the .classes_ attribute to put the column labels in the right order
probs = pd.DataFrame(
    knn.predict_proba(X_test),
    columns = knn.classes_)
probs['y_pred'] = y_pred

print("Predicted probabilities: \n", probs.head())
```

# Plotnine scatterplot for k-NN classification of test data
## First unscale the test data
X_test_unscaled = scaler.inverse_transform(X_test)

## create dataframe 
penguins_test = pd.DataFrame(
    X_test_unscaled,
    columns=['bill_length_mm', 'bill_depth_mm']
)

## add actual and predicted species 
penguins_test['y_actual'] = y_test.values
penguins_test['y_pred'] = y_pred
penguins_test['correct'] = penguins_test['y_actual'] == penguins_test['y_pred']

```{python}
#| slideshow: {slide_type: slide}
#| vscode: {languageId: python}


## plot
plot3 = (ggplot(penguins_test, aes(x="bill_length_mm", y="bill_depth_mm", color="y_actual", fill = 'y_pred', size = 'correct'))
 + geom_point()
 + scale_size_manual(values={True: 2, False: 5})
 + ggtitle("k-NN Classification Results")
 + theme_bw())

display(plot3)
```

```{python}
#| slideshow: {slide_type: slide}
#| vscode: {languageId: python}
## eval knn performance
# Calculate accuracy and print classification report -> accuracy_score is a function! 
knn_accuracy = accuracy_score(y_test, y_pred)
print(f"k-NN Accuracy: {knn_accuracy:.2f}")
print(classification_report(y_test, y_pred))
```

```{python}
#| slideshow: {slide_type: slide}
#| vscode: {languageId: python}
##  making a summary table
# Creating a summary table
summary_table = pd.DataFrame({
    "Metric": ["k-Means Adjusted Rand Index", "k-NN Accuracy"],
    "Value": [kmeans_ari, knn_accuracy]
})
print(summary_table)
```


### **Key Takeaways from This Session**  
üîπ **Machine learning models in Python are objects** ‚Äì you create an instance of a class. 
üîπ We use a combination of functions and objects for this...
üîπ **OOP enables code reuse and modularity** ‚Äì `.fit()`, `.predict()`, and `.transform()` are standard across models.  
üîπ **Understanding inheritance and mixins** is important for customizing models in PyTorch and TensorFlow.  

Python‚Äôs OOP approach makes machine learning **efficient, reusable, and scalable**. üöÄ  

### **Pre-Reading for This Session**  
- [Scikit-learn Documentation](https://scikit-learn.org/stable/user_guide.html)  
- [Introduction to OOP in Python (Real Python)](https://realpython.com/python3-object-oriented-programming/)  
- [Plotnine Reference](https://plotnine.org/reference/)

