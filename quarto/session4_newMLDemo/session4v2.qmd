---
title: '**Session 4 – Object-Oriented Programming and Modeling Libraries**'
jupyter: python3
format: 
    revealjs:
        code-copy: true   
        smaller: true
        code-block-height: 750px
        highlight-style: pygments
        width: 1400   # default is 960
        height: 800  # default is 700
        css: slideshowv2.scss
execute:
  freeze: auto
  eval: true
  echo: true
---

## Session Overview

<br>

<p style="font-size: 1.15em">
In this session, we'll explore how Python's object-oriented nature affects our modeling workflows. 
</p><br>

<p style="font-size: 1.25em">
<strong>Topics:</strong>
</p>
<ul style="font-size: 1.15em; line-height: 1.6; max-width: 700px; margin: 0 auto;">
  <li><strong>Intro to OOP</strong> and how it makes modeling in Python different from R</li>
  <li><strong>Building and extending classes</strong> using inheritance and mixins</li>
  <li><strong>Applying OOP to machine learning</strong> through demos with scikit-learn</li>
</ul>


# Introduction

## Why Python? 🐍

::: columns
::: column

#### R: Built by Statisticians for Statisticians
- Excels at:
    - Statistical analysis and modeling  
    - Clean outputs and tables from models
    - Beautiful data visualizations with simple code  


:::

::: column

#### Python: General-Purpose Language
- Excels at: 
    - Machine Learning, Neural Networks & Deep Learning (scikit-learn, PyTorch, TensorFlow)   
    - Image & Genomic Data Analysis (scikit-image, biopython, scanpy)
    - Software & Command Line Interfaces, Web Scraping, Automation

:::
:::


Python’s broader ecosystem makes it the go-to language in domains like AI, bioinformatics, data engineering, and computational biology.  

> **Note:** Packages like `rpy2` and `reticulate` make it possible to use both R and Python in the same project, but those are beyond the scope of this course.  
> A primer on `reticulate` is available here: [https://www.r-bloggers.com/2022/04/getting-started-with-python-using-r-and-reticulate/](https://www.r-bloggers.com/2022/04/getting-started-with-python-using-r-and-reticulate/)

## Programming Styles: R vs Python
<br>
In the first session, we talked briefly about functional vs object-oriented programming:   
<br>

>  <span style="color: #007acc"><strong>Functional programming:</strong></span> focuses on functions as the primary unit of code <br>
>  <span style="color: #007acc"><strong>Object-oriented programming:</strong></span> uses objects with attached attributes(data) and methods(behaviors) <br>

- R leans heavily on the functional paradigm — you pass data into functions and get back results, in most cases without altering the original data. Functions and pipes (%>%) dominate most workflows.

- In Python, <span style="color: #007acc"><strong>everything is an object</strong></span>, even basic things like lists, strings, and dataframes, and a lot of 'functions' are written as object-associated methods. Some of these methods modify the objects in-place by altering the attached data (attributes). <span style="color: #007acc; font-weight:bold">Understanding how this works is key to using Python effectively!</span>

> You’ve already seen this object-oriented style in Sessions 2 and 3 — you create objects like lists or dataframes, then call methods on them like `.append()` or `.sort_values()`. In python, instead of piping, we sometimes chain methods together.

## Functions vs Objects in Python
<br>

<span style="color: #007acc"><strong>Python absolutely uses **functions**—just like R!</strong></span>
They're helpful for **data transformation**, **wrangling**, and **automation tasks** like looping and parallelization. <br>

But when it comes to **modeling**, libraries are designed around **classes**: blueprints for creating objects that store data (**attributes**) and define behaviors (**methods**).  <br>

  - `scikit-learn` is great for getting started—everything from regression to clustering follows a simple, consistent OOP interface. Its API is also consistant with other python modeling packages, like [xgboost](https://xgboost.readthedocs.io/en/release_3.0.0/) for gradient boosting and [scvi-tools](https://docs.scvi-tools.org/en/stable/index.html) for transcriptomics data.
  - `PyTorch` and `TensorFlow` are essential if you go deeper into neural networks or custom models—you’ll define your **own model classes** with attributes and methods, but the basic structure is similar to `scikit-learn`.  
  - [<u>`statsmodels`</u>](https://www.statsmodels.org/stable/gettingstarted.html) is an alternative to `scikit-learn` for statistical analyses and has R-like syntax and outputs. It's a bit more complex than `scikit-learn` and a bit less consistant with other packages in the python ecosystem.

> 💡 To work effectively in Python, especially for tasks involving modeling or model training, <span style="color: #007acc"><strong>it helps to think in terms of objects and classes, not just functions.</strong></span>   

## Why Does OOP Matter in Python Modeling?

<p style="font-size: 1.15em; margin-top: 0.5em; margin-bottom: 0.1em;">
<strong>In Python modeling frameworks:</strong>
</p>

<ul style="font-size: 1.05em; line-height: 1.15; margin-top: 0;">
  <li>Models are <strong>instances of classes</strong></li>
  <li>You call methods like <code>.fit()</code>, <code>.predict()</code>, <code>.score()</code></li>
  <li>Internal model details like coefficients or layers are stored as <strong>attributes</strong></li>
</ul>

<p style="font-size: 1.05em; margin-top: 0.5em; margin-bottom: 0.75em;">
This makes model behavior <strong>consistent</strong> and helps manage complexity of things like pipelines that are designed to work for multiple model classes. It also <strong>simplifies</strong> creating/using pre-trained models: both the architecture and learned weights are bundled into a single object with built-in methods like `.predict()` or `.fine_tune()`.
</p>

<blockquote style="font-size: 1.05em; margin-top: 1em;">
 If you're used to <code>lm()</code> or <code>glm()</code> in R returning a list of values, think of the Python approach as a self-contained object with named methods and stored results. Instead of having a separate results object, like in R, you would retrieve your results by accessing an attribute that is stored in the model object itself. 
</blockquote>
<br>

<span style="font-size: 1.15em;color: #007acc"><strong><em>We’ll focus on `scikit-learn` here for the sake of simplicity, but feel free to explore other libraries!</em></strong></span> <br>  
*[https://wesmckinney.com/book/modeling](https://wesmckinney.com/book/modeling) is a good tutorial for statsmodels.*


# Part 1: Object-Oriented Programming

## **Key OOP Principles (Recap)** 

In OOP, code is structured around **objects** (as opposed to functions). This paradigm builds off the following principles: 

. . .  

1. **Encapsulation**: Bundling data and methods together in a single unit.  
   - A `StandardScaler` object stores mean and variance data and has `.fit()` and `.transform()` methods

. . . 

2. **Inheritance**: Creating new classes based on existing ones.  
   - `sklearn.LinearRegression` inherits attributes and methods from a general regression model class.    

. . . 

3. **Abstraction**: Hiding implementation details and exposing only essential functionality.  
   - e.g., `.fit()` works the same way from the outside, regardless of model complexity  


. . . 

4. **Polymorphism**: Objects of different types can be treated the same way if they implement the same methods. 
    - Python’s **duck typing**:  
      - *"If it walks like a duck and quacks like a duck, then it must be a duck."*  
      - ex: If different objects all have a .summarize() method, we can loop over them and call .summarize() without needing to check their class. As long as the method exists, Python will know what to do.
      - This lets us easily create [<u>pipelines</u>](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) that can work for many types of models. 

> We won't cover [pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) here, but they are worth looking into!


## **Classes and Objects**

<br>

**Classes** are **blueprints** for creating objects. Each object contains:  
<ul style="font-size: 1.05em; line-height: 1.6;">
  <li><strong>Attributes</strong> (data): model coefficients, class labels</li>
  <li><strong>Methods</strong> (behaviors): <code>.fit()</code>, <code>.predict()</code></li>
</ul>

<br>

👉 To check if an object is an instance of a particular class, use:

```{python}
#| eval: false
isinstance(object, class)  # Returns True if `object` is an instance of `class`.
```

<br>

<p style="color: #007acc; font-weight: 600;">
Knowing what class an object belongs to helps us understand what methods and attributes it provides.
</p>


# Example: Creating a Class

## Base Classes

A **base class** (or parent class) serves as a template for creating objects. Other classes can inherit from it to reuse its properties and methods.

Classes are defined using the `class` keyword, and their structure is specified using an `__init__()` method for initialization. 

. . . 

For example, we can define a class called `Dog` and give it attributes that store data about a given dog.    

We can also add methods that represent behaviors an object of the `Dog` class can perform:  

. . . 

```{python}
#| code-line-numbers: "|3,4|6-7|9-13"
class Dog: ## begin class definition
    def __init__(self, name, breed): ## define init method
        self.name = name ## add attributes
        self.breed = breed

    def speak(self): ## add methods
        return f"{self.name} says woof!"

    def __str__(self): ## add special methods, __str__(self) tells python what to display when an object is printed
        return f"Our dog {self.name}"

    def __repr__(self): ## add representation to display when dog is called in console
        return f"Dog(name={self.name!r}, breed={self.breed!r})"
```

## Creating a dog

Creating an instance (object) of the `Dog` class lets us model a particular dog:  

. . . 
```{python}
#| output-location: fragment
buddy = Dog("Buddy", "Golden Retriever")
buddy ## displays what was in the __repr__() method
```

. . .

Here, `buddy` is an **object** of the `Dog` class.     

It has **attributes** (`name`, `breed`) and **methods** (`speak()`).   

. . .  

When we make an instance of the Dog class:     
- We set the value of the attributes [`name` and `breed`], which are then stored as part of the `buddy` object  
- We can use any methods defined in the Dog class on `buddy`  

```{python}
#| output-location: fragment
#| code-line-numbers: "1-3|5-6|"
## if we want to see what kind of dog our dog is
## we can call buddy's attributes
print(f"Our dog {buddy.name} is a {buddy.breed}.")

## we can also call any Dog methods
print(buddy.speak())  
```

. . . 
 
<span style="color: #007acc"><strong>Note:</strong> For python methods, the `self` argument is assumed to be passed and therefore we do not put anything in the parentheses when calling `.speak()`.</span>


## Derived (Child) Classes  

Derived/child classes build on base classes using the principle of inheritence. <br>

Now that we have a `Dog` class, we can build on it to create a specialized `GuardDog` class. 

```{python}
#| code-line-numbers: "|1|2-7|"
class GuardDog(Dog):  # GuardDog inherits from Dog
    def __init__(self, name, breed, training_level): ## in addition to name and breed, we can 
        # define a training level. 
        # Call the parent (Dog) class's __init__ method
        super().__init__(name, breed)
        self.training_level = training_level  # New attribute for GuardDog that stores the 
        # training level for the dog

    def guard(self): ## checks if the training level is > 5 and if not says train more
        if self.training_level > 5:
            return f"{self.name} is guarding the house!"
        else:
            return f"{self.name} needs more training before guarding."
    
    def train(self): ## modifies the training_level attribute to increase the dog's training level
        self.training_level = self.training_level + 1
        return f"Training {self.name}. {self.name}'s training level is now {self.training_level}"

# Creating an instance of GuardDog
rex = GuardDog("Rex", "German Shepherd", training_level= 5)
```
---

Now that we have a dog (rex), we can call on any of the methods/attributes introduced in the `Dog` class as well as the new `GuardDog` class.
```{python}
#| slide-type: fragment
#| output-location: fragment
# Using methods from the base class
print(rex.speak())  # Inherited from Dog -> Output: "Rex says Woof!"
rex
```
<br>

```{python}
#| slide-type: fragment
#| output-location: fragment
# Using a method from the derived class
print(f"{rex.name}'s training level is {rex.training_level}.")
print(rex.guard()) 
```

. . . 

<span style="color: #007acc"><strong>This is the power of inheritance</span></strong>—we don’t have to rewrite everything from scratch!
---

Unlike standalone functions, methods in Python often update objects in-place—meaning they modify the object itself rather than returning a new one.

```{python}
#| slide-type: fragment
#| output-location: fragment
## we can use the `train` method to increase rex's training level, 
print(rex.train())
```

. . .

```{python}
#| slide-type: fragment
#| output-location: fragment
## now check if he can guard 
print(rex.guard()) 
```
<br>

. . . 

As we saw with Rex, <span style="color: #007acc"><strong>child classes inherit all attributes (`.name` and `.breed`) and methods (`.speak()` `__repr__()`) from parent classes.</strong></span> They can also have new methods (`.train()`).

## Mixins
A **mixin** is a special kind of class designed to add **functionality** to another class. Unlike base classes, mixins aren’t used alone.  

. . . 

For example, scikit-learn uses mixins like:  
- `sklearn.base.ClassifierMixin` (adds classifier-specific methods)  
- `sklearn.base.RegressorMixin` (adds regression-specific methods)  

which it adds to the `BaseEstimator` class to add functionality.

To finish up our dog example, we are going to define a mixin class that adds a functionality to the base `Dog()` class which allows us to teach a dog tricks. 

---

- When creating a mixin class, we let the other base classes carry most of the initialization
- However, we can add other attributes like `.tricks`, which stores the set of learned tricks
- We can also add methods to define additional behaviors

```{python}
#| code-line-numbers: "|2,3|4|6-17|"
class TrickMixin: ## mixin that will let us teach a dog tricks
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)  # Ensures proper initialization in multiple inheritance
        self.tricks = []  # Store learned tricks

    def learn_trick(self, trick):
        """Teaches the dog a new trick."""
        if trick not in self.tricks:
            self.tricks.append(trick)
            return f"{self.name} learned a new trick: {trick}!"
        return f"{self.name} already knows {trick}!"

    def perform_tricks(self):
        """Returns a list of tricks the dog knows."""
        if self.tricks:
            return f"{self.name} can perform: {', '.join(self.tricks)}."
        return f"{self.name} hasn't learned any tricks yet."

## note: the TrickMixin class is not a standalone class! it does not let us create a dog on its own!!!
```
. . . 

Using this Trick mixin, we can then create a new class of dog (SmartDog) using both `Dog`and `TrickMixin` as base classes. 

---

By including both `Dog` and `TrickMixin` as base classes, we can give objects of class `SmartDog` the ability to speak and learn tricks!

```{python}
#| slide-type: fragment
#| output-location: fragment
class SmartDog(Dog, TrickMixin):
    def __init__(self, name, breed):
        super().__init__(name, breed)  # Initialize Dog class
        TrickMixin.__init__(self)  # Initialize TrickMixin separately
```

. . . 

```{python}
#| slide-type: fragment
#| output-location: fragment
# a SmartDog object can use methods from both parent object `Dog` and mixin `TrickMixin`.
my_smart_dog = SmartDog("Buddy", "Border Collie")
```

. . . 

```{python}
#| slide-type: fragment
#| output-location: fragment
print(my_smart_dog.speak()) 
```

. . . 

```{python}
#| slide-type: fragment
#| output-location: fragment
print(my_smart_dog.learn_trick("Sit"))  
print(my_smart_dog.learn_trick("Roll Over")) 
print(my_smart_dog.learn_trick("Sit"))  

```
. . . 

```{python}
#| slide-type: fragment
#| output-location: fragment
print(my_smart_dog.perform_tricks()) 
```

## Duck Typing 

>"If it quacks like a duck and walks like a duck, it's a duck."

Python doesn’t require explicit interfaces.
If an object implements the expected methods, it can be used interchangeably with other objects. This is called duck typing.

. . . 

We can demonstrate this by defining two new base classes that are different than `Dog` but also have a `speak()` method.

. . . 

```{python}
#| code-line-numbers: "|5-6, 12-13"
class Human:
    def __init__(self, name):
        self.name = name

    def speak(self):
        return f"{self.name} says hello!"

class Parrot:
    def __init__(self, name):
        self.name = name

    def speak(self):
        return f"{self.name} says squawk!"

```

## Duck Typing in Action

Even though `Dog`, `Human` and `Parrot` are entirely different classes...

```{python}
#| output-location: fragment
def call_speaker(obj):
    print(obj.speak())

call_speaker(Dog("Fido", "Labrador"))
call_speaker(Human("Alice"))
call_speaker(Parrot("Polly"))
```

. . .  

They all implement `.speak()`, so Python treats them the same!

In the context of our work, this would allow us to make a pipeline using models from different libraries that do not share a base class/mixin but have the same methods. 

---

<span style="color: #007acc"><strong>While our dog example was very simple, this is the same way that model classes work in python.</strong></span> 

## **OOP in Machine Learning and Modeling?**  

Machine learning models in Python are implemented as **classes**.    
    - When you create a model, you’re **instantiating an object** of a predefined class (e.g., `LogisticRegression()`).    
    - That model **inherits** attributes (parameters, coefficients) and methods (like `.fit()` and `.predict()`).    

--- 

For example, a **logistic regression model** in `scikit-learn` is an instance of the `LogisticRegression` class:
```{.python}
## Example: 
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()  # Creating an instance of the LogisticRegression class
model.fit(X_train, y_train)   # Calling a method to train the model
predictions = model.predict(X_test)  # Calling a method to make predictions
```
Here, **`model`** is an **object** that has inherited attributes and methods from `LogisticRegression`.  

## **Key Benefits of OOP in Machine Learning**  

1. **Encapsulation** – Models store parameters and methods inside a single object.  
2. **Inheritance** – New models can build on base models, reusing existing functionality.  
3. **Abstraction**  – `.fit()` should work as expected, regardless of complexity of underlying implimentation.
4. **Polymorphism (Duck Typing)** – Different models share the same method names (`.fit()`, `.predict()`), making them easy to use interchangeably, particularly in analysis pipelines. 

Understanding **base classes** and **mixins** is especially important when working with deep learning frameworks like **PyTorch and TensorFlow**, as they allow for easy customization of models.  

By using **object-oriented programming**, Python makes it easy to **structure machine learning workflows** in a reusable and scalable way. The fact that all ML models in scikit-learn follow the **same structure** (with `.fit()`, `.predict()`, `.score()`, etc.) makes it easier to switch between models and automate processes. Additionally, the model classes in the statsmodels package have many of the same methods (`.fit()`, `.predict()`, `.score()`). 


# Part B - Demo Projects

**Apply knowledge of OOP to modeling using scikit-learn**

---

## 🐧 Mini Project: Classifying Penguins with scikit-learn

Now that you understand **classes** and **data structures** in Python, let’s apply that knowledge!

In this project, we’ll try to classify **penguin species** using two features:  
- `bill_length_mm`  
- `bill_depth_mm`  

We’ll explore:  
- **Unsupervised learning** with **K-Means** clustering (model doesn't 'know' y)
- **Supervised learning** with a **k-NN classifier** (model trained w/ y information)


## Modeling with scikit-learn Classes

We'll use models from **scikit-learn**, which are built using object-oriented design.  
Each model is an **instance of a class** (inheriting from `BaseEstimator`) with:

**Common Methods:**  
- `.fit()` — Train the model  
- `.predict()` — Make predictions  

**Common Attributes:**  
- `.get_params()`, `.classes_`, `.n_clusters_`, etc.  

> We're using `scikit-learn` here for its simplicity, but the concepts apply to more advanced frameworks like **PyTorch** and **TensorFlow** too!

## General Modeling Workflow

::: columns
::: column

**Step 0: Prepare Workspace**  
- Import necessary libraries/modules:  
  - **Functions** (e.g., `train_test_split`, `accuracy_score`)  
  - **Classes** (e.g., `KMeans`, `KNeighborsClassifier`)  

**Step 1: Data Preparation**  
- Load data (`pandas`)  
- Clean data (`pandas`, `numpy`)  
- Transform/scale features (`sklearn.preprocessing`)  
- Optionally split data into training and testing sets  

**Step 2: Initialize the Model**  
- Create an instance of the model class (`KMeans`, `KNeighborsClassifier`)  
- Set parameters during instantiation (e.g., `n_clusters=3`, `n_neighbors=5`)  

:::

::: column

**Step 3: Fit the Model**  
- Use `.fit(X)` for unsupervised models  
- Use `.fit(X_train, y_train)` for supervised models  

**Step 4: Make Predictions** *(optional)*  
- Use `.predict(X_test)` to generate predictions  
- Use `.predict_proba()` to get class probabilities (if available)  

**Step 5: Evaluate Model Performance**  
- Compare predictions to true values  
- Use visualizations or metrics (e.g., accuracy, ARI, classification report)  

:::
:::


## Step 0: Import Libraries

<br>

<span style="color: #007acc"><strong>Before any analysis, we must import the necessary libraries.</strong></span>  

For large libraries like **scikit-learn**, **PyTorch**, or **TensorFlow**, we usually do **not** import the entire package. Instead, we selectively import the **classes** and **functions** we need.

. . . 

> 🔤 **Naming Tip**:  
> - `CamelCase` = **Classes**  
> - `snake_case` = **Functions**

--- 

In this project, we’ll use the following:

**Classes**  
- `StandardScaler` — for feature scaling  
- `KNeighborsClassifier` — for supervised k-NN classification  
- `KMeans` — for unsupervised clustering

<br>

**Functions**  
- `train_test_split()` — to split data into training and test sets  
- `accuracy_score()` — to evaluate classification accuracy  
- `classification_report()` — to print precision, recall, F1 (balance of precision and recall), Support (number of true instances per class)
- `adjusted_rand_score()` — to evaluate clustering performance


## Import Libraries

```{python}
#| code-line-numbers: "|12|14-17|19-21|"
## imports
import pandas as pd
import numpy as np

from plotnine import *
import seaborn as sns
import matplotlib.pyplot as plt

from great_tables import GT

## sklearn imports

## import classes
from sklearn.preprocessing import StandardScaler 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans

## import functions
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, adjusted_rand_score
```  

## Step 1: Data Preparation {.smaller}

```{python}
#| code-line-numbers: "1,2|4-11|13-16|18-19|"
#| output-location: slide
# Load the Penguins dataset
penguins = sns.load_dataset("penguins").dropna()

# Make a summary table for the penguins dataset, grouping by species. 
summary_table = penguins.groupby("species").agg({
    "bill_length_mm": ["mean", "std", "min", "max"],
    "bill_depth_mm": ["mean", "std", "min", "max"],
    "flipper_length_mm": ["mean", "std", "min", "max"],
    "body_mass_g": ["mean", "std", "min", "max"],
    "sex": lambda x: x.value_counts().to_dict()  # Count of males and females
})

# Round numeric values to 1 decimal place (excluding the 'sex' column)
for col in summary_table.columns:
    if summary_table[col].dtype in [float, int]:
        summary_table[col] = summary_table[col].round(1)

# Display the result
display(summary_table)
```


## Scaling the data - Understanding the Standard Scaler class

For our clustering to work well, the predictors should be on the same scale. <br>

To achieve this, we use an instance of the `StandardScaler` class. 


## Standard Scaler

```python
class sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)
```
<br>

. . .   

**Parameters** are supplied by user  
- copy, with_mean, with_std <br>

**Attributes** contain the `data` of the object  
- `scale_`: scaling factor  
- `mean_`: mean value for each feature  
- `var_`: variance for each feature  
- `n_features_in_`: number of features seen during fit  
- `n_samples_seen`: number of samples processed for each feature <br>

**Methods** describe the `behaviors` of the object and/or `modify` its attributes  
- `fit(X)` -> compute mean and std used for scaling -> fit scaler to data X  
    * updates the attributes of the scaler object  
- `transform(X)` -> perform standardization by centering and scaling with fitted scaler  

## Data Preparation

```{python}
#| code-line-numbers: "1-3|5-7|"
# Selecting features for clustering -> let's just use bill length and bill depth.
X = penguins[["bill_length_mm", "bill_depth_mm"]]
y = penguins["species"]

# Standardizing the features for better clustering performance
scaler = StandardScaler() ## create instance of StandardScaler
X_scaled = scaler.fit_transform(X) ## same as calling scaler.fit(X) then X_scaled = scaler.transform(X)
```

## Understanding the KMeans model class

```python
class sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, 
tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')
```
<br>

**Parameters**: Set by user at time of instantiation  
- n_clusters, max_iter, algorithm  <br>

**Attributes**: Store object data    
- `cluster_centers_`: stores coordinates of cluster centers  
- `labels_`: stores labels of each point 
- `n_iter_`: number of iterations run (will be changed during method run)  
- `n_features_in` and `feature_names_in_`: store info about features seen during fit  <br>

**Methods**: Define object behaviors      
- `fit(X)` -> same as usage as `train()` -> fit model to data X  
- `predict(X)` -> predict closest cluster each sample in X belongs to  
- `transform(X)` -> transform X to cluster-distance space  

---

### Step 2: Create model

```{python}
## Choosing 3 clusters b/c we have 3 species
kmeans = KMeans(n_clusters=3, random_state=42) ## make an instance of the K means class
```

### Step 3: Fit model to data

```{python}
#| code-line-numbers: "1-2|4-5|"
#| output-location: fragment
## the fit
penguins["kmeans_cluster"] = kmeans.fit_predict(X_scaled)

## now that we fit the model, we should have cluster centers
print("Coordinates of cluster centers:", kmeans.cluster_centers_)
```

## Step 5: Visualize and Evaluate

To do visualization, we can use either seaborn or plotnine. `plotnine` mirrors `ggplot2` syntax from R and is great for layered grammar-of-graphics plots, while `seaborn` works directly with **numpy arrays** and **pandas DataFrames**. It builds on top of matplotlib objects—which are themselves instances of classes. Most built-in plotting methods in Python packages use matplotlib.<br>

`seaborn` is often used for quick statistical visualizations in python and is more convienient if you want to put multiple plots on the same figure.

## Plotting with plotnine vs seaborn

::: columns
::: column

### **plotnine (like ggplot2 in R)**  
The biggest differences between `plotnine` and `ggplot2` syntax are: 
- With `plotnine` the whole call is wrapped in `()` parentheses
- Variables are called with strings (`""` are needed!)
- If you don't use `from plotnine import *`, you will need to import each individual function you plan to use!

:::

::: column

### **seaborn (base matplotlib + enhancements)**  
- Designed for **quick, polished plots**
- Works well with **pandas DataFrames** or **NumPy arrays**
- Integrates with `matplotlib` for customization
- Often used in ML for things like **decision boundaries** or **heatmaps**

:::
:::


## Scatterplot with plotnine

To take at the distribution of our species by bill length and bill depth...

```{python}
#| output-location: slide
# Plotnine scatterplot of species by bill length/depth
plot1 = (ggplot(penguins, aes(x="bill_length_mm", y="bill_depth_mm", color="species"))
 + geom_point()
 + ggtitle("Penguin Species")
 + theme_bw())

display(plot1)
```

## Scatterplot with seaborn

We can also create a plot for the K-means clustering results. This time, we'll use seaborn.

```{python}
#| output-location: slide
# Create the figure and axes
fig, ax = plt.subplots(figsize=(8, 6))

# Save seaborn scatterplot to variable plot1 by drawing it on ax
plot1 = sns.scatterplot(
    data=penguins,
    x="bill_length_mm",
    y="bill_depth_mm",
    hue="species",
    style="sex",             # optional
    palette="Set2",
    edgecolor="black",
    s=100,
    ax=ax                    # draw on the axes object
)

# Add decorations to the same Axes
ax.set_title("Penguin Bill Length vs Depth by Species")
ax.set_xlabel("Bill Length (mm)")
ax.set_ylabel("Bill Depth (mm)")
ax.legend(title="Species")

# Plot the figure
fig.tight_layout() 
```

## Use function to calculate ARI

To check how good our model is, we can use one of the functions included in the sklearn library

The `adjusted_rand_score()` function evaluates how well the cluster groupings agree with the species groupings while adjusting for chance. 

```{python}
#| output-location: fragment
# Calculate clustering performance using Adjusted Rand Index (ARI)
kmeans_ari = adjusted_rand_score(penguins['species'], penguins["kmeans_cluster"])
print(f"k-Means Adjusted Rand Index: {kmeans_ari:.2f}")
```

---

### We can also use methods on our data structure to create new data

- We can use the `.groupby()` method to help us plot cluster agreement with species label as a heatmap
- If we want to add sex as a variable to see if that is why our clusters don't agree with our species, we can use a scatterplot
- Using seaborn and matplotlib, we can easily put both of these plots on the same figure. 
<br>


Setting up data... <br>
```{python}
# Count occurrences of each species-cluster-sex combination( .size gives the count as index, use reset_index to get count column. )
scatter_data = penguins.groupby(["species", "kmeans_cluster", "sex"]).size().reset_index(name="count")
# Create a mapping to add horizontal jitter for each sex for scatterplot
sex_jitter = {'Male': -0.1, 'Female': 0.1}
scatter_data['x_jittered'] = scatter_data.apply(
    lambda row: scatter_data['species'].unique().tolist().index(row['species']) + sex_jitter.get(row['sex'], 0),
    axis=1
)
heatmap_data = scatter_data.pivot_table(index="kmeans_cluster", columns="species", values="count", aggfunc="sum", fill_value=0)
```

---

#### Creating Plots ....

```{python}
#| output-location: slide
# Prepare the figure with 2 subplots the fig2 object represents the overall figure the axes object will contain both plots
fig2, axes = plt.subplots(1, 2, figsize=(14, 6)) ## 1 row 2 columns

# Plot heatmap on the first axis
sns.heatmap(data = heatmap_data, cmap="Blues", linewidths=0.5, linecolor='white', annot=True, fmt='d', ax=axes[0])
axes[0].set_title("Heatmap of KMeans Clustering by Species")
axes[0].set_xlabel("Species")
axes[0].set_ylabel("KMeans Cluster")

# Scatterplot with jitter
sns.scatterplot(data=scatter_data, x="x_jittered", y="kmeans_cluster",
    hue="species", style="sex", size="count", sizes=(50, 300),
    alpha=0.8, ax=axes[1], legend="brief"
)

# Fix the x-axis ticks and legend
species_order = list(scatter_data['species'].unique())
axes[1].set_xticks(range(len(species_order)))
axes[1].set_xticklabels(species_order)
axes[1].set_title("Cluster Assignment by Species and Sex (Jittered)")
axes[1].set_ylabel("KMeans Cluster")
axes[1].set_xlabel("Species")
axes[1].set_yticks([0, 1, 2])
axes[1].legend(bbox_to_anchor=(1.05, 0.5), loc='center left', borderaxespad=0.0, title="Legend")

fig2.tight_layout()
```

## Project 2 -> KNN classification

For our KNN classification, the model is supervised (meaning it is dependent on the outcome 'y' data) and therefore we need to split our data into a training and test set. <br>

. . . 

The **function** `train_test_split()` from scikit-learn is helpful here! Our classifier object has built in methods for fitting models and predicting.

```{python}
# Splitting dataset into training and testing sets (still using scaled X!)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
```

. . . 

>Unlike R functions, which return a single object (often a list when multiple outputs are needed), Python functions can return multiple values as a tuple—letting you unpack them directly into separate variables.

## Understanding KNeighborsClassifier class
```{.python}
class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', 
algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
```
. . . 

**Parameters**: Set by user at time of instantiation    
- n_neigbors, weights, algorithm, etc.  <br>

**Attributes**: Store object data     
- `classes_`: class labels known to the classifier  
- `effective_metric_`: distance metric used  
- `effective_metric_params_`: parameters for the metric function  
- `n_features_in` and `feature_names_in_`: store info about features seen during fit    
- `n_samples_fit_`: number of samples in fitted data  <br> 

**Methods**: Define object behaviors      
- `.fit(X, y)` -> fit knn classifier from training dataset (X and y)  
- `.predict(X)` -> predict class labels for provided data X  
- `.predict_proba(X)` -> return probability estimates for test data X  
- `.score(X, y)` -> return mean accuracy on given test data X and labels y  

---

### Making an instance of KNeighborsClassifier and fitting to training data
- For a supervised model, y_train is included in `.fit()`!
```{python}
#| output: false
## perform knn classification
# Applying k-NN classification with 5 neighbors
knn = KNeighborsClassifier(n_neighbors=5) ## make an instance of the KNeighborsClassifier class
# and set the n_neighbors parameter to be 5. 

# Use the fit method to fit the model to the training data
knn.fit(X_train, y_train)
```

## Once the model is fit...

- Once the model is fit, we can look at its attributes (ex: `.classes_`) which gives the class labels as known to the classifier

```{python}
#| output-location: fragment
print(knn.classes_)
```

## Next Step: Use fitted model to predict species for test data

```{python}
#| output-location: fragment
# Use the predict method on the test data to get the predictions for the test data
y_pred = knn.predict(X_test)

# Also can take a look at the prediction probabilities, 
# and use the .classes_ attribute to put the column labels in the right order
probs = pd.DataFrame(
    knn.predict_proba(X_test),
    columns = knn.classes_)
probs['y_pred'] = y_pred

print("Predicted probabilities: \n", probs.head())
```

## Scatterplot for k-NN classification of test data

- Create dataframe of unscaled X_test, `bill_length_mm`, and `bill_depth_mm`.
- Add to it the actual and predicted species labels

```{python}
## First unscale the test data
X_test_unscaled = scaler.inverse_transform(X_test)

## create dataframe 
penguins_test = pd.DataFrame(
    X_test_unscaled,
    columns=['bill_length_mm', 'bill_depth_mm']
)

## add actual and predicted species 
penguins_test['y_actual'] = y_test.values
penguins_test['y_pred'] = y_pred
penguins_test['correct'] = penguins_test['y_actual'] == penguins_test['y_pred']
```

## Plotnine scatterplot for k-NN classification of test data

```{python}
#| output-location: slide
## Build the plot
plot3 = (ggplot(penguins_test, aes(x="bill_length_mm", y="bill_depth_mm", 
color="y_actual", fill = 'y_pred', size = 'correct'))
 + geom_point()
 + scale_size_manual(values={True: 2, False: 5})
 + ggtitle("k-NN Classification Results")
 + theme_bw())

display(plot3)
```

## Visualizing Decision Boundary with seaborn and matplotlib

```{python}
#| code-line-numbers: "1-8|10-18|20-22|24-34|36-41|"
#| output-location: slide
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.preprocessing import LabelEncoder

# Create and fit label encoder for y (just makes y numeric because it makes the scatter plot happy)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Create the plot objects
fig, ax = plt.subplots(figsize=(8, 6))

# Create display object
disp = DecisionBoundaryDisplay.from_estimator(
    knn,
    X_test,
    response_method = 'predict',
    plot_method = 'pcolormesh',
    xlabel = "bill_length_scaled",
    ylabel = "bill_depth_scaled",
    shading = 'auto',
    alpha = 0.5,
    ax = ax
)

# Use method from display object to create scatter plot
scatter = disp.ax_.scatter(X_scaled[:,0], X_scaled[:,1], c=y_encoded, edgecolors = 'k')
disp.ax_.legend(
    scatter.legend_elements()[0],
    knn.classes_,
    loc = 'lower left',
    title = 'Species'
    
)
_ = disp.ax_.set_title("Penguin Classification")

plt.show()
```

## Evaluate KNN performance

```{python}
#| output-location: fragment
## eval knn performance
# Calculate accuracy and print classification report -> 
# accuracy_score and classification_report are functions! 
knn_accuracy = accuracy_score(y_test, y_pred)
print(f"k-NN Accuracy: {knn_accuracy:.2f}")
print(classification_report(y_test, y_pred))
```

## Make a Summary Table of Metrics for Both Models

```{python}
#| output-location: fragment
##  making a summary table
# Creating a summary table
summary_table = pd.DataFrame({
    "Metric": ["k-Means Adjusted Rand Index", "k-NN Accuracy"],
    "Value": [kmeans_ari, knn_accuracy]
})
GT(summary_table).show() ## round the values!!!!!
```


## Key Takeaways from This Session

<p style="font-size: 1.25em"> 
</p>
<ul style="font-size: 1.1em; line-height: 1.6; max-width: 700px; margin: 0 auto;">
  <li><strong>Python workflows rely on object-oriented structures in addition to functions:</strong><br>
   Understanding the OOP paradigm makes Python a lot easier!</li>
  <li><strong>Everything is an object!</strong></li>
  <li><strong>Duck Typing:</strong><br>
  If an object has a method, that method can be called regardless of the object type. Caveat being, make sure the arguments (if any) in the method are specified correctly for all objects!
  </li>
    <li>Python packages use <strong>common methods</strong> that make it easy to change between model types without changing a lot of code. 
  </li>
</ul>

## Additional Insights

<div style="font-size: 1.1em; max-width: 900px; margin: 1em auto 0 auto; line-height: 1.7;">
  <ul>
    <li>
      <strong>Predictable APIs enable seamless model switching:</strong><br>
      Swapping models like <code>LogisticRegression</code> → <code>RandomForestClassifier</code> usually requires minimal code changes.
    </li>
    <li style="margin-top: 1em;">
      <strong>scikit-learn prioritizes interoperability:</strong><br>
      Its consistent class design integrates with tools like <code>Pipeline</code>, <code>GridSearchCV</code>, and <code>cross_val_score</code>.
    </li>
    <li style="margin-top: 1em;">
      <strong>Class attributes improve model transparency:</strong><br>
      Access attributes like <code>.coef_</code>, <code>.classes_</code>, and <code>.feature_importances_</code> for model interpretation and debugging.
    </li>
    <li style="margin-top: 1em;">
      <strong>Custom classes are central to deep learning:</strong><br>
      Frameworks like PyTorch and TensorFlow require you to define your own model classes by subclassing base models.
    </li>
    <li style="margin-top: 1em;">
      <strong>Mixins support modular design:</strong><br>
      Mixins (e.g., <code>ClassifierMixin</code>) let you add specific functionality without duplicating code.
    </li>
  </ul>
</div>

## **Pre-Reading for This Session**  
- [Scikit-learn Documentation](https://scikit-learn.org/stable/user_guide.html)  
- [Introduction to OOP in Python (Real Python)](https://realpython.com/python3-object-oriented-programming/)  
- [Plotnine Reference](https://plotnine.org/reference/)

