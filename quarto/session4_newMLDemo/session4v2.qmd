---
title: '**Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries**'
jupyter: python3
format: 
    revealjs:
        smaller: true
        code-block-height: 650px
        highlight-style: pygments
        width: 1400   # default is 960
        height: 650   # default is 700
execute:
  freeze: auto
  eval: true
  echo: true
---

---

## Session Overview

This session is divided into two parts:  
1. **A quick recap of Object-Oriented Programming (OOP)** and why it‚Äôs useful.  
2. **Applying OOP concepts to machine learning** by building models in scikit-learn.

---

## Introduction

Both R and python use objects, but not everything in R is object-oriented... If that sounds confusing that's because it is! 
<br>

**Functional programming:** focuses on functions as the primary unit of code
<br>

**Object-oriented programming:** uses objects with attached attributes(data) and methods(behaviors)
<br>

Functional and object-oriented programming are *paradigms* (styles) and these styles can be applied in both R and Python. However, Python libraries and workflows tend to rely more on object-oriented programming than those designed for R. 

<br>

R originated from another statistical programming language called S, which is not object-oriented, and R tends to lend itself better to functional programming than object-oriented programming in many cases. 

## Why Python? üêç

::: columns
::: column

### R: Built by Statisticians  
- Excels at statistical analysis and modeling  
- Beautiful data visualizations with simple code  

:::

::: column

### Python: A General-Purpose Language (like C++, Java)
- Excels at deep learning, image analysis, and NLP  
- Also great for:  
  - Automation  
  - Software & CLI (command line interface) development  
  - Web development  

:::
:::

<br>

Python is a good go-to for many things that R isn't quite suited for and is very flexible. 
There are also packages like `rpy2` and `reticulate` that make it easy to use both R and Python in the same project, but those are beyond the scope of this course. A primer on `reticulate` <a href='https://www.r-bloggers.com/2022/04/getting-started-with-python-using-r-and-reticulate/.'>https://www.r-bloggers.com/2022/04/getting-started-with-python-using-r-and-reticulate/</a> 

## Functions vs Objects in Python

<br>

Python absolutely still uses functions (just like R), and they‚Äôre incredibly useful‚Äîparticularly for data transformations, wrangling, or tasks like parallel processing.   
  
<span style="color: #007acc"><strong>But when it comes to modeling, the dominant paradigm is object-oriented</strong></span>

## Why OOP Matters in Python
<br>

Many of the most popular Python libraries for modeling‚Äîsuch as scikit-learn, statsmodels, PyTorch, and TensorFlow‚Äîare built around the principles of object-oriented programming (OOP).  

This means that to work effectively in Python, especially for tasks involving modeling or model training, 
<span style="color: #007acc"><strong>it helps to think in terms of objects and classes, not just functions.</strong></span>
<br>

. . . 

#### Models in Python: 
- Typically instances of classes
- Come with built-in methods (like `.fit()` or `.predict()`) and attributes (like `.coef_`) that define their behavior and internal state

## Why OOP Matters for Machine Learning/ Modeling

If you‚Äôre doing machine learning, deep learning, or building custom models in Python, you're often working in domains where R doesn't offer as much built-in support. That‚Äôs where packages like:

- **scikit-learn** (machine learning)
- **PyTorch** and **TensorFlow** (deep learning and neural networks)

come in.

. . .  

- **Scikit-learn** provides a wide array of **ready-to-use model classes**, making it a great entry point
- **PyTorch** and **TensorFlow**, especially for custom neural network architectures, require you to **create your own classes** using inheritance from base classes and mixins

*I won‚Äôt go deep into PyTorch or TensorFlow here, but feel free to ask me later or explore tutorials online if you're curious!*


## Statsmodels

- **Statsmodels** offers a more R-like interface for regression models but requires some additional setup for design matrices. (You can check out their excellent [documentation here](https://www.statsmodels.org/stable/gettingstarted.html).)

*I won't cover statsmodels here, but it is worth looking up if you are interested.* 

## What We'll Cover

To get comfortable with this way of thinking, we‚Äôll first do a **brief recap of object-oriented programming**‚Äîwhat classes are, how inheritance works, and how you can define your own classes in Python. <br>

Then we‚Äôll shift focus to using **model classes in Python**, particularly with **scikit-learn**.  <br>

Whether you‚Äôre using a prebuilt model class or writing your own from scratch, the workflow is often similar. You‚Äôll still need to define or use common methods like `.fit()`, `.predict()`, and `.score()`, and understand how the model stores internal data like coefficients and hyperparameters.


# Part 1: Object-Oriented Programming

---

Object-Oriented Programming (OOP) is a core programming paradigm used throughout Python, especially in modeling libraries like scikit-learn, PyTorch, and TensorFlow.

<span style="color: #007acc"><strong>It helps us build modular, reusable, and intuitive code by organizing logic around objects.</strong></span>

---

## **Key Principles of OOP**  

1. **Encapsulation**: Bundling data and methods together in a single unit.  
   - A `StandardScaler` object stores mean and variance data and has `.fit()` and `.transform()` methods

2. **Inheritance**: Creating new classes based on existing ones.  
   - `sklearn.LinearRegression` inherits from a general regression model class.    

3. **Abstraction**: Hiding implementation details and exposing only essential functionality.  
   - e.g., .fit() works the same way from the outside, regardless of model complexity  

4. **Polymorphism**: Objects of different types can be treated the same way if they implement the same methods. 
   - e.g., any object with `.fit()` and `.predict()` can be passed into a pipeline 
   - Python‚Äôs **duck typing**:  
     - *"If it walks like a duck and quacks like a duck, then it must be a duck."*  
     - If an object has the right attributes and methods, it can be used in the same way as another object of a different class. 

## Recap: What Are Classes and Objects?

A **class** is a **blueprint** for creating objects. An **object** is an **instance** of a class that contains **data (attributes)** and **behaviors (methods)**.  

. . . 

For example, in Python, we can define a class called `Dog` and give it attributes that store data about a given dog.    

We can also define methods that represent behaviors an object of the `Dog` class can perform:  

. . . 

```{python}
#| code-line-numbers: "|3,4|6"
class Dog:
    def __init__(self, name, breed):
        self.name = name
        self.breed = breed

    def speak(self):
        return f"{self.name} says woof!"
```

## Creating a dog

Creating an instance (object) of the `Dog` class lets us model a particular dog:  

. . . 
```{python}
#| code-line-numbers: "1|2|"
#| output-location: fragment
my_dog = Dog("Buddy", "Golden Retriever")
print(my_dog.speak())  
```

. . .

Here, `my_dog` is an **object** of the `Dog` class.     

It has **attributes** (`name`, `breed`) and **methods** (`speak()`).   

. . .  

When we make an instance of the Dog class:     
- We set the value of the attributes [`name` and `breed`], which are then stored as part of the `my_dog` object  
- We can use any methods defined in the Dog class on `my_dog`  

. . . 
 
<span style="color: #007acc"><strong>Note:</strong> For python methods, the `self` argument is assumed to be passed and therefore we do not put anything in the parentheses when calling `.speak()`.</span>

## **How Does This Relate to Machine Learning and Modeling?**  

<br> 

Machine learning models in Python are implemented as **classes**.    
    - When you create a model, you‚Äôre **instantiating an object** of a predefined class (e.g., `LogisticRegression()`).    
    - That model **inherits** attributes (parameters, coefficients) and methods (like `.fit()` and `.predict()`).    

--- 

For example, a **logistic regression model** in `scikit-learn` is an instance of the `LogisticRegression` class:
```{.python}
## Example: 
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()  # Creating an instance of the LogisticRegression class
model.fit(X_train, y_train)   # Calling a method to train the model
predictions = model.predict(X_test)  # Calling a method to make predictions
```
Here, **`model`** is an **object** that has inherited attributes and methods from `LogisticRegression`.  

. . .

üëâ **To check if an object is an instance of a particular class**, use:
```{.python}
isinstance(object, class)  # Returns True if `object` is an instance of `class`.
```

. . . 

<span style="color: #007acc"><strong>Knowing what class an object is helps us know what methods we can expect to have access to.</strong></span>


## Example: Understanding Classes - Definition, Inheritance, Mixins

Before we get into the machine learning demo projects, I want to quickly demonstrate how classes work and how we can leverage inheritance when making our own classes.  

Even though this example is very simple, the same method applies to making your own classes for machine learning and neural network models.


## Base Classes
A **base class** (or parent class) serves as a template for creating objects. Other classes can inherit from it to reuse its properties and methods.

For example, the class we created earlier, `Dog`, could be a base class. 

```{python}
class Dog: ## class definition
    def __init__(self, name, breed): ## sets up the initialization for an instance of class Dog. 
        ### Allows us to assign name and breed when we instantiate dog. 
        self.name = name ## attributes
        self.breed = breed

    def speak(self): ## method
        return f"{self.name} says Woof!"


```

. . .  

And we can make an instance of `Dog` and make it do things. 

```{python}
#| output-location: fragment
my_dog = Dog("Fido", "Labrador") ## create a dog of name 'Fido' and breed 'Labrador'
print(my_dog.speak())

## if we want to see what kind of dog our dog is
print(f"Our dog {my_dog.name} is a {my_dog.breed}.")
```

## Derived (Child) Classes  

Now that we have a `Dog` class, let‚Äôs define a new class called `GuardDog`. This class will inherit all the properties and methods from `Dog`, while also adding its own unique attributes and behaviors.

<span style="color: #007acc"><strong>This is the power of inheritance</span></strong>‚Äîwe don‚Äôt have to rewrite everything from scratch! Instead, we can extend the existing functionality of `Dog` to create a more specialized class.

---

We can extend the `Dog` class to create a `GuardDog` class. 

```{python}
#| code-line-numbers: "|1|2-7|"
class GuardDog(Dog):  # GuardDog inherits from Dog
    def __init__(self, name, breed, training_level): ## in addition to name and breed, we can 
        # define a training level. 
        # Call the parent (Dog) class's __init__ method
        super().__init__(name, breed)
        self.training_level = training_level  # New attribute for GuardDog that stores the 
        # training level for the dog

    def guard(self): ## checks if the training level is > 5 and if not says train more
        if self.training_level > 5:
            return f"{self.name} is guarding the house!"
        else:
            return f"{self.name} needs more training before guarding."
    
    def train(self): ## modifies the training_level attribute to increase the dog's training level
        self.training_level = self.training_level + 1
        return f"Training {self.name}. {self.name}'s training level is now {self.training_level}"

# Creating an instance of GuardDog
my_guard_dog = GuardDog("Rex", "German Shepherd", training_level= 5)
```
---

Now that we have a dog (my_guard_dog), we can call on any of the methods introduced in the `Dog` class as well as the new `GuardDog` class.
```{python}
#| slide-type: fragment
#| output-location: fragment
# Using methods from the base class
print(my_guard_dog.speak())  # Inherited from Dog -> Output: "Rex says Woof!"
```
```{python}
#| slide-type: fragment
#| output-location: fragment
# Using a method from the derived class
print(f"{my_guard_dog.name}'s training level is {my_guard_dog.training_level}.")
print(my_guard_dog.guard()) 
```
```{python}
#| slide-type: fragment
#| output-location: fragment
## if we want to train Rex and increase his training level, 
print(my_guard_dog.train())

## now check if he can guard 
print(my_guard_dog.guard()) 
```
---
class: middle

As we saw with Rex, <span style="color: #007acc"><strong>child classes inherit all attributes (`.name` and `.breed`) and methods (`.speak()`) from parent classes.</strong></span> They can also have new methods (`.train()`).

## Mixins
A **mixin** is a special kind of class designed to add **functionality** to another class. Unlike base classes, mixins aren‚Äôt used alone.  

. . . 

For example, scikit-learn uses mixins like:  
- `sklearn.base.ClassifierMixin` (adds classifier-specific methods)  
- `sklearn.base.RegressorMixin` (adds regression-specific methods)  

which it adds to the `BaseEstimator` class to add functionality.

To finish up our dog example, we are going to define a mixin class that adds a functionality to the base `Dog()` class which allows us to teach a dog tricks. 

---

- When creating a mixin class, we let the other base classes carry most of the initialization
- However, we can add other attributes like `.tricks`, which stores the set of learned tricks
- We can also add methods to define additional behaviors

```{python}
#| code-line-numbers: "|2,3|4|6-17|"
class TrickMixin: ## mixin that will let us teach a dog tricks
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)  # Ensures proper initialization in multiple inheritance
        self.tricks = []  # Store learned tricks

    def learn_trick(self, trick):
        """Teaches the dog a new trick."""
        if trick not in self.tricks:
            self.tricks.append(trick)
            return f"{self.name} learned a new trick: {trick}!"
        return f"{self.name} already knows {trick}!"

    def perform_tricks(self):
        """Returns a list of tricks the dog knows."""
        if self.tricks:
            return f"{self.name} can perform: {', '.join(self.tricks)}."
        return f"{self.name} hasn't learned any tricks yet."

## note: the TrickMixin class is not a standalone class! it does not let us create a dog on its own!!!
```
. . . 

Using this Trick mixin, we can then create a new class of dog (SmartDog) using both `Dog`and `TrickMixin` as base classes. 

---

By including both `Dog` and `TrickMixin` as base classes, we can give objects of class `SmartDog` the ability to speak and learn tricks!

```{python}
#| slide-type: fragment
#| output-location: fragment
class SmartDog(Dog, TrickMixin):
    def __init__(self, name, breed):
        super().__init__(name, breed)  # Initialize Dog class
        TrickMixin.__init__(self)  # Initialize TrickMixin separately
```

. . . 

```{python}
#| slide-type: fragment
#| output-location: fragment
# a SmartDog object can use methods from both parent object `Dog` and mixin `TrickMixin`.
my_smart_dog = SmartDog("Buddy", "Border Collie")
```

. . . 

```{python}
#| slide-type: fragment
#| output-location: fragment
print(my_smart_dog.speak()) 
```

. . . 

```{python}
#| slide-type: fragment
#| output-location: fragment
print(my_smart_dog.learn_trick("Sit"))  
print(my_smart_dog.learn_trick("Roll Over")) 
print(my_smart_dog.learn_trick("Sit"))  

```
. . . 

```{python}
#| slide-type: fragment
#| output-location: fragment
print(my_smart_dog.perform_tricks()) 
```

## Duck Typing 

>"If it quacks like a duck and walks like a duck, it's a duck."

Python doesn‚Äôt require explicit interfaces.
If an object implements the expected methods, it can be used interchangeably with other objects. This is called duck typing.

. . . 

We can demonstrate this by defining two new base classes that are different than `Dog` but also have a `speak()` method.

. . . 

```{python}
#| code-line-numbers: "|5-6, 12-13"
class Human:
    def __init__(self, name):
        self.name = name

    def speak(self):
        return f"{self.name} says hello!"

class Parrot:
    def __init__(self, name):
        self.name = name

    def speak(self):
        return f"{self.name} says squawk!"

```

## Duck Typing in Action

Even though `Dog`, `Human` and `Parrot` are entirely different classes...

```{python}
#| output-location: fragment
def call_speaker(obj):
    print(obj.speak())

call_speaker(Dog("Fido", "Labrador"))
call_speaker(Human("Alice"))
call_speaker(Parrot("Polly"))
```

. . .  

They all implement `.speak()`, so Python treats them the same!

In the context of our work, this would allow us to make a pipeline using models from different libraries that do not share a base class/mixin but have the same methods. 

---

<span style="color: #007acc"><strong>While our dog example was very simple, this is the same way that model classes work in python.</strong></span> 

## OOP In ML Recap

#### **Key Benefits of OOP in Machine Learning**  

1. **Encapsulation** ‚Äì Models store parameters and methods inside a single object.  
2. **Inheritance** ‚Äì New models can build on base models, reusing existing functionality.  
3. **Abstraction**  ‚Äì `.fit()` should work as expected, regardless of complexity of underlying implimentation.
4. **Polymorphism (Duck Typing)** ‚Äì Different models share the same method names (`.fit()`, `.predict()`), making them easy to use interchangeably, particularly in analysis pipelines. 

Understanding **base classes** and **mixins** is especially important when working with deep learning frameworks like **PyTorch and TensorFlow**, as they allow for easy customization of models.  

By using **object-oriented programming**, Python makes it easy to **structure machine learning workflows** in a reusable and scalable way. The fact that all ML models in scikit-learn follow the **same structure** (with `.fit()`, `.predict()`, `.score()`, etc.) makes it easier to switch between models and automate processes. Additionally, the model classes in the statsmodels package have many of the same methods (`.fit()`, `.predict()`, `.score()`). 


# Part B - Demo Projects

**Apply knowledge of OOP to modeling using scikit-learn**

---

## üêß Mini Project: Classifying Penguins with scikit-learn

Now that you understand **classes** and **data structures** in Python, let‚Äôs apply that knowledge!

In this project, we‚Äôll try to classify **penguin species** using two features:  
- `bill_length_mm`  
- `bill_depth_mm`  

We‚Äôll explore:  
- **Unsupervised learning** with **K-Means** clustering (model doesn't 'know' y)
- **Supervised learning** with a **k-NN classifier** (model trained w/ y information)


## Modeling with scikit-learn Classes

We'll use models from **scikit-learn**, which are built using object-oriented design.  
Each model is an **instance of a class** (inheriting from `BaseEstimator`) with:

**Common Methods:**  
- `.fit()` ‚Äî Train the model  
- `.predict()` ‚Äî Make predictions  

**Common Attributes:**  
- `.get_params()`, `.classes_`, `.n_clusters_`, etc.  

> We're using `scikit-learn` here for its simplicity, but the concepts apply to more advanced frameworks like **PyTorch** and **TensorFlow** too!

## General Modeling Workflow

::: columns
::: column

**Step 0: Prepare Workspace**  
- Import necessary libraries/modules:  
  - **Functions** (e.g., `train_test_split`, `accuracy_score`)  
  - **Classes** (e.g., `KMeans`, `KNeighborsClassifier`)  

**Step 1: Data Preparation**  
- Load data (`pandas`)  
- Clean data (`pandas`, `numpy`)  
- Transform/scale features (`sklearn.preprocessing`)  
- Optionally split data into training and testing sets  

**Step 2: Initialize the Model**  
- Create an instance of the model class (`KMeans`, `KNeighborsClassifier`)  
- Set parameters during instantiation (e.g., `n_clusters=3`, `n_neighbors=5`)  

:::

::: column

**Step 3: Fit the Model**  
- Use `.fit(X)` for unsupervised models  
- Use `.fit(X_train, y_train)` for supervised models  

**Step 4: Make Predictions** *(optional)*  
- Use `.predict(X_test)` to generate predictions  
- Use `.predict_proba()` to get class probabilities (if available)  

**Step 5: Evaluate Model Performance**  
- Compare predictions to true values  
- Use visualizations or metrics (e.g., accuracy, ARI, classification report)  

:::
:::


## Step 0: Import Libraries

<br>

<span style="color: #007acc"><strong>Before any analysis, we must import the necessary libraries.</strong></span>  

For large libraries like **scikit-learn**, **PyTorch**, or **TensorFlow**, we usually do **not** import the entire package. Instead, we selectively import the **classes** and **functions** we need.

. . . 

> üî§ **Naming Tip**:  
> - `CamelCase` = **Classes**  
> - `snake_case` = **Functions**

--- 

In this project, we‚Äôll use the following:

**Classes**  
- `StandardScaler` ‚Äî for feature scaling  
- `KNeighborsClassifier` ‚Äî for supervised k-NN classification  
- `KMeans` ‚Äî for unsupervised clustering

<br>

**Functions**  
- `train_test_split()` ‚Äî to split data into training and test sets  
- `accuracy_score()` ‚Äî to evaluate classification accuracy  
- `classification_report()` ‚Äî to print precision, recall, F1 (balance of precision and recall), Support (number of true instances per class)
- `adjusted_rand_score()` ‚Äî to evaluate clustering performance


## Import Libraries

```{python}
#| code-line-numbers: "|12|14-17|19-21|"
## imports
import pandas as pd
import numpy as np

from plotnine import *
import seaborn as sns
import matplotlib.pyplot as plt

from great_tables import GT
from tabulate import tabulate

## sklearn imports

## import classes
from sklearn.preprocessing import StandardScaler 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans

## import functions
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, adjusted_rand_score
```  

## Step 1: Data Preparation {.smaller}

```{python}
#| code-line-numbers: "1,2|4-11|13-16|18-19|"
#| output-location: slide
# Load the Penguins dataset
penguins = sns.load_dataset("penguins").dropna()

# Make a summary table for the penguins dataset, grouping by species. 
summary_table = penguins.groupby("species").agg({
    "bill_length_mm": ["mean", "std", "min", "max"],
    "bill_depth_mm": ["mean", "std", "min", "max"],
    "flipper_length_mm": ["mean", "std", "min", "max"],
    "body_mass_g": ["mean", "std", "min", "max"],
    "sex": lambda x: x.value_counts().to_dict()  # Count of males and females
})

# Round numeric values to 1 decimal place (excluding the 'sex' column)
for col in summary_table.columns:
    if summary_table[col].dtype in [float, int]:
        summary_table[col] = summary_table[col].round(1)

# Display the result
display(summary_table)
```


## Scaling the data - Understanding the Standard Scaler class

For our clustering to work well, the predictors should be on the same scale. <br>

To achieve this, we use an instance of the `StandardScaler` class. 


## Standard Scaler

```python
class sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)
```
<br>

. . .   

**Parameters** are supplied by user  
- copy, with_mean, with_std <br>

**Attributes** contain the `data` of the object  
- `scale_`: scaling factor  
- `mean_`: mean value for each feature  
- `var_`: variance for each feature  
- `n_features_in_`: number of features seen during fit  
- `n_samples_seen`: number of samples processed for each feature <br>

**Methods** describe the `behaviors` of the object and/or `modify` its attributes  
- `fit(X)` -> compute mean and std used for scaling -> fit scaler to data X  
    * updates the attributes of the scaler object  
- `transform(X)` -> perform standardization by centering and scaling with fitted scaler  

## Data Preparation

```{python}
#| code-line-numbers: "1-3|5-7|"
# Selecting features for clustering -> let's just use bill length and bill depth.
X = penguins[["bill_length_mm", "bill_depth_mm"]]
y = penguins["species"]

# Standardizing the features for better clustering performance
scaler = StandardScaler() ## create instance of StandardScaler
X_scaled = scaler.fit_transform(X) ## same as calling scaler.fit(X) then X_scaled = scaler.transform(X)
```

## Understanding the KMeans model class

```python
class sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, 
tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')
```
<br>

**Parameters**: Set by user at time of instantiation  
- n_clusters, max_iter, algorithm  <br>

**Attributes**: Store object data    
- `cluster_centers_`: stores coordinates of cluster centers  
- `labels_`: stores labels of each point 
- `n_iter_`: number of iterations run (will be changed during method run)  
- `n_features_in` and `feature_names_in_`: store info about features seen during fit  <br>

**Methods**: Define object behaviors      
- `fit(X)` -> same as usage as `train()` -> fit model to data X  
- `predict(X)` -> predict closest cluster each sample in X belongs to  
- `transform(X)` -> transform X to cluster-distance space  

---

### Step 2: Create model

```{python}
## Choosing 3 clusters b/c we have 3 species
kmeans = KMeans(n_clusters=3, random_state=42) ## make an instance of the K means class
```

### Step 3: Fit model to data

```{python}
#| code-line-numbers: "1-2|4-5|"
#| output-location: fragment
## the fit
penguins["kmeans_cluster"] = kmeans.fit_predict(X_scaled)

## now that we fit the model, we should have cluster centers
print("Coordinates of cluster centers:", kmeans.cluster_centers_)
```

## Step 5: Visualize and Evaluate

To do visualization, we can use either seaborn or plotnine. Plotnine is nice because it is just a python port of ggplot!

To take at the distribution of our species by bill length and bill depth...

```{python}
#| output-location: slide
# Plotnine scatterplot of species by bill length/depth
plot1 = (ggplot(penguins, aes(x="bill_length_mm", y="bill_depth_mm", color="species"))
 + geom_point()
 + ggtitle("Penguin Species")
 + theme_bw())

display(plot1)
```

. . . 

The biggest differences between `plotnine` and `ggplot2` syntax are: 
- With `plotnine` the whole call is wrapped in `()` parentheses
- Variables are called with strings (`""` are needed!)
- If you don't use `from plotnine import *`, you will need to import each individual function you plan to use!

---

We can also create a plot for the K-means clustering results

```{python}
#| output-location: slide
# Plotnine scatterplot of k-Means clusters
plot2 = (ggplot(penguins, aes(x="bill_length_mm", y="bill_depth_mm", color="factor(kmeans_cluster)"))
 + geom_point()
 + ggtitle("K-Means Clustering Results")
 + theme_bw())

display(plot2)
```
---

## Use function to calculate ARI

To check how good our model is, we can use one of the functions included in the sklearn library

The `adjusted_rand_score()` function evaluates how well the cluster groupings agree with the species groupings while adjusting for chance. 

```{python}
#| output-location: fragment
# Calculate clustering performance using Adjusted Rand Index (ARI)
kmeans_ari = adjusted_rand_score(penguins['species'], penguins["kmeans_cluster"])
print(f"k-Means Adjusted Rand Index: {kmeans_ari:.2f}")
```

---

### We can also use methods on our data structure to create new data

- We can use the `.groupby()` method to help us plot cluster agreement with species label

```{python}
#| code-line-numbers: "1-3|5-20"
#| output-location: slide
# Count occurrences of each species-cluster-sex combination
# ( .size gives the count as index, use reset_index to get count column. )
scatter_data = penguins.groupby(["species", "kmeans_cluster", "sex"]).size().reset_index(name="count")

# Create a heatmap of the cluster assignments by species
heatmap_plot = (
    ggplot(scatter_data, aes(x="species", y="kmeans_cluster", fill="count"))
    + geom_tile(color="white")  # Add white grid lines for separation
    + scale_fill_gradient(low="lightblue", high="darkblue")  # Heatmap colors
    + labs(
        title="Heatmap of KMeans Clustering by Species",
        x="Species",
        y="KMeans Cluster",
        fill="Count"
    )
    + theme_bw()
)

# Display the plot
display(heatmap_plot)
```

---

- If we want to add sex as a variable to see if that is why our clusters don't agree with our species, we can use a scatterplot

```{python}
#| output-location: slide
scatter_plot = (
    ggplot(scatter_data, aes(x="species", y="kmeans_cluster", color="species", shape="sex", size="count"))
    + geom_point(alpha=0.7, position=position_dodge(width=0.5))  # Horizontal separation
    + scale_size(range=(2, 10))  # Adjust point sizes
    + scale_y_continuous(breaks=[0, 1, 2])  # Set y-axis ticks to only 0, 1, 2
    + theme_bw()
    + labs(
        title="KMeans Clustering vs Species (Size = Count)",
        x="Species",
        y="KMeans Cluster"
    )
)
#Display the plot
display(scatter_plot)
```

## Project 2 -> KNN classification

For our KNN classification, the model is supervised (meaning it is dependent on the outcome 'y' data) and therefore we need to split our data into a training and test set. <br>

. . . 

The **function** `train_test_split()` from scikit-learn is helpful here! Our classifier object has built in methods for fitting models and predicting.

```{python}
# Splitting dataset into training and testing sets (still using scaled X!)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
```

. . . 

>Unlike R functions, which return a single object (often a list when multiple outputs are needed), Python functions can return multiple values as a tuple‚Äîletting you unpack them directly into separate variables.

## Understanding KNeighborsClassifier class
```{.python}
class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', 
algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
```
. . . 

**Parameters**: Set by user at time of instantiation    
- n_neigbors, weights, algorithm, etc.  <br>

**Attributes**: Store object data     
- `classes_`: class labels known to the classifier  
- `effective_metric_`: distance metric used  
- `effective_metric_params_`: parameters for the metric function  
- `n_features_in` and `feature_names_in_`: store info about features seen during fit    
- `n_samples_fit_`: number of samples in fitted data  <br> 

**Methods**: Define object behaviors      
- `.fit(X, y)` -> fit knn classifier from training dataset (X and y)  
- `.predict(X)` -> predict class labels for provided data X  
- `.predict_proba(X)` -> return probability estimates for test data X  
- `.score(X, y)` -> return mean accuracy on given test data X and labels y  

---

### Making an instance of KNeighborsClassifier and fitting to training data
- For a supervised model, y_train is included in `.fit()`!
```{python}
#| output: false
## perform knn classification
# Applying k-NN classification with 5 neighbors
knn = KNeighborsClassifier(n_neighbors=5) ## make an instance of the KNeighborsClassifier class
# and set the n_neighbors parameter to be 5. 

# Use the fit method to fit the model to the training data
knn.fit(X_train, y_train)
```

## Once the model is fit...

- Once the model is fit, we can look at its attributes (ex: `.classes_`) which gives the class labels as known to the classifier

```{python}
#| output-location: fragment
print(knn.classes_)
```

## Next Step: Use fitted model to predict species for test data

```{python}
#| output-location: fragment
# Use the predict method on the test data to get the predictions for the test data
y_pred = knn.predict(X_test)

# Also can take a look at the prediction probabilities, 
# and use the .classes_ attribute to put the column labels in the right order
probs = pd.DataFrame(
    knn.predict_proba(X_test),
    columns = knn.classes_)
probs['y_pred'] = y_pred

print("Predicted probabilities: \n", probs.head())
```

## Scatterplot for k-NN classification of test data

- Create dataframe of unscaled X_test, `bill_length_mm`, and `bill_depth_mm`.
- Add to it the actual and predicted species labels

```{python}
## First unscale the test data
X_test_unscaled = scaler.inverse_transform(X_test)

## create dataframe 
penguins_test = pd.DataFrame(
    X_test_unscaled,
    columns=['bill_length_mm', 'bill_depth_mm']
)

## add actual and predicted species 
penguins_test['y_actual'] = y_test.values
penguins_test['y_pred'] = y_pred
penguins_test['correct'] = penguins_test['y_actual'] == penguins_test['y_pred']
```

## Plotnine scatterplot for k-NN classification of test data

```{python}
#| output-location: slide
## Build the plot
plot3 = (ggplot(penguins_test, aes(x="bill_length_mm", y="bill_depth_mm", 
color="y_actual", fill = 'y_pred', size = 'correct'))
 + geom_point()
 + scale_size_manual(values={True: 2, False: 5})
 + ggtitle("k-NN Classification Results")
 + theme_bw())

display(plot3)
```

## Evaluate KNN performance

```{python}
#| output-location: fragment
## eval knn performance
# Calculate accuracy and print classification report -> 
# accuracy_score and classification_report are functions! 
knn_accuracy = accuracy_score(y_test, y_pred)
print(f"k-NN Accuracy: {knn_accuracy:.2f}")
print(classification_report(y_test, y_pred))
```

## Make a Summary Table of Metrics for Both Models

```{python}
#| output-location: fragment
##  making a summary table
# Creating a summary table
summary_table = pd.DataFrame({
    "Metric": ["k-Means Adjusted Rand Index", "k-NN Accuracy"],
    "Value": [kmeans_ari, knn_accuracy]
})
GT(summary_table).show() ## round the values!!!!!
```


## Key Takeaways from This Session

üîπ **Machine learning models in Python are implemented as classes**, and model objects are instantiated from these classes.  
üîπ Python modeling workflows make extensive use of both **functions** and **object-oriented structures** (e.g., methods and attributes).  
üîπ The consistent use of methods like `.fit()`, `.predict()`, and `.transform()` across libraries encourages **code reuse, modularity, and interoperability**.  
üîπ A working knowledge of **inheritance** and **mixins** is essential for extending functionality, particularly in deep learning frameworks.

Python‚Äôs object-oriented design makes machine learning pipelines more **robust, scalable, and production-ready**. üöÄ


## Additional Insights

#### Predictable APIs Make Model Switching Seamless  
Once you're familiar with Python's object-method pattern, swapping models (e.g., `LogisticRegression` ‚Üí `RandomForestClassifier`) requires only minimal changes.

#### scikit-learn Prioritizes Interoperability  
The standardized class structure enables integration with tools like `Pipeline`, `GridSearchCV`, and `cross_val_score`‚Äîpromoting reusable and testable workflows.

#### Class Attributes Support Model Transparency  
Understanding common attributes like `.coef_`, `.classes_`, and `.feature_importances_` is critical for **model inspection, debugging, and reporting**.

#### Custom Classes Are Central in Deep Learning  
Frameworks like PyTorch and TensorFlow rely on **subclassing base models**‚Äîreinforcing the need to understand object-oriented programming practices.

#### Mixins Enable Modular Design Across Libraries  
Mixins (e.g., `ClassifierMixin`, `TransformerMixin`) extend core functionality without redundancy and are a cornerstone of Python‚Äôs library design patterns.

## **Pre-Reading for This Session**  
- [Scikit-learn Documentation](https://scikit-learn.org/stable/user_guide.html)  
- [Introduction to OOP in Python (Real Python)](https://realpython.com/python3-object-oriented-programming/)  
- [Plotnine Reference](https://plotnine.org/reference/)

