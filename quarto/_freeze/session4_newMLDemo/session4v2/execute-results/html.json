{
  "hash": "7e91d9e602c6d6a31abb41770ab5e4aa",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: '**Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries**'\njupyter: python3\nformat: \n    revealjs:\n        smaller: true\n        code-block-height: 650px\n        highlight-style: pygments\nexecute:\n  freeze: auto\n  eval: true\n  echo: true\n---\n\n\n---\n\n## Session Overview\n\nThis session is divided into two parts:  \n1. **A quick recap of Object-Oriented Programming (OOP)** and why it‚Äôs useful.  \n2. **Applying OOP concepts to machine learning** by building models in scikit-learn.\n\n---\n\n## Introduction\n\nBoth R and python use objects, but not everything in R is object-oriented... If that sounds confusing that's because it is! \n\n**Functional programming:** focuses on functions as the primary unit of code\n\n**Object-oriented programming:** uses objects with attached attributes(data) and methods(behaviors)\n\nFunctional and object-oriented programming are paradigms (styles) and these styles can be applied in both R and Python. However, Python libraries and workflows tend to rely more on object-oriented programming than those designed for R. \n\nR originated from another statistical programming language called S, which is not object-oriented, and R tends to lend itself better to functional programming than object-oriented programming in many cases. \n\n## Why Python? üêç\n \n- R built by statisticians for statisticians: \n    * Excels at statistical analysis and modeling\n    * Beautiful data visualizations with fairly simple code \n- Python is a general purpose language (like C++, Java):\n    * Excels at deep learning, image analysis, text analysis\n    * But also: \n        - Automation\n        - Software/Application development (including CLI [command line interface])\n        - Web development\n\n## Why Python? üêç\n\nMany of the most popular Python libraries for modeling‚Äîsuch as scikit-learn, statsmodels, PyTorch, and TensorFlow‚Äîare built around the principles of object-oriented programming (OOP).  \n\nThis means that to work effectively in Python, especially for tasks involving modeling or model training, <span style=\"color: #007acc\"><strong>it helps to think in terms of objects and classes, not just functions.</strong></span>\n\n## Functions vs Objects in Python\n\nPython absolutely still uses functions (just like R), and they‚Äôre incredibly useful‚Äîparticularly for data transformations, wrangling, or tasks like parallel processing.   \n  \n<span style=\"color: #007acc\"><strong>But when it comes to modeling, the dominant paradigm is object-oriented</strong></span>\n\n. . . \n\n### Models in Python: \n- Typically instances of classes\n- Come with built-in methods (like `.fit()` or `.predict()`) and attributes (like `.coef_`) that define their behavior and internal state\n\n## Why This Matters for Machine Learning/ Modeling\n\nIf you‚Äôre doing machine learning, deep learning, or building custom models in Python, you're often working in domains where R doesn't offer as much built-in support. That‚Äôs where packages like:\n\n- **scikit-learn** (machine learning)\n- **PyTorch** and **TensorFlow** (deep learning and neural networks)\n\ncome in‚Äîand they all require an understanding of how to work with **objects and classes**.\n\n- **Scikit-learn** provides a wide array of **ready-to-use model classes**, making it a great entry point.  \n- **PyTorch** and **TensorFlow**, especially for custom neural network architectures, require you to **create your own classes** using inheritance from base classes and mixins.\n\n*I won‚Äôt go deep into PyTorch or TensorFlow here, but feel free to ask me later or explore tutorials online if you're curious!*\n\n\n## Statsmodels\n\n- **Statsmodels** offers a more R-like interface for regression models but requires some additional setup for design matrices. (You can check out their excellent [documentation here](https://www.statsmodels.org/stable/gettingstarted.html).)\n\n*I won't cover it here, but it is worth looking up if you are interested.* \n\n## What We'll Cover\n\nTo get comfortable with this way of thinking, we‚Äôll first do a **brief recap of object-oriented programming**‚Äîwhat classes are, how inheritance works, and how you can define your own classes in Python.  \n  \nThen we‚Äôll shift focus to using **model classes in Python**, particularly with **scikit-learn**.   \n\nWhether you‚Äôre using a prebuilt model class or writing your own from scratch, the workflow is often similar. You‚Äôll still need to define or use common methods like `.fit()`, `.predict()`, and `.score()`, and understand how the model stores internal data like coefficients and hyperparameters.\n\n\n# **Part 1: Object-Oriented Programming**  \n\n## **Recap: What Are Classes and Objects?**   \n\nA **class** is a **blueprint** for creating objects. An **object** is an **instance** of a class that contains **data (attributes)** and **behaviors (methods)**.  \n\n. . . \n\nFor example, in Python, we can define a class called Dog and give it attributes that store data about a given dog.    \n\nWe can also define methods that represent behaviors an object of the dog class can perform:  \n\n. . . \n\n::: {#9ddf964e .cell execution_count=1}\n``` {.python .cell-code code-line-numbers=\"|3,4|6\"}\nclass Dog:\n    def __init__(self, name, breed):\n        self.name = name\n        self.breed = breed\n\n    def bark(self):\n        return f\"{self.name} says woof!\"\n```\n:::\n\n\n## Creating a dog\n\nCreating an instance (object) of the `Dog` class lets us model a particular dog:  \n\n. . . \n\n::: {#a30457f3 .cell output-location='fragment' execution_count=2}\n``` {.python .cell-code}\nmy_dog = Dog(\"Buddy\", \"Golden Retriever\")\nprint(my_dog.bark())  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuddy says woof!\n```\n:::\n:::\n\n\n. . .\n\nHere, `my_dog` is an **object** of the `Dog` class.     \n\nIt has **attributes** (`name`, `breed`) and **methods** (`bark()`).   \n\nWhen we make an instance of the Dog class:     \n- We set the value of the attributes [`name` and `breed`], which are then stored as part of the `my_dog` object  \n- We can use any methods defined in the Dog class on `my_dog`  \n\n. . . \n \n**Note:** For python methods, the `self` argument is assumed to be passed and therefore we do not put anything in the parentheses when calling `.bark()`. \n\n## **How Does This Relate to Machine Learning and Modeling?**  \nMachine learning models in Python are implemented as **classes**.    \n    - When you create a model, you‚Äôre **instantiating an object** of a predefined class (e.g., `LogisticRegression()`).    \n    - That model **inherits** attributes (parameters, coefficients) and methods (like `.fit()` and `.predict()`).    \n\n--- \n\nFor example, a **logistic regression model** in `scikit-learn` is an instance of the `LogisticRegression` class:\n```{.python}\n## Example: \nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()  # Creating an instance of the LogisticRegression class\nmodel.fit(X_train, y_train)   # Calling a method to train the model\npredictions = model.predict(X_test)  # Calling a method to make predictions\n```\nHere, **`model`** is an **object** that has inherited attributes and methods from `LogisticRegression`.  \n\n. . .\n\nüëâ **To check if an object is an instance of a particular class**, use:\n```{.python}\nisinstance(object, class)  # Returns True if `object` is an instance of `class`.\n```\n\n. . . \n\nKnowing what class an object is helps us know what methods we can expect to have access to.\n\n## **Key Benefits of OOP in Machine Learning**  \n\n1. **Encapsulation** ‚Äì Models store parameters and methods inside a single object.  \n2. **Inheritance** ‚Äì New models can build on base models, reusing existing functionality.  \n3. **Abstraction**  ‚Äì `.fit()` should work as expected, regardless of complexity of underlying implimentation.\n4. **Polymorphism** ‚Äì Different models share the same method names (`.fit()`, `.predict()`), making them easy to use interchangeably. \n\n\n## Example: Understanding Classes - Definition, Inheritance, Mixins\n\nBefore we get into the machine learning demo projects, I want to quickly demonstrate how classes work and how we can leverage inheritance when making our own classes.  \n\nEven though this example is very simple, the same method applies to making your own classes for machine learning and neural network models.\n\n\n## Base Classes\nA **base class** (or parent class) serves as a template for creating objects. Other classes can inherit from it to reuse its properties and methods.\n\nFor example, the class we created earlier, `Dog`, could be a base class. \n\n::: {#9219ba28 .cell execution_count=3}\n``` {.python .cell-code}\nclass Dog: ## class definition\n    def __init__(self, name, breed): ## sets up the initialization for an instance of class Dog. \n        ### Allows us to assign name and breed when we instantiate dog. \n        self.name = name ## attributes\n        self.breed = breed\n\n    def bark(self): ## method\n        return f\"{self.name} says Woof!\"\n\n```\n:::\n\n\n. . .  \n\nAnd we can make an instance of `Dog` and make it do things. \n\n::: {#f5bc512c .cell output-location='fragment' execution_count=4}\n``` {.python .cell-code}\nmy_dog = Dog(\"Fido\", \"Labrador\") ## create a dog of name 'Fido' and breed 'Labrador'\nprint(my_dog.bark())\n\n## if we want to see what kind of dog our dog is\nprint(f\"Our dog {my_dog.name} is a {my_dog.breed}.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFido says Woof!\nOur dog Fido is a Labrador.\n```\n:::\n:::\n\n\n## Derived (Child) Classes  \n\nNow that we have a `Dog` class, let‚Äôs define a new class called `GuardDog`. This class will inherit all the properties and methods from `Dog`, while also adding its own unique attributes and behaviors.\n\n<span style=\"color: #007acc\"><strong>This is the power of inheritance</span></strong>‚Äîwe don‚Äôt have to rewrite everything from scratch! Instead, we can extend the existing functionality of `Dog` to create a more specialized class.\n\n---\n\nWe can extend the `Dog` class to create a `GuardDog` class. \n\n::: {#2caadf4c .cell execution_count=5}\n``` {.python .cell-code code-line-numbers=\"|1|2-6|\"}\nclass GuardDog(Dog):  # GuardDog inherits from Dog\n    def __init__(self, name, breed, training_level): ## in addition to name and breed, we can \n        # define a training level. \n        # Call the parent (Dog) class's __init__ method\n        super().__init__(name, breed)\n        self.training_level = training_level  # New attribute for GuardDog that stores the \n        # training level for the dog\n\n    def guard(self): ## checks if the training level is > 5 and if not says train more\n        if self.training_level > 5:\n            return f\"{self.name} is guarding the house!\"\n        else:\n            return f\"{self.name} needs more training before guarding.\"\n    \n    def train(self): ## modifies the training_level attribute to increase the dog's training level\n        self.training_level = self.training_level + 1\n        return f\"Training {self.name}. {self.name}'s training level is now {self.training_level}\"\n\n# Creating an instance of GuardDog\nmy_guard_dog = GuardDog(\"Rex\", \"German Shepherd\", training_level= 5)\n```\n:::\n\n\n---\n\nNow that we have a dog (my_guard_dog), we can call on any of the methods introduced in the `Dog` class as well as the new `GuardDog` class.\n\n::: {#fc3874eb .cell output-location='fragment' execution_count=6}\n``` {.python .cell-code}\n# Using methods from the base class\nprint(my_guard_dog.bark())  # Inherited from Dog -> Output: \"Rex says Woof!\"\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRex says Woof!\n```\n:::\n:::\n\n\n::: {#2404e555 .cell output-location='fragment' execution_count=7}\n``` {.python .cell-code}\n# Using a method from the derived class\nprint(f\"{my_guard_dog.name}'s training level is {my_guard_dog.training_level}.\")\nprint(my_guard_dog.guard()) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRex's training level is 5.\nRex needs more training before guarding.\n```\n:::\n:::\n\n\n::: {#0fbc971b .cell output-location='fragment' execution_count=8}\n``` {.python .cell-code}\n## if we want to train Rex and increase his training level, \nprint(my_guard_dog.train())\n\n## now check if he can guard \nprint(my_guard_dog.guard()) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining Rex. Rex's training level is now 6\nRex is guarding the house!\n```\n:::\n:::\n\n\n---\n\n\nAs we saw with Rex, child classes inherit all attributes (`.name` and `.breed`) and methods (`.bark()`) from parent classes. They can also have new methods (`.train()`).\n\n## Mixins\nA **mixin** is a special kind of class designed to add **functionality** to another class. Unlike base classes, mixins aren‚Äôt used alone.  \n\nFor example, scikit-learn uses mixins like:  \n- `sklearn.base.ClassifierMixin` (adds classifier-specific methods)  \n- `sklearn.base.RegressorMixin` (adds regression-specific methods)  \n\nwhich it adds to the `BaseEstimator` class to add functionality.\n\nTo finish up our dog example, we are going to define a mixin class that adds a functionality to the base `Dog()` class which allows us to teach a dog tricks. \n\n---\n\n- When creating a mixin class, we let the other base classes carry most of the initialization\n- However, we can add other attributes like `.tricks`, which stores the set of learned tricks\n- We can also add methods to define additional behaviors\n\n::: {#e886da3f .cell execution_count=9}\n``` {.python .cell-code code-line-numbers=\"|2,3|4|6-17|\"}\nclass TrickMixin: ## mixin that will let us teach a dog tricks\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)  # Ensures proper initialization in multiple inheritance\n        self.tricks = []  # Store learned tricks\n\n    def learn_trick(self, trick):\n        \"\"\"Teaches the dog a new trick.\"\"\"\n        if trick not in self.tricks:\n            self.tricks.append(trick)\n            return f\"{self.name} learned a new trick: {trick}!\"\n        return f\"{self.name} already knows {trick}!\"\n\n    def perform_tricks(self):\n        \"\"\"Returns a list of tricks the dog knows.\"\"\"\n        if self.tricks:\n            return f\"{self.name} can perform: {', '.join(self.tricks)}.\"\n        return f\"{self.name} hasn't learned any tricks yet.\"\n\n## note: the TrickMixin class is not a standalone class! it does not let us create a dog on its own!!!\n```\n:::\n\n\n. . . \n\nUsing this Trick mixin, we can then create a new class of dog (SmartDog) using both `Dog`and `TrickMixin` as base classes. \n\n---\n\nBy including the \n\n::: {#14cd9758 .cell output-location='fragment' execution_count=10}\n``` {.python .cell-code}\nclass SmartDog(Dog, TrickMixin):\n    def __init__(self, name, breed):\n        super().__init__(name, breed)  # Initialize Dog class\n        TrickMixin.__init__(self)  # Initialize TrickMixin separately\n\n# Creating a SmartDog that can learn tricks\nmy_smart_dog = SmartDog(\"Buddy\", \"Border Collie\")\n\n# a SmartDog object can use methods from both parent object `Dog` and mixin `TrickMixin`.\nprint(my_smart_dog.bark()) \nprint(my_smart_dog.learn_trick(\"Sit\"))  \nprint(my_smart_dog.learn_trick(\"Roll Over\")) \nprint(my_smart_dog.learn_trick(\"Sit\"))  \nprint(my_smart_dog.perform_tricks()) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuddy says Woof!\nBuddy learned a new trick: Sit!\nBuddy learned a new trick: Roll Over!\nBuddy already knows Sit!\nBuddy can perform: Sit, Roll Over.\n```\n:::\n:::\n\n\n. . . \n\nWhile our dog example was very simple, this is the same way that model classes work in python. \n\n## **OOP In ML Recap**   \n\nUnderstanding **base classes** and **mixins** is especially important when working with deep learning frameworks like **PyTorch and TensorFlow**, as they allow for easy customization of models.  \n\nBy using **object-oriented programming**, Python makes it easy to **structure machine learning workflows** in a reusable and scalable way. The fact that all ML models in scikit-learn follow the **same structure** (with `.fit()`, `.predict()`, `.score()`, etc.) makes it easier to switch between models and automate processes. Additionally, the model classes in the statsmodels package have many of the same methods (`.fit()`, `.predict()`, `.score()`). \n\n# Part B - Demo Projects\n\n## **üêß Mini Project: Classifying Penguins with scikit-learn**\n\nNow that you understand **classes** and **data structures** in Python, let‚Äôs apply that knowledge!\n\nIn this project, we‚Äôll try to classify **penguin species** using two features:  \n- `bill_length_mm`  \n- `bill_depth_mm`  \n\nWe‚Äôll explore:\n- **Unsupervised learning** with **K-Means** clustering (model doesn't 'know' y)\n- **Supervised learning** with a **k-NN classifier** (model trained w/ y information)\n\n\n## **üîç Modeling with scikit-learn Classes**\n\nWe'll use models from **scikit-learn**, which are built using object-oriented design.  \nEach model is an **instance of a class** (inheriting from `BaseEstimator`) with:\n\n**Common Methods:**  \n- `.fit()` ‚Äî Train the model  \n- `.predict()` ‚Äî Make predictions  \n\n**Common Attributes:**  \n- `.get_params()`, `.classes_`, `.n_clusters_`, etc.  \n\n> We're using `scikit-learn` here for its simplicity, but the concepts apply to more advanced frameworks like **PyTorch** and **TensorFlow** too!\n\n## General Modeling Workflow\n\n::: columns\n::: column\n\n**Step 0: Prepare Workspace**  \n- Import necessary libraries/modules:  \n  - **Functions** (e.g., `train_test_split`, `accuracy_score`)  \n  - **Classes** (e.g., `KMeans`, `KNeighborsClassifier`)  \n\n**Step 1: Data Preparation**  \n- Load data (`pandas`)  \n- Clean data (`pandas`, `numpy`)  \n- Transform/scale features (`sklearn.preprocessing`)  \n- Optionally split data into training and testing sets  \n\n**Step 2: Initialize the Model**  \n- Create an instance of the model class (`KMeans`, `KNeighborsClassifier`)  \n- Set parameters during instantiation (e.g., `n_clusters=3`, `n_neighbors=5`)  \n\n:::\n\n::: column\n\n**Step 3: Fit the Model**  \n- Use `.fit(X)` for unsupervised models  \n- Use `.fit(X_train, y_train)` for supervised models  \n\n**Step 4: Make Predictions** *(optional)*  \n- Use `.predict(X_test)` to generate predictions  \n- Use `.predict_proba()` to get class probabilities (if available)  \n\n**Step 5: Evaluate Model Performance**  \n- Compare predictions to true values  \n- Use visualizations or metrics (e.g., accuracy, ARI, classification report)  \n\n:::\n:::\n\n\n## Step 0: Import Libraries\n\nBefore any analysis, we must import the necessary libraries.  \n\nFor large libraries like **scikit-learn**, **PyTorch**, or **TensorFlow**, we usually do **not** import the entire package. Instead, we selectively import the **classes** and **functions** we need.\n\n. . . \n\n> üî§ **Naming Tip**:  \n> - `CamelCase` = **Classes**  \n> - `snake_case` = **Functions**\n\n--- \n\nIn this project, we‚Äôll use the following:\n\n**Classes**  \n- `StandardScaler` ‚Äî for feature scaling  \n- `KNeighborsClassifier` ‚Äî for supervised k-NN classification  \n- `KMeans` ‚Äî for unsupervised clustering\n\n**Functions**  \n- `train_test_split()` ‚Äî to split data into training and test sets  \n- `accuracy_score()` ‚Äî to evaluate classification accuracy  \n- `classification_report()` ‚Äî to print precision, recall, and F1  \n- `adjusted_rand_score()` ‚Äî to evaluate clustering performance\n\n\n## Import Libraries\n\n::: {#f710f392 .cell execution_count=11}\n``` {.python .cell-code code-line-numbers=\"|12|14-17|19-21|\"}\n## imports\nimport pandas as pd\nimport numpy as np\n\nfrom plotnine import *\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom great_tables import GT\nfrom tabulate import tabulate\n\n## sklearn imports\n\n## import classes\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\n\n## import functions\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, adjusted_rand_score\n```\n:::\n\n\n## Step 1: Data Preparation\n\n::: {#32f2a490 .cell output-location='slide' execution_count=12}\n``` {.python .cell-code code-line-numbers=\"1,2|4-11|13-16|18-19|\"}\n# Load the Penguins dataset\npenguins = sns.load_dataset(\"penguins\").dropna()\n\n# Make a summary table for the penguins dataset, grouping by species. \nsummary_table = penguins.groupby(\"species\").agg({\n    \"bill_length_mm\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"bill_depth_mm\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"flipper_length_mm\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"body_mass_g\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"sex\": lambda x: x.value_counts().to_dict()  # Count of males and females\n})\n\n# Round numeric values to 1 decimal place (excluding the 'sex' column)\nfor col in summary_table.columns:\n    if summary_table[col].dtype in [float, int]:\n        summary_table[col] = summary_table[col].round(1)\n\n# Display the result\ndisplay(summary_table)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"4\" halign=\"left\">bill_length_mm</th>\n      <th colspan=\"4\" halign=\"left\">bill_depth_mm</th>\n      <th colspan=\"4\" halign=\"left\">flipper_length_mm</th>\n      <th colspan=\"4\" halign=\"left\">body_mass_g</th>\n      <th>sex</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>max</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>max</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>max</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>max</th>\n      <th>&lt;lambda&gt;</th>\n    </tr>\n    <tr>\n      <th>species</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Adelie</th>\n      <td>38.8</td>\n      <td>2.7</td>\n      <td>32.1</td>\n      <td>46.0</td>\n      <td>18.3</td>\n      <td>1.2</td>\n      <td>15.5</td>\n      <td>21.5</td>\n      <td>190.1</td>\n      <td>6.5</td>\n      <td>172.0</td>\n      <td>210.0</td>\n      <td>3706.2</td>\n      <td>458.6</td>\n      <td>2850.0</td>\n      <td>4775.0</td>\n      <td>{'Male': 73, 'Female': 73}</td>\n    </tr>\n    <tr>\n      <th>Chinstrap</th>\n      <td>48.8</td>\n      <td>3.3</td>\n      <td>40.9</td>\n      <td>58.0</td>\n      <td>18.4</td>\n      <td>1.1</td>\n      <td>16.4</td>\n      <td>20.8</td>\n      <td>195.8</td>\n      <td>7.1</td>\n      <td>178.0</td>\n      <td>212.0</td>\n      <td>3733.1</td>\n      <td>384.3</td>\n      <td>2700.0</td>\n      <td>4800.0</td>\n      <td>{'Female': 34, 'Male': 34}</td>\n    </tr>\n    <tr>\n      <th>Gentoo</th>\n      <td>47.6</td>\n      <td>3.1</td>\n      <td>40.9</td>\n      <td>59.6</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>13.1</td>\n      <td>17.3</td>\n      <td>217.2</td>\n      <td>6.6</td>\n      <td>203.0</td>\n      <td>231.0</td>\n      <td>5092.4</td>\n      <td>501.5</td>\n      <td>3950.0</td>\n      <td>6300.0</td>\n      <td>{'Male': 61, 'Female': 58}</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Scaling the data - Understanding the Standard Scaler class\n\nFor our clustering to work well, the predictors should be on the same scale.\n\nTo achieve this, we use an instance of the `StandardScaler` class. \n\n\n## Standard Scaler\n\n```python\nclass sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)\n```\n<br>\n. . .  \n\n**Parameters** are supplied by user  \n- copy, with_mean, with_std\n<br>\n\n**Attributes** contain the `data` of the object  \n- `scale_`: scaling factor\n- `mean_`: mean value for each feature\n- `var_`: variance for each feature\n- `n_features_in_`: number of features seen nduring fit\n- `n_samples_seen`: number of samples processed for each feature\n<br>\n\n**Methods** describe the `behaviors` of the object and/or `modify` its attributes  \n- `fit(X)` -> compute mean and std used for scaling -> fit scaler to data X\n    * updates the attributes of the scaler object\n- `transform(X)` -> perform standardization by centering and scaling with fitted scaler\n\n## Data Preparation\n\n::: {#232d9849 .cell execution_count=13}\n``` {.python .cell-code code-line-numbers=\"1-3|5-7|\"}\n# Selecting features for clustering -> let's just use bill length and bill depth.\nX = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]]\ny = penguins[\"species\"]\n\n# Standardizing the features for better clustering performance\nscaler = StandardScaler() ## create instance of StandardScaler\nX_scaled = scaler.fit_transform(X) ## same as calling scaler.fit(X) then X_scaled = scaler.transform(X)\n```\n:::\n\n\n## Understanding the KMeans model class\n\n```python\nclass sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, \ntol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')\n```\n<br>\n**Parameters**: Set by user at time of instantiation  \n- n_clusters, max_iter, algorithm  \n<br>\n**Attributes**: Store object data    \n- `cluster_centers_`: stores coordinates of cluster centers  \n- `labels_`: stores labels of each point \n- `n_iter_`: number of iterations run (will be changed during method run)  \n- `n_features_in` and `feature_names_in_`: store info about features seen during fit  \n<br>\n**Methods**: Define object behaviors      \n- `fit(X)` -> same as usage as `train()` -> fit model to data X  \n- `predict(X)` -> predict closest cluster each sample in X belongs to  \n- `transform(X)` -> transform X to cluster-distance space  \n\n## Step 2: Create model\n\n::: {#a7eb4688 .cell output-location='fragment' execution_count=14}\n``` {.python .cell-code code-line-numbers=\"1-2|4-6|\"}\n## Choosing 3 clusters b/c we have 3 species\nkmeans = KMeans(n_clusters=3, random_state=42) ## make an instance of the K means class\n```\n:::\n\n\n## Step 3: Fit model to data\n\n::: {#843a6419 .cell output-location='fragment' execution_count=15}\n``` {.python .cell-code code-line-numbers=\"1-2|4-5|\"}\n## the fit\npenguins[\"kmeans_cluster\"] = kmeans.fit_predict(X_scaled)\n\n## now that we fit the model, we should have cluster centers\nprint(\"Coordinates of cluster centers:\", kmeans.cluster_centers_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoordinates of cluster centers: [[-0.95023997  0.55393493]\n [ 0.58644397 -1.09805504]\n [ 1.0886843   0.79503579]]\n```\n:::\n:::\n\n\n## Step 5: Visualize and Evaluate\n\nTo do visualization, we can use either seaborn or plotnine. Plotnine is nice because it is just a python port of ggplot!\n\nTo take at the distribution of our species by bill length and bill depth...\n\n::: {#81971fea .cell output-location='fragment' execution_count=16}\n``` {.python .cell-code}\n# Plotnine scatterplot of species by bill length/depth\nplot1 = (ggplot(penguins, aes(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"species\"))\n + geom_point()\n + ggtitle(\"Penguin Species\")\n + theme_bw())\n\ndisplay(plot1)\n```\n\n::: {.cell-output .cell-output-display}\n![](session4v2_files/figure-revealjs/cell-17-output-1.png){width=960 height=480}\n:::\n:::\n\n\n---\n\nWe can also look at the K-means clustering results\n\n::: {#f99d8edd .cell output-location='fragment' execution_count=17}\n``` {.python .cell-code}\n# Plotnine scatterplot of k-Means clusters\nplot2 = (ggplot(penguins, aes(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"factor(kmeans_cluster)\"))\n + geom_point()\n + ggtitle(\"K-Means Clustering Results\")\n + theme_bw())\n\ndisplay(plot2)\n```\n\n::: {.cell-output .cell-output-display}\n![](session4v2_files/figure-revealjs/cell-18-output-1.png){width=960 height=480}\n:::\n:::\n\n\n---\n\n## Use function to calculate ARI\n\nTo check how good our model is, we can use one of the functions included in the sklearn library\n\n::: {#b8c4f1c3 .cell output-location='fragment' execution_count=18}\n``` {.python .cell-code}\n# Calculate clustering performance using Adjusted Rand Index (ARI)\nkmeans_ari = adjusted_rand_score(penguins['species'], penguins[\"kmeans_cluster\"])\nprint(f\"k-Means Adjusted Rand Index: {kmeans_ari:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nk-Means Adjusted Rand Index: 0.82\n```\n:::\n:::\n\n\n## We can also use methods on our data structure to create new data\n\n- we can use this to plot cluster agreement with species label\n\n::: {#c17a9099 .cell output-location='slide' execution_count=19}\n``` {.python .cell-code code-line-numbers=\"1-3|5-20|\"}\n# Count occurrences of each species-cluster-sex combination\n# ( .size gives the count as index, use reset_index to get count column. )\nscatter_data = penguins.groupby([\"species\", \"kmeans_cluster\", \"sex\"]).size().reset_index(name=\"count\")\n\n# Create a heatmap of the cluster assignments by species\nheatmap_plot = (\n    ggplot(scatter_data, aes(x=\"species\", y=\"kmeans_cluster\", fill=\"count\"))\n    + geom_tile(color=\"white\")  # Add white grid lines for separation\n    + scale_fill_gradient(low=\"lightblue\", high=\"darkblue\")  # Heatmap colors\n    + labs(\n        title=\"Heatmap of KMeans Clustering by Species\",\n        x=\"Species\",\n        y=\"KMeans Cluster\",\n        fill=\"Count\"\n    )\n    + theme_bw()\n)\n\n# Display the plot\ndisplay(heatmap_plot)\n```\n\n::: {.cell-output .cell-output-display}\n![](session4v2_files/figure-revealjs/cell-20-output-1.png){width=960 height=480}\n:::\n:::\n\n\n---\n\n\n- If we want to add sex as a variable to see if that is why our clusters don't agree with our species, we can use a scatterplot\n\n::: {#ca8fdff7 .cell output-location='fragment' execution_count=20}\n``` {.python .cell-code}\nscatter_plot = (\n    ggplot(scatter_data, aes(x=\"species\", y=\"kmeans_cluster\", color=\"species\", shape=\"sex\", size=\"count\"))\n    + geom_point(alpha=0.7, position=position_dodge(width=0.5))  # Horizontal separation\n    + scale_size(range=(2, 10))  # Adjust point sizes\n    + scale_y_continuous(breaks=[0, 1, 2])  # Set y-axis ticks to only 0, 1, 2\n    + theme_bw()\n    + labs(\n        title=\"KMeans Clustering vs Species (Size = Count)\",\n        x=\"Species\",\n        y=\"KMeans Cluster\"\n    )\n)\n#Display the plot\ndisplay(scatter_plot)\n```\n\n::: {.cell-output .cell-output-display}\n![](session4v2_files/figure-revealjs/cell-21-output-1.png){width=960 height=480}\n:::\n:::\n\n\n## Project 2 -> KNN classification\n\nFor our KNN classification, the model is supervised (meaning it is dependent on the outcome 'y' data) and therefore we need to split into a training and test set. \n\n. . . \n\nThe **function** train_test_split() from scikit-learn is helpful here! Our classifier object has built in methods for fitting models and predicting.\n\n::: {#b5890bba .cell execution_count=21}\n``` {.python .cell-code}\n# Splitting dataset into training and testing sets (still using scaled X!)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n```\n:::\n\n\n## Understanding KNeighborsClassifier class\n```{.python}\nclass sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', \nalgorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n```\n. . . \n\n**Parameters**: Set by user at time of instantiation    \n- n_neigbors, weights, algorithm, etc.  \n\n<br>\n\n**Attributes**: Store object data     \n- `classes_`: class labels known to the classifier  \n- `effective_metric_`: distance metric used  \n- `effective_metric_params_`: parameters for the metric function  \n- `n_features_in` and `feature_names_in_`: store info about features seen during fit    \n- `n_samples_fit_`: number of samples in fitted data  \n\n<br> \n\n**Methods**: Define object behaviors      \n- `.fit(X, y)` -> fit knn classifier from training dataset (X and y)  \n- `.predict(X)` -> predict class labels for provided data X  \n- `.predict_proba(X)` -> return probability estimates for test data X  \n- `.score(X, y)` -> return mean accuracy on given test data X and labels y  \n\n---\n\n## Making an instance of KNeighborsClassifier and fitting to training data\n- For a supervised model, y_train is included in `.fit()`!\n\n::: {#044617b1 .cell execution_count=22}\n``` {.python .cell-code}\n## perform knn classification\n# Applying k-NN classification with 5 neighbors\nknn = KNeighborsClassifier(n_neighbors=5) ## make an instance of the KNeighborsClassifier class\n# and set the n_neighbors parameter to be 5. \n\n# Use the fit method to fit the model to the training data\nknn.fit(X_train, y_train)\n```\n:::\n\n\n## Once the model is fit...\n\n- Once the model is fit, we can look at its attributes (ex: `.classes_') which gives the class labels as known to the classifier\n\n::: {#df62ef5f .cell output-location='fragment' execution_count=23}\n``` {.python .cell-code}\nprint(knn.classes_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Adelie' 'Chinstrap' 'Gentoo']\n```\n:::\n:::\n\n\n. . . \n\n::: {#f58f6be9 .cell output-location='fragment' execution_count=24}\n``` {.python .cell-code}\n# Use the predict method on the test data to get the predictions for the test data\ny_pred = knn.predict(X_test)\n\n# Also can take a look at the prediction probabilities, \n# and use the .classes_ attribute to put the column labels in the right order\nprobs = pd.DataFrame(\n    knn.predict_proba(X_test),\n    columns = knn.classes_)\nprobs['y_pred'] = y_pred\n\nprint(\"Predicted probabilities: \\n\", probs.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredicted probabilities: \n    Adelie  Chinstrap  Gentoo     y_pred\n0     1.0        0.0     0.0     Adelie\n1     0.0        0.0     1.0     Gentoo\n2     1.0        0.0     0.0     Adelie\n3     0.0        0.6     0.4  Chinstrap\n4     1.0        0.0     0.0     Adelie\n```\n:::\n:::\n\n\n## Plotnine scatterplot for k-NN classification of test data\n\n- Create dataframe of unscaled X_test `bill_length_mm` and `bill_depth_mm`.\n- Add to it the actual and predicted species labels\n\n::: {#ce8a9ec2 .cell execution_count=25}\n``` {.python .cell-code}\n## First unscale the test data\nX_test_unscaled = scaler.inverse_transform(X_test)\n\n## create dataframe \npenguins_test = pd.DataFrame(\n    X_test_unscaled,\n    columns=['bill_length_mm', 'bill_depth_mm']\n)\n\n## add actual and predicted species \npenguins_test['y_actual'] = y_test.values\npenguins_test['y_pred'] = y_pred\npenguins_test['correct'] = penguins_test['y_actual'] == penguins_test['y_pred']\n```\n:::\n\n\n## Plotnine scatterplot for k-NN classification of test data\n\n::: {#3df4d420 .cell output-location='fragment' execution_count=26}\n``` {.python .cell-code}\n## Build the plot\nplot3 = (ggplot(penguins_test, aes(x=\"bill_length_mm\", y=\"bill_depth_mm\", \ncolor=\"y_actual\", fill = 'y_pred', size = 'correct'))\n + geom_point()\n + scale_size_manual(values={True: 2, False: 5})\n + ggtitle(\"k-NN Classification Results\")\n + theme_bw())\n\ndisplay(plot3)\n```\n\n::: {.cell-output .cell-output-display}\n![](session4v2_files/figure-revealjs/cell-27-output-1.png){width=960 height=480}\n:::\n:::\n\n\n## Evaluate KNN performance\n\n::: {#6d9dce52 .cell output-location='fragment' execution_count=27}\n``` {.python .cell-code}\n## eval knn performance\n# Calculate accuracy and print classification report -> \n# accuracy_score and classification_report are functions! \nknn_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"k-NN Accuracy: {knn_accuracy:.2f}\")\nprint(classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nk-NN Accuracy: 0.94\n              precision    recall  f1-score   support\n\n      Adelie       0.98      0.98      0.98        48\n   Chinstrap       0.80      0.89      0.84        18\n      Gentoo       0.97      0.91      0.94        34\n\n    accuracy                           0.94       100\n   macro avg       0.92      0.93      0.92       100\nweighted avg       0.94      0.94      0.94       100\n\n```\n:::\n:::\n\n\n## Make a Summary Table of Metrics for Both Models\n\n::: {#29748c91 .cell output-location='fragment' execution_count=28}\n``` {.python .cell-code}\n##  making a summary table\n# Creating a summary table\nsummary_table = pd.DataFrame({\n    \"Metric\": [\"k-Means Adjusted Rand Index\", \"k-NN Accuracy\"],\n    \"Value\": [kmeans_ari, knn_accuracy]\n})\nGT(summary_table)\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```{=html}\n<div id=\"ctszcpyyhd\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>\n#ctszcpyyhd table {\n          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n          -webkit-font-smoothing: antialiased;\n          -moz-osx-font-smoothing: grayscale;\n        }\n\n#ctszcpyyhd thead, tbody, tfoot, tr, td, th { border-style: none; }\n tr { background-color: transparent; }\n#ctszcpyyhd p { margin: 0; padding: 0; }\n #ctszcpyyhd .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n #ctszcpyyhd .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n #ctszcpyyhd .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n #ctszcpyyhd .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n #ctszcpyyhd .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #ctszcpyyhd .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #ctszcpyyhd .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #ctszcpyyhd .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n #ctszcpyyhd .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n #ctszcpyyhd .gt_column_spanner_outer:first-child { padding-left: 0; }\n #ctszcpyyhd .gt_column_spanner_outer:last-child { padding-right: 0; }\n #ctszcpyyhd .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n #ctszcpyyhd .gt_spanner_row { border-bottom-style: hidden; }\n #ctszcpyyhd .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n #ctszcpyyhd .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n #ctszcpyyhd .gt_from_md> :first-child { margin-top: 0; }\n #ctszcpyyhd .gt_from_md> :last-child { margin-bottom: 0; }\n #ctszcpyyhd .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n #ctszcpyyhd .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n #ctszcpyyhd .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n #ctszcpyyhd .gt_row_group_first td { border-top-width: 2px; }\n #ctszcpyyhd .gt_row_group_first th { border-top-width: 2px; }\n #ctszcpyyhd .gt_striped { background-color: rgba(128,128,128,0.05); }\n #ctszcpyyhd .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #ctszcpyyhd .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n #ctszcpyyhd .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n #ctszcpyyhd .gt_left { text-align: left; }\n #ctszcpyyhd .gt_center { text-align: center; }\n #ctszcpyyhd .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n #ctszcpyyhd .gt_font_normal { font-weight: normal; }\n #ctszcpyyhd .gt_font_bold { font-weight: bold; }\n #ctszcpyyhd .gt_font_italic { font-style: italic; }\n #ctszcpyyhd .gt_super { font-size: 65%; }\n #ctszcpyyhd .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n #ctszcpyyhd .gt_asterisk { font-size: 100%; vertical-align: 0; }\n \n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n<thead>\n\n<tr class=\"gt_col_headings\">\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Metric\">Metric</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Value\">Value</th>\n</tr>\n</thead>\n<tbody class=\"gt_table_body\">\n  <tr>\n    <td class=\"gt_row gt_left\">k-Means Adjusted Rand Index</td>\n    <td class=\"gt_row gt_right\">0.8203520973164866</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">k-NN Accuracy</td>\n    <td class=\"gt_row gt_right\">0.94</td>\n  </tr>\n</tbody>\n\n\n</table>\n\n</div>\n        \n```\n:::\n:::\n\n\n## **Key Takeaways from This Session**  \nüîπ **Machine learning models in Python are objects** ‚Äì you create an instance of a class.   \nüîπ We use a combination of functions and objects for this...  \nüîπ **OOP enables code reuse and modularity** ‚Äì `.fit()`, `.predict()`, and `.transform()` are standard across models.    \nüîπ **Understanding inheritance and mixins** is important for customizing models in PyTorch and TensorFlow.    \n\nPython‚Äôs OOP approach makes machine learning **efficient, reusable, and scalable**. üöÄ  \n\n## **Pre-Reading for This Session**  \n- [Scikit-learn Documentation](https://scikit-learn.org/stable/user_guide.html)  \n- [Introduction to OOP in Python (Real Python)](https://realpython.com/python3-object-oriented-programming/)  \n- [Plotnine Reference](https://plotnine.org/reference/)\n\n",
    "supporting": [
      "session4v2_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}