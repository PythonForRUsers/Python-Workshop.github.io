{
  "hash": "866d3abfdfccc0ee9a3d0940e9d0d11e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: '**Session 4 – Object-Oriented Programming and Modeling Libraries**'\njupyter: python3\nformat: \n    revealjs:\n        code-copy: true   \n        smaller: true\n        code-block-height: 750px\n        highlight-style: pygments\n        width: 1400   # default is 960\n        height: 800  # default is 700\n        css: quarto\\session4_newMLDemo\\slideshowv2.scss\nexecute:\n  freeze: auto\n  eval: true\n  echo: true\n---\n\n\n## Session Overview\n\n<br>\n\n<p style=\"font-size: 1.15em\">\nIn this session, we'll explore how Python's object-oriented nature affects our modeling workflows. \n</p><br>\n\n<p style=\"font-size: 1.25em\">\n<strong>Topics:</strong>\n</p>\n<ul style=\"font-size: 1.15em; line-height: 1.6; max-width: 700px; margin: 0 auto;\">\n  <li><strong>Intro to OOP</strong> and how it makes modeling in Python different from R</li>\n  <li><strong>Building and extending classes</strong> using inheritance and mixins</li>\n  <li><strong>Applying OOP to machine learning</strong> through demos with scikit-learn</li>\n</ul>\n\n\n# Introduction\n\n## Why Python? 🐍\n\n::: columns\n::: column\n\n#### R: Built by Statisticians for Statisticians\n- Excels at:\n    - Statistical analysis and modeling  \n    - Clean outputs and tables from models\n    - Beautiful data visualizations with simple code  \n\n\n:::\n\n::: column\n\n#### Python: General-Purpose Language\n- Excels at: \n    - Machine Learning, Neural Networks & Deep Learning (scikit-learn, PyTorch, TensorFlow)   \n    - Image & Genomic Data Analysis (scikit-image, biopython, scanpy)\n    - Software & Command Line Interfaces, Web Scraping, Automation\n\n:::\n:::\n\n\nPython’s broader ecosystem makes it the go-to language in domains like AI, bioinformatics, data engineering, and computational biology.  \n\n> **Note:** Packages like `rpy2` and `reticulate` make it possible to use both R and Python in the same project, but those are beyond the scope of this course.  \n> A primer on `reticulate` is available here: [https://www.r-bloggers.com/2022/04/getting-started-with-python-using-r-and-reticulate/](https://www.r-bloggers.com/2022/04/getting-started-with-python-using-r-and-reticulate/)\n\n## Programming Styles: R vs Python\n<br>\nIn the first session, we talked briefly about functional vs object-oriented programming:   \n<br>\n\n>  <span style=\"color: #007acc\"><strong>Functional programming:</strong></span> focuses on functions as the primary unit of code <br>\n>  <span style=\"color: #007acc\"><strong>Object-oriented programming:</strong></span> uses objects with attached attributes(data) and methods(behaviors) <br>\n\n- R leans heavily on the functional paradigm — you pass data into functions and get back results, in most cases without altering the original data. Functions and pipes (%>%) dominate most workflows.\n\n- In Python, <span style=\"color: #007acc\"><strong>everything is an object</strong></span>, even basic things like lists, strings, and dataframes, and a lot of 'functions' are written as object-associated methods. Some of these methods modify the objects in-place by altering the attached data (attributes). <span style=\"color: #007acc; font-weight:bold\">Understanding how this works is key to using Python effectively!</span>\n\n> You’ve already seen this object-oriented style in Sessions 2 and 3 — you create objects like lists or dataframes, then call methods on them like `.append()` or `.sort_values()`. In python, instead of piping, we sometimes chain methods together.\n\n## Functions vs Objects in Python\n<br>\n\n<span style=\"color: #007acc\"><strong>Python absolutely uses **functions**—just like R!</strong></span>\nThey're helpful for **data transformation**, **wrangling**, and **automation tasks** like looping and parallelization. <br>\n\nBut when it comes to **modeling**, libraries are designed around **classes**: blueprints for creating objects that store data (**attributes**) and define behaviors (**methods**).  <br>\n\n  - `scikit-learn` is great for getting started—everything from regression to clustering follows a simple, consistent OOP interface. Its API is also consistant with other python modeling packages, like [xgboost](https://xgboost.readthedocs.io/en/release_3.0.0/) for gradient boosting and [scvi-tools](https://docs.scvi-tools.org/en/stable/index.html) for transcriptomics data.\n  - [<u>`scikit-survival`</u>](https://scikit-survival.readthedocs.io/en/stable/) is built off \n  - `PyTorch` and `TensorFlow` are essential if you go deeper into neural networks or custom models—you’ll define your **own model classes** with attributes and methods, but the basic structure is similar to `scikit-learn`.  \n  - [<u>`statsmodels`</u>](https://www.statsmodels.org/stable/gettingstarted.html) is an alternative to `scikit-learn` for statistical analyses and has R-like syntax and outputs. It's a bit more complex than `scikit-learn` and a bit less consistant with other packages in the python ecosystem.\n\n> 💡 To work effectively in Python, especially for tasks involving modeling or model training, <span style=\"color: #007acc\"><strong>it helps to think in terms of objects and classes, not just functions.</strong></span>   \n\n## Why Does OOP Matter in Python Modeling?\n\n<p style=\"font-size: 1.15em; margin-top: 0.5em; margin-bottom: 0.1em;\">\n<strong>In Python modeling frameworks:</strong>\n</p>\n\n<ul style=\"font-size: 1.05em; line-height: 1.15; margin-top: 0;\">\n  <li>Models are <strong>instances of classes</strong></li>\n  <li>You call methods like <code>.fit()</code>, <code>.predict()</code>, <code>.score()</code></li>\n  <li>Internal model details like coefficients or layers are stored as <strong>attributes</strong></li>\n</ul>\n\n<p style=\"font-size: 1.05em; margin-top: 0.5em; margin-bottom: 0.75em;\">\nThis makes model behavior <strong>consistent</strong> and helps manage complexity of things like pipelines that are designed to work for multiple model classes. It also <strong>simplifies</strong> creating/using pre-trained models: both the architecture and learned weights are bundled into a single object with built-in methods like `.predict()` or `.fine_tune()`.\n</p>\n\n<blockquote style=\"font-size: 1.05em; margin-top: 1em;\">\n If you're used to <code>lm()</code> or <code>glm()</code> in R returning a list of values, think of the Python approach as a self-contained object with named methods and stored results. Instead of having a separate results object, like in R, you would retrieve your results by accessing an attribute that is stored in the model object itself. \n</blockquote>\n<br>\n\n<span style=\"font-size: 1.15em;color: #007acc\"><strong><em>We’ll focus on `scikit-learn` here for the sake of simplicity, but feel free to explore other libraries!</em></strong></span> <br>  \n*[https://wesmckinney.com/book/modeling](https://wesmckinney.com/book/modeling) is a good tutorial for statsmodels.*\n\n\n# Part 1: Object-Oriented Programming\n\n## **Key OOP Principles (Recap)** \n\nIn OOP, code is structured around **objects** (as opposed to functions). This paradigm builds off the following principles: \n\n. . .  \n\n1. **Encapsulation**: Bundling data and methods together in a single unit.  \n   - A `StandardScaler` object stores mean and variance data and has `.fit()` and `.transform()` methods\n\n. . . \n\n2. **Inheritance**: Creating new classes based on existing ones.  \n   - `sklearn.LinearRegression` inherits attributes and methods from a general regression model class.    \n\n. . . \n\n3. **Abstraction**: Hiding implementation details and exposing only essential functionality.  \n   - e.g., `.fit()` works the same way from the outside, regardless of model complexity  \n\n\n. . . \n\n4. **Polymorphism**: Objects of different types can be treated the same way if they implement the same methods. \n    - Python’s **duck typing**:  \n      - *\"If it walks like a duck and quacks like a duck, then it must be a duck.\"*  \n      - ex: If different objects all have a .summarize() method, we can loop over them and call .summarize() without needing to check their class. As long as the method exists, Python will know what to do.\n      - This lets us easily create [<u>pipelines</u>](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) that can work for many types of models. \n\n> We won't cover [pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) here, but they are worth looking into!\n\n\n## **Classes and Objects**\n\n<br>\n\n**Classes** are **blueprints** for creating objects. Each object contains:  \n<ul style=\"font-size: 1.05em; line-height: 1.6;\">\n  <li><strong>Attributes</strong> (data): model coefficients, class labels</li>\n  <li><strong>Methods</strong> (behaviors): <code>.fit()</code>, <code>.predict()</code></li>\n</ul>\n\n<br>\n\n👉 To check if an object is an instance of a particular class, use:\n\n::: {#00ae888a .cell execution_count=1}\n``` {.python .cell-code}\nisinstance(object, class)  # Returns True if `object` is an instance of `class`.\n```\n:::\n\n\n<br>\n\n<p style=\"color: #007acc; font-weight: 600;\">\nKnowing what class an object belongs to helps us understand what methods and attributes it provides.\n</p>\n\n\n# Example: Creating a Class\n\n## Base Classes\n\nA **base class** (or parent class) serves as a template for creating objects. Other classes can inherit from it to reuse its properties and methods.\n\nClasses are defined using the `class` keyword, and their structure is specified using an `__init__()` method for initialization. \n\n. . . \n\nFor example, we can define a class called `Dog` and give it attributes that store data about a given dog.    \n\nWe can also add methods that represent behaviors an object of the `Dog` class can perform:  \n\n. . . \n\n::: {#cb56ecb4 .cell execution_count=2}\n``` {.python .cell-code code-line-numbers=\"|3,4|6-7|9-13\"}\nclass Dog: ## begin class definition\n    def __init__(self, name, breed): ## define init method\n        self.name = name ## add attributes\n        self.breed = breed\n\n    def speak(self): ## add methods\n        return f\"{self.name} says woof!\"\n\n    def __str__(self): ## add special methods, __str__(self) tells python what to display when an object is printed\n        return f\"Our dog {self.name}\"\n\n    def __repr__(self): ## add representation to display when dog is called in console\n        return f\"Dog(name={self.name!r}, breed={self.breed!r})\"\n```\n:::\n\n\n## Creating a dog\n\nCreating an instance (object) of the `Dog` class lets us model a particular dog:  \n\n. . . \n\n::: {#d1dbc5d7 .cell output-location='fragment' execution_count=3}\n``` {.python .cell-code}\nbuddy = Dog(\"Buddy\", \"Golden Retriever\")\nbuddy ## displays what was in the __repr__() method\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\nDog(name='Buddy', breed='Golden Retriever')\n```\n:::\n:::\n\n\n. . .\n\nHere, `buddy` is an **object** of the `Dog` class.     \n\nIt has **attributes** (`name`, `breed`) and **methods** (`speak()`).   \n\n. . .  \n\nWhen we make an instance of the Dog class:     \n- We set the value of the attributes [`name` and `breed`], which are then stored as part of the `buddy` object  \n- We can use any methods defined in the Dog class on `buddy`  \n\n::: {#4c9745e8 .cell output-location='fragment' execution_count=4}\n``` {.python .cell-code code-line-numbers=\"1-3|5-6|\"}\n## if we want to see what kind of dog our dog is\n## we can call buddy's attributes\nprint(f\"Our dog {buddy.name} is a {buddy.breed}.\")\n\n## we can also call any Dog methods\nprint(buddy.speak())  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOur dog Buddy is a Golden Retriever.\nBuddy says woof!\n```\n:::\n:::\n\n\n. . . \n \n<span style=\"color: #007acc\"><strong>Note:</strong> For python methods, the `self` argument is assumed to be passed and therefore we do not put anything in the parentheses when calling `.speak()`.</span>\n\n\n## Derived (Child) Classes  \n\nDerived/child classes build on base classes using the principle of inheritence. <br>\n\nNow that we have a `Dog` class, we can build on it to create a specialized `GuardDog` class. \n\n::: {#fd3a6dd8 .cell execution_count=5}\n``` {.python .cell-code code-line-numbers=\"|1|2-7|\"}\nclass GuardDog(Dog):  # GuardDog inherits from Dog\n    def __init__(self, name, breed, training_level): ## in addition to name and breed, we can \n        # define a training level. \n        # Call the parent (Dog) class's __init__ method\n        super().__init__(name, breed)\n        self.training_level = training_level  # New attribute for GuardDog that stores the \n        # training level for the dog\n\n    def guard(self): ## checks if the training level is > 5 and if not says train more\n        if self.training_level > 5:\n            return f\"{self.name} is guarding the house!\"\n        else:\n            return f\"{self.name} needs more training before guarding.\"\n    \n    def train(self): ## modifies the training_level attribute to increase the dog's training level\n        self.training_level = self.training_level + 1\n        return f\"Training {self.name}. {self.name}'s training level is now {self.training_level}\"\n\n# Creating an instance of GuardDog\nrex = GuardDog(\"Rex\", \"German Shepherd\", training_level= 5)\n```\n:::\n\n\n---\n\nNow that we have a dog (rex), we can call on any of the methods/attributes introduced in the `Dog` class as well as the new `GuardDog` class.\n\n::: {#6e0f2db5 .cell output-location='fragment' slide-type='fragment' execution_count=6}\n``` {.python .cell-code}\n# Using methods from the base class\nprint(rex.speak())  # Inherited from Dog -> Output: \"Rex says Woof!\"\nrex\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRex says woof!\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nDog(name='Rex', breed='German Shepherd')\n```\n:::\n:::\n\n\n<br>\n\n::: {#ffdbb439 .cell output-location='fragment' slide-type='fragment' execution_count=7}\n``` {.python .cell-code}\n# Using a method from the derived class\nprint(f\"{rex.name}'s training level is {rex.training_level}.\")\nprint(rex.guard()) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRex's training level is 5.\nRex needs more training before guarding.\n```\n:::\n:::\n\n\n. . . \n\n<span style=\"color: #007acc\"><strong>This is the power of inheritance</span></strong>—we don’t have to rewrite everything from scratch!\n---\n\n\nUnlike standalone functions, methods in Python often update objects in-place—meaning they modify the object itself rather than returning a new one.\n\n::: {#afe5b68a .cell output-location='fragment' slide-type='fragment' execution_count=8}\n``` {.python .cell-code}\n## we can use the `train` method to increase rex's training level, \nprint(rex.train())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining Rex. Rex's training level is now 6\n```\n:::\n:::\n\n\n. . .\n\n::: {#3a8cc4d4 .cell output-location='fragment' slide-type='fragment' execution_count=9}\n``` {.python .cell-code}\n## now check if he can guard \nprint(rex.guard()) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRex is guarding the house!\n```\n:::\n:::\n\n\n<br>\n\n. . . \n\nAs we saw with Rex, <span style=\"color: #007acc\"><strong>child classes inherit all attributes (`.name` and `.breed`) and methods (`.speak()` `__repr__()`) from parent classes.</strong></span> They can also have new methods (`.train()`).\n\n## Mixins\nA **mixin** is a special kind of class designed to add **functionality** to another class. Unlike base classes, mixins aren’t used alone.  \n\n. . . \n\nFor example, scikit-learn uses mixins like:  \n- `sklearn.base.ClassifierMixin` (adds classifier-specific methods)  \n- `sklearn.base.RegressorMixin` (adds regression-specific methods)  \n\nwhich it adds to the `BaseEstimator` class to add functionality.\n\nTo finish up our dog example, we are going to define a mixin class that adds a functionality to the base `Dog()` class which allows us to teach a dog tricks. \n\n---\n\n- When creating a mixin class, we let the other base classes carry most of the initialization\n- However, we can add other attributes like `.tricks`, which stores the set of learned tricks\n- We can also add methods to define additional behaviors\n\n::: {#08301c6d .cell execution_count=10}\n``` {.python .cell-code code-line-numbers=\"|2,3|4|6-17|\"}\nclass TrickMixin: ## mixin that will let us teach a dog tricks\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)  # Ensures proper initialization in multiple inheritance\n        self.tricks = []  # Store learned tricks\n\n    def learn_trick(self, trick):\n        \"\"\"Teaches the dog a new trick.\"\"\"\n        if trick not in self.tricks:\n            self.tricks.append(trick)\n            return f\"{self.name} learned a new trick: {trick}!\"\n        return f\"{self.name} already knows {trick}!\"\n\n    def perform_tricks(self):\n        \"\"\"Returns a list of tricks the dog knows.\"\"\"\n        if self.tricks:\n            return f\"{self.name} can perform: {', '.join(self.tricks)}.\"\n        return f\"{self.name} hasn't learned any tricks yet.\"\n\n## note: the TrickMixin class is not a standalone class! it does not let us create a dog on its own!!!\n```\n:::\n\n\n. . . \n\nUsing this Trick mixin, we can then create a new class of dog (SmartDog) using both `Dog`and `TrickMixin` as base classes. \n\n---\n\nBy including both `Dog` and `TrickMixin` as base classes, we can give objects of class `SmartDog` the ability to speak and learn tricks!\n\n::: {#70117584 .cell output-location='fragment' slide-type='fragment' execution_count=11}\n``` {.python .cell-code}\nclass SmartDog(Dog, TrickMixin):\n    def __init__(self, name, breed):\n        super().__init__(name, breed)  # Initialize Dog class\n        TrickMixin.__init__(self)  # Initialize TrickMixin separately\n```\n:::\n\n\n. . . \n\n::: {#4bb01ddc .cell output-location='fragment' slide-type='fragment' execution_count=12}\n``` {.python .cell-code}\n# a SmartDog object can use methods from both parent object `Dog` and mixin `TrickMixin`.\nmy_smart_dog = SmartDog(\"Buddy\", \"Border Collie\")\n```\n:::\n\n\n. . . \n\n::: {#0b9c8f92 .cell output-location='fragment' slide-type='fragment' execution_count=13}\n``` {.python .cell-code}\nprint(my_smart_dog.speak()) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuddy says woof!\n```\n:::\n:::\n\n\n. . . \n\n::: {#a9c78092 .cell output-location='fragment' slide-type='fragment' execution_count=14}\n``` {.python .cell-code}\nprint(my_smart_dog.learn_trick(\"Sit\"))  \nprint(my_smart_dog.learn_trick(\"Roll Over\")) \nprint(my_smart_dog.learn_trick(\"Sit\"))  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuddy learned a new trick: Sit!\nBuddy learned a new trick: Roll Over!\nBuddy already knows Sit!\n```\n:::\n:::\n\n\n. . . \n\n::: {#f89c3222 .cell output-location='fragment' slide-type='fragment' execution_count=15}\n``` {.python .cell-code}\nprint(my_smart_dog.perform_tricks()) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBuddy can perform: Sit, Roll Over.\n```\n:::\n:::\n\n\n## Duck Typing \n\n>\"If it quacks like a duck and walks like a duck, it's a duck.\"\n\nPython doesn’t require explicit interfaces.\nIf an object implements the expected methods, it can be used interchangeably with other objects. This is called duck typing.\n\n. . . \n\nWe can demonstrate this by defining two new base classes that are different than `Dog` but also have a `speak()` method.\n\n. . . \n\n::: {#9ad20b8d .cell execution_count=16}\n``` {.python .cell-code code-line-numbers=\"|5-6, 12-13\"}\nclass Human:\n    def __init__(self, name):\n        self.name = name\n\n    def speak(self):\n        return f\"{self.name} says hello!\"\n\nclass Parrot:\n    def __init__(self, name):\n        self.name = name\n\n    def speak(self):\n        return f\"{self.name} says squawk!\"\n```\n:::\n\n\n## Duck Typing in Action\n\nEven though `Dog`, `Human` and `Parrot` are entirely different classes...\n\n::: {#6d8f96d7 .cell output-location='fragment' execution_count=17}\n``` {.python .cell-code}\ndef call_speaker(obj):\n    print(obj.speak())\n\ncall_speaker(Dog(\"Fido\", \"Labrador\"))\ncall_speaker(Human(\"Alice\"))\ncall_speaker(Parrot(\"Polly\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFido says woof!\nAlice says hello!\nPolly says squawk!\n```\n:::\n:::\n\n\n. . .  \n\nThey all implement `.speak()`, so Python treats them the same!\n\nIn the context of our work, this would allow us to make a pipeline using models from different libraries that do not share a base class/mixin but have the same methods. \n\n---\n\n<span style=\"color: #007acc\"><strong>While our dog example was very simple, this is the same way that model classes work in python.</strong></span> \n\n## **OOP in Machine Learning and Modeling?**  \n\nMachine learning models in Python are implemented as **classes**.    \n    - When you create a model, you’re **instantiating an object** of a predefined class (e.g., `LogisticRegression()`).    \n    - That model **inherits** attributes (parameters, coefficients) and methods (like `.fit()` and `.predict()`).    \n\n--- \n\nFor example, a **logistic regression model** in `scikit-learn` is an instance of the `LogisticRegression` class:\n```{.python}\n## Example: \nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression()  # Creating an instance of the LogisticRegression class\nmodel.fit(X_train, y_train)   # Calling a method to train the model\npredictions = model.predict(X_test)  # Calling a method to make predictions\n```\nHere, **`model`** is an **object** that has inherited attributes and methods from `LogisticRegression`.  \n\n## **Key Benefits of OOP in Machine Learning**  \n\n1. **Encapsulation** – Models store parameters and methods inside a single object.  \n2. **Inheritance** – New models can build on base models, reusing existing functionality.  \n3. **Abstraction**  – `.fit()` should work as expected, regardless of complexity of underlying implimentation.\n4. **Polymorphism (Duck Typing)** – Different models share the same method names (`.fit()`, `.predict()`), making them easy to use interchangeably, particularly in analysis pipelines. \n\nUnderstanding **base classes** and **mixins** is especially important when working with deep learning frameworks like **PyTorch and TensorFlow**, as they allow for easy customization of models.  \n\nBy using **object-oriented programming**, Python makes it easy to **structure machine learning workflows** in a reusable and scalable way. The fact that all ML models in scikit-learn follow the **same structure** (with `.fit()`, `.predict()`, `.score()`, etc.) makes it easier to switch between models and automate processes. Additionally, the model classes in the statsmodels package have many of the same methods (`.fit()`, `.predict()`, `.score()`). \n\n\n# Part B - Demo Projects\n\n**Apply knowledge of OOP to modeling using scikit-learn**\n\n---\n\n## 🐧 Mini Project: Classifying Penguins with scikit-learn\n\nNow that you understand **classes** and **data structures** in Python, let’s apply that knowledge!\n\nIn this project, we’ll try to classify **penguin species** using two features:  \n- `bill_length_mm`  \n- `bill_depth_mm`  \n\nWe’ll explore:  \n- **Unsupervised learning** with **K-Means** clustering (model doesn't 'know' y)\n- **Supervised learning** with a **k-NN classifier** (model trained w/ y information)\n\n\n## Modeling with scikit-learn Classes\n\nWe'll use models from **scikit-learn**, which are built using object-oriented design.  \nEach model is an **instance of a class** (inheriting from `BaseEstimator`) with:\n\n**Common Methods:**  \n- `.fit()` — Train the model  \n- `.predict()` — Make predictions  \n\n**Common Attributes:**  \n- `.get_params()`, `.classes_`, `.n_clusters_`, etc.  \n\n> We're using `scikit-learn` here for its simplicity, but the concepts apply to more advanced frameworks like **PyTorch** and **TensorFlow** too!\n\n## General Modeling Workflow\n\n::: columns\n::: column\n\n**Step 0: Prepare Workspace**  \n- Import necessary libraries/modules:  \n  - **Functions** (e.g., `train_test_split`, `accuracy_score`)  \n  - **Classes** (e.g., `KMeans`, `KNeighborsClassifier`)  \n\n**Step 1: Data Preparation**  \n- Load data (`pandas`)  \n- Clean data (`pandas`, `numpy`)  \n- Transform/scale features (`sklearn.preprocessing`)  \n- Optionally split data into training and testing sets  \n\n**Step 2: Initialize the Model**  \n- Create an instance of the model class (`KMeans`, `KNeighborsClassifier`)  \n- Set parameters during instantiation (e.g., `n_clusters=3`, `n_neighbors=5`)  \n\n:::\n\n::: column\n\n**Step 3: Fit the Model**  \n- Use `.fit(X)` for unsupervised models  \n- Use `.fit(X_train, y_train)` for supervised models  \n\n**Step 4: Make Predictions** *(optional)*  \n- Use `.predict(X_test)` to generate predictions  \n- Use `.predict_proba()` to get class probabilities (if available)  \n\n**Step 5: Evaluate Model Performance**  \n- Compare predictions to true values  \n- Use visualizations or metrics (e.g., accuracy, ARI, classification report)  \n\n:::\n:::\n\n\n## Step 0: Import Libraries\n\n<br>\n\n<span style=\"color: #007acc\"><strong>Before any analysis, we must import the necessary libraries.</strong></span>  \n\nFor large libraries like **scikit-learn**, **PyTorch**, or **TensorFlow**, we usually do **not** import the entire package. Instead, we selectively import the **classes** and **functions** we need.\n\n. . . \n\n> 🔤 **Naming Tip**:  \n> - `CamelCase` = **Classes**  \n> - `snake_case` = **Functions**\n\n--- \n\nIn this project, we’ll use the following:\n\n**Classes**  \n- `StandardScaler` — for feature scaling  \n- `KNeighborsClassifier` — for supervised k-NN classification  \n- `KMeans` — for unsupervised clustering\n\n<br>\n\n**Functions**  \n- `train_test_split()` — to split data into training and test sets  \n- `accuracy_score()` — to evaluate classification accuracy  \n- `classification_report()` — to print precision, recall, F1 (balance of precision and recall), Support (number of true instances per class)\n- `adjusted_rand_score()` — to evaluate clustering performance\n\n\n## Import Libraries\n\n::: {#b3e4e41e .cell execution_count=18}\n``` {.python .cell-code code-line-numbers=\"|12|14-17|19-21|\"}\n## imports\nimport pandas as pd\nimport numpy as np\n\nfrom plotnine import *\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom great_tables import GT\n\n## sklearn imports\n\n## import classes\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.cluster import KMeans\n\n## import functions\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, adjusted_rand_score\n```\n:::\n\n\n## Step 1: Data Preparation {.smaller}\n\n::: {#8fd56594 .cell output-location='slide' execution_count=19}\n``` {.python .cell-code code-line-numbers=\"1,2|4-11|13-16|18-19|\"}\n# Load the Penguins dataset\npenguins = sns.load_dataset(\"penguins\").dropna()\n\n# Make a summary table for the penguins dataset, grouping by species. \nsummary_table = penguins.groupby(\"species\").agg({\n    \"bill_length_mm\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"bill_depth_mm\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"flipper_length_mm\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"body_mass_g\": [\"mean\", \"std\", \"min\", \"max\"],\n    \"sex\": lambda x: x.value_counts().to_dict()  # Count of males and females\n})\n\n# Round numeric values to 1 decimal place (excluding the 'sex' column)\nfor col in summary_table.columns:\n    if summary_table[col].dtype in [float, int]:\n        summary_table[col] = summary_table[col].round(1)\n\n# Display the result\ndisplay(summary_table)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead tr th {\n        text-align: left;\n    }\n\n    .dataframe thead tr:last-of-type th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr>\n      <th></th>\n      <th colspan=\"4\" halign=\"left\">bill_length_mm</th>\n      <th colspan=\"4\" halign=\"left\">bill_depth_mm</th>\n      <th colspan=\"4\" halign=\"left\">flipper_length_mm</th>\n      <th colspan=\"4\" halign=\"left\">body_mass_g</th>\n      <th>sex</th>\n    </tr>\n    <tr>\n      <th></th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>max</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>max</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>max</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>max</th>\n      <th>&lt;lambda&gt;</th>\n    </tr>\n    <tr>\n      <th>species</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Adelie</th>\n      <td>38.8</td>\n      <td>2.7</td>\n      <td>32.1</td>\n      <td>46.0</td>\n      <td>18.3</td>\n      <td>1.2</td>\n      <td>15.5</td>\n      <td>21.5</td>\n      <td>190.1</td>\n      <td>6.5</td>\n      <td>172.0</td>\n      <td>210.0</td>\n      <td>3706.2</td>\n      <td>458.6</td>\n      <td>2850.0</td>\n      <td>4775.0</td>\n      <td>{'Male': 73, 'Female': 73}</td>\n    </tr>\n    <tr>\n      <th>Chinstrap</th>\n      <td>48.8</td>\n      <td>3.3</td>\n      <td>40.9</td>\n      <td>58.0</td>\n      <td>18.4</td>\n      <td>1.1</td>\n      <td>16.4</td>\n      <td>20.8</td>\n      <td>195.8</td>\n      <td>7.1</td>\n      <td>178.0</td>\n      <td>212.0</td>\n      <td>3733.1</td>\n      <td>384.3</td>\n      <td>2700.0</td>\n      <td>4800.0</td>\n      <td>{'Female': 34, 'Male': 34}</td>\n    </tr>\n    <tr>\n      <th>Gentoo</th>\n      <td>47.6</td>\n      <td>3.1</td>\n      <td>40.9</td>\n      <td>59.6</td>\n      <td>15.0</td>\n      <td>1.0</td>\n      <td>13.1</td>\n      <td>17.3</td>\n      <td>217.2</td>\n      <td>6.6</td>\n      <td>203.0</td>\n      <td>231.0</td>\n      <td>5092.4</td>\n      <td>501.5</td>\n      <td>3950.0</td>\n      <td>6300.0</td>\n      <td>{'Male': 61, 'Female': 58}</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Scaling the data - Understanding the Standard Scaler class\n\nFor our clustering to work well, the predictors should be on the same scale. <br>\n\nTo achieve this, we use an instance of the `StandardScaler` class. \n\n\n## Standard Scaler\n\n```python\nclass sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)\n```\n<br>\n\n. . .   \n\n**Parameters** are supplied by user  \n- copy, with_mean, with_std <br>\n\n**Attributes** contain the `data` of the object  \n- `scale_`: scaling factor  \n- `mean_`: mean value for each feature  \n- `var_`: variance for each feature  \n- `n_features_in_`: number of features seen during fit  \n- `n_samples_seen`: number of samples processed for each feature <br>\n\n**Methods** describe the `behaviors` of the object and/or `modify` its attributes  \n- `fit(X)` -> compute mean and std used for scaling -> fit scaler to data X  \n    * updates the attributes of the scaler object  \n- `transform(X)` -> perform standardization by centering and scaling with fitted scaler  \n\n## Data Preparation\n\n::: {#b84c3c7d .cell execution_count=20}\n``` {.python .cell-code code-line-numbers=\"1-3|5-7|\"}\n# Selecting features for clustering -> let's just use bill length and bill depth.\nX = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]]\ny = penguins[\"species\"]\n\n# Standardizing the features for better clustering performance\nscaler = StandardScaler() ## create instance of StandardScaler\nX_scaled = scaler.fit_transform(X) ## same as calling scaler.fit(X) then X_scaled = scaler.transform(X)\n```\n:::\n\n\n## Understanding the KMeans model class\n\n```python\nclass sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, \ntol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')\n```\n<br>\n\n**Parameters**: Set by user at time of instantiation  \n- n_clusters, max_iter, algorithm  <br>\n\n**Attributes**: Store object data    \n- `cluster_centers_`: stores coordinates of cluster centers  \n- `labels_`: stores labels of each point \n- `n_iter_`: number of iterations run (will be changed during method run)  \n- `n_features_in` and `feature_names_in_`: store info about features seen during fit  <br>\n\n**Methods**: Define object behaviors      \n- `fit(X)` -> same as usage as `train()` -> fit model to data X  \n- `predict(X)` -> predict closest cluster each sample in X belongs to  \n- `transform(X)` -> transform X to cluster-distance space  \n\n---\n\n### Step 2: Create model\n\n::: {#07904d75 .cell execution_count=21}\n``` {.python .cell-code}\n## Choosing 3 clusters b/c we have 3 species\nkmeans = KMeans(n_clusters=3, random_state=42) ## make an instance of the K means class\n```\n:::\n\n\n### Step 3: Fit model to data\n\n::: {#fc01feaa .cell output-location='fragment' execution_count=22}\n``` {.python .cell-code code-line-numbers=\"1-2|4-5|\"}\n## the fit\npenguins[\"kmeans_cluster\"] = kmeans.fit_predict(X_scaled)\n\n## now that we fit the model, we should have cluster centers\nprint(\"Coordinates of cluster centers:\", kmeans.cluster_centers_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoordinates of cluster centers: [[-0.95023997  0.55393493]\n [ 0.58644397 -1.09805504]\n [ 1.0886843   0.79503579]]\n```\n:::\n:::\n\n\n## Step 5: Visualize and Evaluate\n\nTo do visualization, we can use either seaborn or plotnine. `plotnine` mirrors `ggplot2` syntax from R and is great for layered grammar-of-graphics plots, while `seaborn` works directly with **numpy arrays** and **pandas DataFrames**. It builds on top of matplotlib objects—which are themselves instances of classes. Most built-in plotting methods in Python packages use matplotlib.<br>\n\n`seaborn` is often used for quick statistical visualizations in python and is more convienient if you want to put multiple plots on the same figure.\n\n## Plotting with plotnine vs seaborn\n\n::: columns\n::: column\n\n### **plotnine (like ggplot2 in R)**  \nThe biggest differences between `plotnine` and `ggplot2` syntax are: \n- With `plotnine` the whole call is wrapped in `()` parentheses\n- Variables are called with strings (`\"\"` are needed!)\n- If you don't use `from plotnine import *`, you will need to import each individual function you plan to use!\n\n:::\n\n::: column\n\n### **seaborn (base matplotlib + enhancements)**  \n- Designed for **quick, polished plots**\n- Works well with **pandas DataFrames** or **NumPy arrays**\n- Integrates with `matplotlib` for customization\n- Often used in ML for things like **decision boundaries** or **heatmaps**\n\n:::\n:::\n\n\n## Scatterplot with plotnine\n\nTo take at the distribution of our species by bill length and bill depth...\n\n::: {#bab1be4c .cell output-location='slide' execution_count=23}\n``` {.python .cell-code}\n# Plotnine scatterplot of species by bill length/depth\nplot1 = (ggplot(penguins, aes(x=\"bill_length_mm\", y=\"bill_depth_mm\", color=\"species\"))\n + geom_point()\n + ggtitle(\"Penguin Species\")\n + theme_bw())\n\ndisplay(plot1)\n```\n\n::: {.cell-output .cell-output-display}\n![](session4v2_files/figure-revealjs/cell-24-output-1.png){width=960 height=480}\n:::\n:::\n\n\n## Scatterplot with seaborn\n\nWe can also create a plot for the K-means clustering results. This time, we'll use seaborn.\n\n::: {#1cea498e .cell output-location='slide' execution_count=24}\n``` {.python .cell-code}\n# Create the figure and axes\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Save seaborn scatterplot to variable plot1 by drawing it on ax\nplot1 = sns.scatterplot(\n    data=penguins,\n    x=\"bill_length_mm\",\n    y=\"bill_depth_mm\",\n    hue=\"species\",\n    style=\"sex\",             # optional\n    palette=\"Set2\",\n    edgecolor=\"black\",\n    s=100,\n    ax=ax                    # draw on the axes object\n)\n\n# Add decorations to the same Axes\nax.set_title(\"Penguin Bill Length vs Depth by Species\")\nax.set_xlabel(\"Bill Length (mm)\")\nax.set_ylabel(\"Bill Depth (mm)\")\nax.legend(title=\"Species\")\n\n# Plot the figure\nfig.tight_layout() \n```\n\n::: {.cell-output .cell-output-display}\n![](session4v2_files/figure-revealjs/cell-25-output-1.png){width=757 height=564}\n:::\n:::\n\n\n## Use function to calculate ARI\n\nTo check how good our model is, we can use one of the functions included in the sklearn library\n\nThe `adjusted_rand_score()` function evaluates how well the cluster groupings agree with the species groupings while adjusting for chance. \n\n::: {#5aed9844 .cell output-location='fragment' execution_count=25}\n``` {.python .cell-code}\n# Calculate clustering performance using Adjusted Rand Index (ARI)\nkmeans_ari = adjusted_rand_score(penguins['species'], penguins[\"kmeans_cluster\"])\nprint(f\"k-Means Adjusted Rand Index: {kmeans_ari:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nk-Means Adjusted Rand Index: 0.82\n```\n:::\n:::\n\n\n---\n\n### We can also use methods on our data structure to create new data\n\n- We can use the `.groupby()` method to help us plot cluster agreement with species label as a heatmap\n- If we want to add sex as a variable to see if that is why our clusters don't agree with our species, we can use a scatterplot\n- Using seaborn and matplotlib, we can easily put both of these plots on the same figure. \n<br>\n\n\nSetting up data... <br>\n\n::: {#4027aef1 .cell execution_count=26}\n``` {.python .cell-code}\n# Count occurrences of each species-cluster-sex combination( .size gives the count as index, use reset_index to get count column. )\nscatter_data = penguins.groupby([\"species\", \"kmeans_cluster\", \"sex\"]).size().reset_index(name=\"count\")\n# Create a mapping to add horizontal jitter for each sex for scatterplot\nsex_jitter = {'Male': -0.1, 'Female': 0.1}\nscatter_data['x_jittered'] = scatter_data.apply(\n    lambda row: scatter_data['species'].unique().tolist().index(row['species']) + sex_jitter.get(row['sex'], 0),\n    axis=1\n)\nheatmap_data = scatter_data.pivot_table(index=\"kmeans_cluster\", columns=\"species\", values=\"count\", aggfunc=\"sum\", fill_value=0)\n```\n:::\n\n\n---\n\n#### Creating Plots ....\n\n::: {#dca7390c .cell output-location='slide' execution_count=27}\n``` {.python .cell-code}\n# Prepare the figure with 2 subplots the fig2 object represents the overall figure the axes object will contain both plots\nfig2, axes = plt.subplots(1, 2, figsize=(14, 6)) ## 1 row 2 columns\n\n# Plot heatmap on the first axis\nsns.heatmap(data = heatmap_data, cmap=\"Blues\", linewidths=0.5, linecolor='white', annot=True, fmt='d', ax=axes[0])\naxes[0].set_title(\"Heatmap of KMeans Clustering by Species\")\naxes[0].set_xlabel(\"Species\")\naxes[0].set_ylabel(\"KMeans Cluster\")\n\n# Scatterplot with jitter\nsns.scatterplot(data=scatter_data, x=\"x_jittered\", y=\"kmeans_cluster\",\n    hue=\"species\", style=\"sex\", size=\"count\", sizes=(50, 300),\n    alpha=0.8, ax=axes[1], legend=\"brief\"\n)\n\n# Fix the x-axis ticks and legend\nspecies_order = list(scatter_data['species'].unique())\naxes[1].set_xticks(range(len(species_order)))\naxes[1].set_xticklabels(species_order)\naxes[1].set_title(\"Cluster Assignment by Species and Sex (Jittered)\")\naxes[1].set_ylabel(\"KMeans Cluster\")\naxes[1].set_xlabel(\"Species\")\naxes[1].set_yticks([0, 1, 2])\naxes[1].legend(bbox_to_anchor=(1.05, 0.5), loc='center left', borderaxespad=0.0, title=\"Legend\")\n\nfig2.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](session4v2_files/figure-revealjs/cell-28-output-1.png){width=1336 height=564}\n:::\n:::\n\n\n## Project 2 -> KNN classification\n\nFor our KNN classification, the model is supervised (meaning it is dependent on the outcome 'y' data) and therefore we need to split our data into a training and test set. <br>\n\n. . . \n\nThe **function** `train_test_split()` from scikit-learn is helpful here! Our classifier object has built in methods for fitting models and predicting.\n\n::: {#af5c888a .cell execution_count=28}\n``` {.python .cell-code}\n# Splitting dataset into training and testing sets (still using scaled X!)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n```\n:::\n\n\n. . . \n\n>Unlike R functions, which return a single object (often a list when multiple outputs are needed), Python functions can return multiple values as a tuple—letting you unpack them directly into separate variables.\n\n## Understanding KNeighborsClassifier class\n```{.python}\nclass sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', \nalgorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)\n```\n. . . \n\n**Parameters**: Set by user at time of instantiation    \n- n_neigbors, weights, algorithm, etc.  <br>\n\n**Attributes**: Store object data     \n- `classes_`: class labels known to the classifier  \n- `effective_metric_`: distance metric used  \n- `effective_metric_params_`: parameters for the metric function  \n- `n_features_in` and `feature_names_in_`: store info about features seen during fit    \n- `n_samples_fit_`: number of samples in fitted data  <br> \n\n**Methods**: Define object behaviors      \n- `.fit(X, y)` -> fit knn classifier from training dataset (X and y)  \n- `.predict(X)` -> predict class labels for provided data X  \n- `.predict_proba(X)` -> return probability estimates for test data X  \n- `.score(X, y)` -> return mean accuracy on given test data X and labels y  \n\n---\n\n### Making an instance of KNeighborsClassifier and fitting to training data\n- For a supervised model, y_train is included in `.fit()`!\n\n::: {#548dd641 .cell execution_count=29}\n``` {.python .cell-code}\n## perform knn classification\n# Applying k-NN classification with 5 neighbors\nknn = KNeighborsClassifier(n_neighbors=5) ## make an instance of the KNeighborsClassifier class\n# and set the n_neighbors parameter to be 5. \n\n# Use the fit method to fit the model to the training data\nknn.fit(X_train, y_train)\n```\n:::\n\n\n## Once the model is fit...\n\n- Once the model is fit, we can look at its attributes (ex: `.classes_`) which gives the class labels as known to the classifier\n\n::: {#a2b4a495 .cell output-location='fragment' execution_count=30}\n``` {.python .cell-code}\nprint(knn.classes_)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Adelie' 'Chinstrap' 'Gentoo']\n```\n:::\n:::\n\n\n## Next Step: Use fitted model to predict species for test data\n\n::: {#9929e6a9 .cell output-location='fragment' execution_count=31}\n``` {.python .cell-code}\n# Use the predict method on the test data to get the predictions for the test data\ny_pred = knn.predict(X_test)\n\n# Also can take a look at the prediction probabilities, \n# and use the .classes_ attribute to put the column labels in the right order\nprobs = pd.DataFrame(\n    knn.predict_proba(X_test),\n    columns = knn.classes_)\nprobs['y_pred'] = y_pred\n\nprint(\"Predicted probabilities: \\n\", probs.head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredicted probabilities: \n    Adelie  Chinstrap  Gentoo     y_pred\n0     1.0        0.0     0.0     Adelie\n1     0.0        0.0     1.0     Gentoo\n2     1.0        0.0     0.0     Adelie\n3     0.0        0.6     0.4  Chinstrap\n4     1.0        0.0     0.0     Adelie\n```\n:::\n:::\n\n\n## Scatterplot for k-NN classification of test data\n\n- Create dataframe of unscaled X_test, `bill_length_mm`, and `bill_depth_mm`.\n- Add to it the actual and predicted species labels\n\n::: {#c40857dc .cell execution_count=32}\n``` {.python .cell-code}\n## First unscale the test data\nX_test_unscaled = scaler.inverse_transform(X_test)\n\n## create dataframe \npenguins_test = pd.DataFrame(\n    X_test_unscaled,\n    columns=['bill_length_mm', 'bill_depth_mm']\n)\n\n## add actual and predicted species \npenguins_test['y_actual'] = y_test.values\npenguins_test['y_pred'] = y_pred\npenguins_test['correct'] = penguins_test['y_actual'] == penguins_test['y_pred']\n```\n:::\n\n\n## Plotnine scatterplot for k-NN classification of test data\n\n::: {#3c77c54d .cell output-location='slide' execution_count=33}\n``` {.python .cell-code}\n## Build the plot\nplot3 = (ggplot(penguins_test, aes(x=\"bill_length_mm\", y=\"bill_depth_mm\", \ncolor=\"y_actual\", fill = 'y_pred', size = 'correct'))\n + geom_point()\n + scale_size_manual(values={True: 2, False: 5})\n + ggtitle(\"k-NN Classification Results\")\n + theme_bw())\n\ndisplay(plot3)\n```\n\n::: {.cell-output .cell-output-display}\n![](session4v2_files/figure-revealjs/cell-34-output-1.png){width=960 height=480}\n:::\n:::\n\n\n## Visualizing Decision Boundary with seaborn and matplotlib\n\n::: {#a8d2e611 .cell output-location='slide' execution_count=34}\n``` {.python .cell-code code-line-numbers=\"1-8|10-18|20-22|24-34|36-41|\"}\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.preprocessing import LabelEncoder\n\n# Create and fit label encoder for y (just makes y numeric because it makes the scatter plot happy)\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Create the plot objects\nfig, ax = plt.subplots(figsize=(8, 6))\n\n# Create display object\ndisp = DecisionBoundaryDisplay.from_estimator(\n    knn,\n    X_test,\n    response_method = 'predict',\n    plot_method = 'pcolormesh',\n    xlabel = \"bill_length_scaled\",\n    ylabel = \"bill_depth_scaled\",\n    shading = 'auto',\n    alpha = 0.5,\n    ax = ax\n)\n\n# Use method from display object to create scatter plot\nscatter = disp.ax_.scatter(X_scaled[:,0], X_scaled[:,1], c=y_encoded, edgecolors = 'k')\ndisp.ax_.legend(\n    scatter.legend_elements()[0],\n    knn.classes_,\n    loc = 'lower left',\n    title = 'Species'\n    \n)\n_ = disp.ax_.set_title(\"Penguin Classification\")\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](session4v2_files/figure-revealjs/cell-35-output-1.png){width=662 height=523}\n:::\n:::\n\n\n## Evaluate KNN performance\n\n::: {#2b30168b .cell output-location='fragment' execution_count=35}\n``` {.python .cell-code}\n## eval knn performance\n# Calculate accuracy and print classification report -> \n# accuracy_score and classification_report are functions! \nknn_accuracy = accuracy_score(y_test, y_pred)\nprint(f\"k-NN Accuracy: {knn_accuracy:.2f}\")\nprint(classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nk-NN Accuracy: 0.94\n              precision    recall  f1-score   support\n\n      Adelie       0.98      0.98      0.98        48\n   Chinstrap       0.80      0.89      0.84        18\n      Gentoo       0.97      0.91      0.94        34\n\n    accuracy                           0.94       100\n   macro avg       0.92      0.93      0.92       100\nweighted avg       0.94      0.94      0.94       100\n\n```\n:::\n:::\n\n\n## Make a Summary Table of Metrics for Both Models\n\n::: {#2d176436 .cell output-location='fragment' execution_count=36}\n``` {.python .cell-code}\n##  making a summary table\n# Creating a summary table\nsummary_table = pd.DataFrame({\n    \"Metric\": [\"k-Means Adjusted Rand Index\", \"k-NN Accuracy\"],\n    \"Value\": [kmeans_ari, knn_accuracy]\n})\nGT(summary_table).show() ## round the values!!!!!\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div id=\"crwmzivjru\" style=\"padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;\">\n<style>\n#crwmzivjru table {\n          font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n          -webkit-font-smoothing: antialiased;\n          -moz-osx-font-smoothing: grayscale;\n        }\n\n#crwmzivjru thead, tbody, tfoot, tr, td, th { border-style: none; }\n tr { background-color: transparent; }\n#crwmzivjru p { margin: 0; padding: 0; }\n #crwmzivjru .gt_table { display: table; border-collapse: collapse; line-height: normal; margin-left: auto; margin-right: auto; color: #333333; font-size: 16px; font-weight: normal; font-style: normal; background-color: #FFFFFF; width: auto; border-top-style: solid; border-top-width: 2px; border-top-color: #A8A8A8; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #A8A8A8; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; }\n #crwmzivjru .gt_caption { padding-top: 4px; padding-bottom: 4px; }\n #crwmzivjru .gt_title { color: #333333; font-size: 125%; font-weight: initial; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; border-bottom-color: #FFFFFF; border-bottom-width: 0; }\n #crwmzivjru .gt_subtitle { color: #333333; font-size: 85%; font-weight: initial; padding-top: 3px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; border-top-color: #FFFFFF; border-top-width: 0; }\n #crwmzivjru .gt_heading { background-color: #FFFFFF; text-align: center; border-bottom-color: #FFFFFF; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #crwmzivjru .gt_bottom_border { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #crwmzivjru .gt_col_headings { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; }\n #crwmzivjru .gt_col_heading { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; padding-left: 5px; padding-right: 5px; overflow-x: hidden; }\n #crwmzivjru .gt_column_spanner_outer { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: normal; text-transform: inherit; padding-top: 0; padding-bottom: 0; padding-left: 4px; padding-right: 4px; }\n #crwmzivjru .gt_column_spanner_outer:first-child { padding-left: 0; }\n #crwmzivjru .gt_column_spanner_outer:last-child { padding-right: 0; }\n #crwmzivjru .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: bottom; padding-top: 5px; padding-bottom: 5px; overflow-x: hidden; display: inline-block; width: 100%; }\n #crwmzivjru .gt_spanner_row { border-bottom-style: hidden; }\n #crwmzivjru .gt_group_heading { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; text-align: left; }\n #crwmzivjru .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; vertical-align: middle; }\n #crwmzivjru .gt_from_md> :first-child { margin-top: 0; }\n #crwmzivjru .gt_from_md> :last-child { margin-bottom: 0; }\n #crwmzivjru .gt_row { padding-top: 8px; padding-bottom: 8px; padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; border-top-width: 1px; border-top-color: #D3D3D3; border-left-style: none; border-left-width: 1px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 1px; border-right-color: #D3D3D3; vertical-align: middle; overflow-x: hidden; }\n #crwmzivjru .gt_stub { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; }\n #crwmzivjru .gt_stub_row_group { color: #333333; background-color: #FFFFFF; font-size: 100%; font-weight: initial; text-transform: inherit; border-right-style: solid; border-right-width: 2px; border-right-color: #D3D3D3; padding-left: 5px; padding-right: 5px; vertical-align: top; }\n #crwmzivjru .gt_row_group_first td { border-top-width: 2px; }\n #crwmzivjru .gt_row_group_first th { border-top-width: 2px; }\n #crwmzivjru .gt_striped { background-color: rgba(128,128,128,0.05); }\n #crwmzivjru .gt_table_body { border-top-style: solid; border-top-width: 2px; border-top-color: #D3D3D3; border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; }\n #crwmzivjru .gt_sourcenotes { color: #333333; background-color: #FFFFFF; border-bottom-style: none; border-bottom-width: 2px; border-bottom-color: #D3D3D3; border-left-style: none; border-left-width: 2px; border-left-color: #D3D3D3; border-right-style: none; border-right-width: 2px; border-right-color: #D3D3D3; }\n #crwmzivjru .gt_sourcenote { font-size: 90%; padding-top: 4px; padding-bottom: 4px; padding-left: 5px; padding-right: 5px; text-align: left; }\n #crwmzivjru .gt_left { text-align: left; }\n #crwmzivjru .gt_center { text-align: center; }\n #crwmzivjru .gt_right { text-align: right; font-variant-numeric: tabular-nums; }\n #crwmzivjru .gt_font_normal { font-weight: normal; }\n #crwmzivjru .gt_font_bold { font-weight: bold; }\n #crwmzivjru .gt_font_italic { font-style: italic; }\n #crwmzivjru .gt_super { font-size: 65%; }\n #crwmzivjru .gt_footnote_marks { font-size: 75%; vertical-align: 0.4em; position: initial; }\n #crwmzivjru .gt_asterisk { font-size: 100%; vertical-align: 0; }\n \n</style>\n<table class=\"gt_table\" data-quarto-disable-processing=\"false\" data-quarto-bootstrap=\"false\">\n<thead>\n\n<tr class=\"gt_col_headings\">\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_left\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Metric\">Metric</th>\n  <th class=\"gt_col_heading gt_columns_bottom_border gt_right\" rowspan=\"1\" colspan=\"1\" scope=\"col\" id=\"Value\">Value</th>\n</tr>\n</thead>\n<tbody class=\"gt_table_body\">\n  <tr>\n    <td class=\"gt_row gt_left\">k-Means Adjusted Rand Index</td>\n    <td class=\"gt_row gt_right\">0.8203520973164866</td>\n  </tr>\n  <tr>\n    <td class=\"gt_row gt_left\">k-NN Accuracy</td>\n    <td class=\"gt_row gt_right\">0.94</td>\n  </tr>\n</tbody>\n\n\n</table>\n\n</div>\n        \n```\n:::\n:::\n\n\n## Key Takeaways from This Session\n\n<p style=\"font-size: 1.25em\"> \n</p>\n<ul style=\"font-size: 1.1em; line-height: 1.6; max-width: 700px; margin: 0 auto;\">\n  <li><strong>Python workflows rely on object-oriented structures in addition to functions:</strong><br>\n   Understanding the OOP paradigm makes Python a lot easier!</li>\n  <li><strong>Everything is an object!</strong></li>\n  <li><strong>Duck Typing:</strong><br>\n  If an object has a method, that method can be called regardless of the object type. Caveat being, make sure the arguments (if any) in the method are specified correctly for all objects!\n  </li>\n    <li>Python packages use <strong>common methods</strong> that make it easy to change between model types without changing a lot of code. \n  </li>\n</ul>\n\n## Additional Insights\n\n<div style=\"font-size: 1.1em; max-width: 900px; margin: 1em auto 0 auto; line-height: 1.7;\">\n  <ul>\n    <li>\n      <strong>Predictable APIs enable seamless model switching:</strong><br>\n      Swapping models like <code>LogisticRegression</code> → <code>RandomForestClassifier</code> usually requires minimal code changes.\n    </li>\n    <li style=\"margin-top: 1em;\">\n      <strong>scikit-learn prioritizes interoperability:</strong><br>\n      Its consistent class design integrates with tools like <code>Pipeline</code>, <code>GridSearchCV</code>, and <code>cross_val_score</code>.\n    </li>\n    <li style=\"margin-top: 1em;\">\n      <strong>Class attributes improve model transparency:</strong><br>\n      Access attributes like <code>.coef_</code>, <code>.classes_</code>, and <code>.feature_importances_</code> for model interpretation and debugging.\n    </li>\n    <li style=\"margin-top: 1em;\">\n      <strong>Custom classes are central to deep learning:</strong><br>\n      Frameworks like PyTorch and TensorFlow require you to define your own model classes by subclassing base models.\n    </li>\n    <li style=\"margin-top: 1em;\">\n      <strong>Mixins support modular design:</strong><br>\n      Mixins (e.g., <code>ClassifierMixin</code>) let you add specific functionality without duplicating code.\n    </li>\n  </ul>\n</div>\n\n## **Pre-Reading for This Session**  \n- [Scikit-learn Documentation](https://scikit-learn.org/stable/user_guide.html)  \n- [Introduction to OOP in Python (Real Python)](https://realpython.com/python3-object-oriented-programming/)  \n- [Plotnine Reference](https://plotnine.org/reference/)\n\n",
    "supporting": [
      "session4v2_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}