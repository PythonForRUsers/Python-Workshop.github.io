---
title: '**Session 4 ‚Äì Object-Oriented Programming and Modeling Libraries**'
jupyter: python3
format: 
    revealjs:
        theme: [default, slideshowv2.scss]
        code-copy: true   
        smaller: false
        code-block-height: 850px
        highlight-style: pygments
        width: 1600   # default is 960
        height: 900  # default is 700
        transition: none
        code-line-numbers: true
        self-contained: true
execute:
  freeze: auto
  eval: true
  echo: true
  warning: false
  error: false
---

## Session Overview

<div class="clean-text">

In this session, we'll explore how Python's object-oriented nature affects our modeling workflows. <br>

<strong style="font-size: 1.15em">Topics:</strong>  
<ul>
  <li><strong>Intro to OOP</strong> and how it makes modeling in Python different from R</li>
  <li><strong>Building and extending classes</strong> using inheritance and mixins</li>
  <li><strong>Applying OOP to machine learning</strong> through demos with scikit-learn</li>
  <ul>
    <li>Creating and using models</li>
    <li>Plotting data with `plotnine` and `seaborn`</li>
  </ul>
</ul>

</div>

  <aside class="notes">
    This is the last session of the intro to python workshops. There will be a brief 'homework' that covers the topics in this session. I do not anticipate having much time left over at the end of this session so if you have any questions about that please feel free to reach out to us via teams!
  </aside>

# Introduction 



## Why Python? üêç
<div class="clean-text">

::: columns
::: column

#### R: Built by Statisticians for Statisticians
- Excels at:
    - Statistical analysis and modeling  
    - Clean outputs and tables from models
    - Beautiful data visualizations with simple code  


:::

::: column

#### Python: General-Purpose Language
- Excels at: 
    - Machine Learning, Neural Networks & Deep Learning (scikit-learn, PyTorch, TensorFlow)   
    - Image & Genomic Data Analysis (scikit-image, biopython, scanpy)
    - Software & Command Line Interfaces, Web Scraping, Automation

:::
:::


Python‚Äôs broader ecosystem makes it the go-to language in domains like AI, bioinformatics, data engineering, and computational biology.  

> **Note:** Packages like `rpy2` and `reticulate` make it possible to use both R and Python in the same project, but those are beyond the scope of this course.  
> A primer on `reticulate` is available here: [https://www.r-bloggers.com/2022/04/getting-started-with-python-using-r-and-reticulate/](https://www.r-bloggers.com/2022/04/getting-started-with-python-using-r-and-reticulate/)
</div>

  <aside class="notes">
As we talked about in session 1, R and Python have different strengths. R was designed for statistics and excels at statistical analysis & modeling, clean outputs and beautiful visualizations. Python is a general purpose programming language that excels at things like machine learning, image/genomic analysis and software. 

Python's broader ecosystem makes it the go-to language for things like AI, bioinformatics, data engineering and computational biology. 

There are packages like rpy2 (for python) and reticulate (for R) that make it possible to use both R and python in the same project, but those are beyond the scope of this course. 
  </aside>

## Programming Styles: R vs Python
<div class="clean-text">
<br>
In the first session, we talked briefly about functional vs object-oriented programming:   
<br>

>  <span style="color: #007acc"><strong>Functional programming:</strong></span> focuses on functions as the primary unit of code <br>
>  <span style="color: #007acc"><strong>Object-oriented programming:</strong></span> uses objects with attached attributes(data) and methods(behaviors) <br>

- R leans heavily on the functional paradigm ‚Äî you pass data into functions and get back results, in most cases without altering the original data. Functions and pipes (%>%) dominate most workflows.

- In Python, <span style="color: #007acc"><strong>everything is an object</strong></span>, even basic things like lists, strings, and dataframes. A lot of 'functions' are instead written as object-associated methods. Some of these methods modify the objects in-place by altering their attributes. <span style="color: #007acc; font-weight:bold">Understanding how this works is key to using Python effectively!</span>

> You‚Äôve already seen this object-oriented style in Sessions 2 and 3 ‚Äî you create objects like lists or dataframes, then call methods on them like `.append()` or `.sort_values()`. In python, instead of piping, we sometimes chain methods together.
</div>

  <aside class="notes">
In the first session we briefly mentioned functional vs object-oriented programming. Functional programming uses functions asa the primary unit of code while object-oriented programming uses objects with attached attributes (data) and methods (behaviors). 

When we're programming in R, we typically use (and sometimes write) functions that we pass data into and get back results without altering the original data. Functions and pipes dominate most workflows. 

In python, we use objects. Everything is an object (lists, strings, dataframes, etc) and a lot of 'functions' are written as object-associated methods. Some of these methods modify the objects in place by altering their attributes instead of returning a new object. Understanding this is key to using python effectively. 
  </aside>


## Modeling in Python 
<div class="small-clean-text">
<span style="color: #007acc"><strong>Python absolutely uses **functions**‚Äîjust like R!</strong></span>
They're helpful for **data transformation**, **wrangling**, and **automation tasks** like looping and parallelization. <br>

But when it comes to **modeling**, libraries are designed around **classes**: blueprints for creating objects that store data (**attributes**) and define behaviors (**methods**).  <br>

  - `scikit-learn` is great for getting started‚Äîeverything follows a simple, consistent OOP interface. Its API is also consistant with other modeling packages, like [xgboost](https://xgboost.readthedocs.io/en/release_3.0.0/) and [scvi-tools](https://docs.scvi-tools.org/en/stable/index.html).
  - [<u>`scikit-survival`</u>](https://scikit-survival.readthedocs.io/en/stable/) is built on top of `scikit-learn`. [https://scikit-survival.readthedocs.io/en/stable/user_guide/00-introduction.html](https://scikit-survival.readthedocs.io/en/stable/user_guide/00-introduction.html) is a good tutorial for it.
  - `PyTorch` and `TensorFlow` are essential if you go deeper into neural networks or custom models‚Äîyou‚Äôll define your **own model classes** with attributes and methods, but the basic structure is similar to `scikit-learn`.  
  - [<u>`statsmodels`</u>](https://www.statsmodels.org/stable/gettingstarted.html) is an alternative to `scikit-learn` for statistical analyses and has R-like syntax and outputs. It's a bit more complex than `scikit-learn` and a bit less consistant with other packages in the python ecosystem. *[https://wesmckinney.com/book/modeling](https://wesmckinney.com/book/modeling) is a good tutorial for statsmodels.*

> üí° To work effectively in Python, especially for tasks involving modeling or model training, <span style="color: #007acc"><strong>it helps to think in terms of objects and classes, not just functions.</strong></span>   
</div>

  <aside class="notes">
Even though python is more object focused, it still uses functions! They are particualrly helpful for things like data transformation, data wrangling and automation tasks like looping and parallelization. However, when it comes to modeling, libraries are designed around classes, which are like blueprints for creating objects that store data and define behaviors. 

These are some of the most commonly used modeling libraries in python. For this session, we will focus on scikit-learn, which is relatively simple and follows a consistent interface. Its API (Application Programming Interface) is also consistant with other modeling packages like xgboost and scvi-tools. scikit-survival is built off of scikit-learn and therefore has a similar API. 

Pytorch and tensorflow are more complex but essential for neural networks/deep learning models. With these packages, you define your own model classes, but the basic class structure is similar to sklearn. 

Finally, there is statsmodels, which is an alternative to sklearn for statistical analyses. It has R-like syntax and outputs and is a bit more complicated to use than sklearn. Personally, if I wanted R-like outputs I'd use reticulate or just save my data and re-load in R, but statsmodels is available if needed. 
  </aside>


## Why Does OOP Matter in Python Modeling? 
<div class="clean-text">
<strong>In Python modeling frameworks:</strong>
</p>

<ul style="line-height: 1.15; margin-top: 0;">
  <li>Models are <strong>instances of classes</strong></li>
  <li>You call methods like <code>.fit()</code>, <code>.predict()</code>, <code>.score()</code></li>
  <li>Internal model details like coefficients or layers are stored as <strong>attributes</strong></li>
</ul>

<p style="margin-top: 0.5em; margin-bottom: 0.75em;">
This makes model behavior <strong>consistent</strong> between model classes and even libraries. It also <strong>simplifies</strong> creating/using pre-trained models: both the architecture and learned weights are bundled into a single object with expected built-in methods like `.predict()` or `.fine_tune()`.
</p>

<blockquote style="margin-top: .75em;">
  Instead of having a separate results object, like in R, you would retrieve your results by accessing an attribute or using a method that is attached to the model object itself. 
</blockquote>
<br>

<span style="color: #007acc; font-size: 0.95em;"> <strong><em>We‚Äôll focus on `scikit-learn` in this session, but these ideas carry over to other libraries like `xgboost`, `statsmodels`, and `PyTorch`.</em></strong> </span> 
</div>

  <aside class="notes">
So why is OOP so important for python modeling? First, models in python are instances of classes that have attahced methods like fit, predict and score. The internal model details like coefficients or layers are stored as attributes. 

Definining models as instances of classes is useful because it makes model behavior consistant between model classes (due to inheritance which i'll explain more about shortly) and even libraries. It also simplifies creating/using pre-trained models because the model architecture and learned weights are bundled into a single object that can be loaded. Expected built-in methods are also included in this bundle. 

Additionally, instead of having a separate results object like in R, you can retrieve your results by either accessing an attribute or using a method attached to the model object itself. 
  </aside>

# Part 1: Object-Oriented Programming {.smaller}

## **Key OOP Principles (Recap)**

<div class="smaller-clean-text">
In OOP, code is structured around **objects** (as opposed to functions). This paradigm builds off the following principles: 

::: {.fragment}
1. **Encapsulation**: Bundling data and methods together in a single unit.  
   - A `StandardScaler` object stores mean and variance data and has `.fit()` and `.transform()` methods
:::

::: {.fragment}
2. **Inheritance**: Creating new classes based on existing ones.  
   - `sklearn.LinearRegression` inherits attributes and methods from a general regression model class.   
:::
::: {.fragment}
3. **Abstraction**: Hiding implementation details and exposing only essential functionality.  
   - e.g., `.fit()` works the same way from the outside, regardless of model complexity  
:::
::: {.fragment}
4. **Polymorphism**: Objects of different types can be treated the same way if they implement the same methods. 
    - Python‚Äôs **duck typing**:  
      - ü¶Ü *"If it walks like a duck and quacks like a duck, then it must be a duck."* ü¶Ü  
      - ex: If different objects all have a .summarize() method, we can loop over them and call .summarize() without needing to check their class. As long as the method exists, Python will know what to do.
      - This lets us easily create [<u>pipelines</u>](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) that can work for many types of models.  

>We won't cover [pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline) here, but they are worth looking into!
:::
</div>

  <aside class="notes">
If you read through the OOP pre-reading this will be a review. In OOP, code is structured around objects and this paradigm builds off of the following principles: 
1: Encapsulation - bundling data and methods together in a single unit. For example, an object of the `Standard Scalar` class, which we'll use later, stores mean and variance data and has fit and transform methods. 
2: Inheritance - which allows us to create new classes that inherit attributes and methods from existing classes. For example, the lienar regression model class in sklearn inherits attributes and methods from a general regression model class
3: Abstraction - hiding implementation detals and exposing only essential functionality
    For example, the fit method works the same way from the outside regardless of model complexity
4: Polymorphism means we can treat objects of different types the same way‚Äîas long as they implement the same method.

In Python, this is done through something called duck typing, which comes from the phrase:
"If it walks like a duck and quacks like a duck, it's probably a duck."
In other words, Python doesn‚Äôt care what class an object is‚Äîit just checks whether it has the method you're trying to call.

So if three different objects each have a .summarize() method, we can loop through them and call .summarize() without needing to check their type first. As long as the method exists and accepts the right arguments, Python will just run it.
This flexibility is one of the biggest reasons OOP is so useful for machine learning. Most model classes in Python‚Äîwhether it‚Äôs logistic regression, random forests, or a neural network‚Äîcome with a standard set of methods like .fit(), .predict(), and .score().
That consistency means we can swap models in and out with very little change to our code. It also makes it easy to build reusable code‚Äîlike pipelines‚Äîthat can work across different model types without needing special handling for each one.
  </aside>

## **Classes and Objects** 
<div class="clean-text">

**Classes** are **blueprints** for creating objects. Each object contains:  
<ul style="font-size: .8em; line-height: 1.2;">
    <li><strong>Attributes</strong> (data): model coefficients, class labels</li>
    <li><strong>Methods</strong> (behaviors): <code>.fit()</code>, <code>.predict()</code></li>
</ul>

<span style="font-size: .8em;">üëâ To Get the class of an object, use:</span>

```{python}
#| eval: false
type(object) # Returns the type of the object
```

<span style="font-size: .8em;">üëâ To check if an object is an instance of a particular class, use:</span>

```{python}
#| eval: false
isinstance(object, class)  # Returns True if `object` is an instance of `class`.
```
<br>

<p style="color: #007acc; font-weight: 600;">
Knowing what class an object belongs to helps us understand what methods and attributes it provides.
</p>
</div>

<aside class="notes">
Like I mentioned earlier, classes are blueprints for creating objects which contain attributes (data) like model coefficients or labels and methods like fit and predict. 

To get the class of an object in python, we can use the 'type' function. To check if an object is an instance of a particular class, we can use the 'isinstance' function. 

Knowing what class an object belongs to can help us understand what methods and attibutes it should have, but we do not need to know class to use a method. 
</aside>

# Example: Creating a Class

## Base Classes 
<div class="clean-text">

A **base class** (or parent class) serves as a template for creating objects. Other classes can inherit from it to reuse its properties and methods.

Classes are defined using the `class` keyword, and their structure is specified using an `__init__()` method for initialization. 

::: {.fragment}

For example, we can define a class called `Dog` and give it attributes that store data about a given dog and methods that represent behaviors an object of the `Dog` class can perform. We can also edit the [**special** or **"dunder"** methods](https://docs.python.org/3/reference/datamodel.html#special-method-names) (short for double underscore) that define how objects behave in certain contexts. 

```{python}
#| code-line-numbers: "1-2|3,4|6-7|9-13"
class Dog: ## begin class definition
    def __init__(self, name, breed): ## define init method
        self.name = name ## add attributes
        self.breed = breed

    def speak(self): ## add methods
        return f"{self.name} says woof!"

    def __str__(self): # __str__(self) tells python what to display when an object is printed
        return f"Our dog {self.name}"

    def __repr__(self): # add representation to display when dog is called in console
        return f"Dog(name={self.name!r}, breed={self.breed!r})"
```
:::
</div>

<aside class="notes">
A base class serves as a template for creating objects and other classes can inherit its attributes and methods. Classes are defined using the 'class' keyword and their structure is specified using an init method for initialization. 

For example, we can create a class called Dog and give it attributes and methods. We can also edit special or dunder methods that define how objects of this class behave in certain contexts. 

First we will define our init method, which sets up the structure for objects of the dog class. The parameters here (name and breed) can be stored as attributes. The speak method here defines the speech behavior for the dog and returns a string, that changes based on the value stored in the 'name' attribute. 

We can also define our str and repr methods which tell python what to do when an object is printed (str) or called in console (repr). These methods are useful for displaying information about objects at a glance. We will see an interesting example of a repr method later on. 

</aside>

## Creating a dog
<div class="clean-text">

Creating an instance of the `Dog` class lets us model a particular dog:  

```{python}
#| output-location: fragment
buddy = Dog("Buddy", "Golden Retriever")
print(f"Buddy is an object of class {type(buddy)}")
```
<ul>
    <li>We set the value of the attributes [`name` and `breed`], which are then stored as part of the `buddy` object </li>   
    <li> We can use any methods defined in the Dog class on `buddy` </li>   
</ul>  

```{python}
#| output-location: fragment
#| code-line-numbers: "1-3|5-6|8-9"
## if we want to see what kind of dog our dog is
## we can call buddy's attributes
print(f"Our dog {buddy.name} is a {buddy.breed}.")

## we can also call any Dog methods
print(buddy.speak())  

## including special methods
buddy ## displays what was in the __repr__() method
```

 
<span style="color: #007acc"><strong>Note:</strong> For python methods, the `self` argument is assumed to be passed and therefore we do not put anything in the parentheses when calling `.speak()`. For attributes, we do not put () at all.</span>

</div>

<aside class="notes">
If we want to model a particular dog, we create an instance of the dog class. The arguments we pass to `Dog` set the name and breed of the dog.

If we want to prove that buddy is an object of class dog, we can use type(buddy).

Once we create buddy, we can use any attributes or methods defined in the dog class. 
</aside>

## Derived (Child) Classes  
<div class="clean-text">

Derived/child classes build on base classes using the principle of inheritence. <br>

Now that we have a `Dog` class, we can build on it to create a specialized `GuardDog` class. 

```{python}
#| code-line-numbers: "|1|2-7|"
class GuardDog(Dog):  # GuardDog inherits from Dog
    def __init__(self, name, breed, training_level): ## in addition to name and breed, we can 
        # define a training level. 
        # Call the parent (Dog) class's __init__ method
        super().__init__(name, breed)
        self.training_level = training_level  # New attribute for GuardDog that stores the 
        # training level for the dog

    def guard(self): ## checks if the training level is > 5 and if not says train more
        if self.training_level > 5:
            return f"{self.name} is guarding the house!"
        else:
            return f"{self.name} needs more training before guarding."
    
    def train(self): # modifies the training_level attribute to increase the dog's training level
        self.training_level = self.training_level + 1
        return f"Training {self.name}. {self.name}'s training level is now {self.training_level}"

# Creating an instance of GuardDog
rex = GuardDog("Rex", "German Shepherd", training_level= 5)
```
</div>

<aside class='notes'>
Derived or child classes build on base classes using the principle of inheritence. This helps us build more specialized classes without having to start from scratch.

If we want to make a new guardDog class, we can start by inheriting the attributes and methods of the dog class by putting 'Dog' in the parentheses here. 

When we define our init method, we can first call the parent class's init method to assign the dog's name and breed. We can also add an additional attribute that stores the dog's training level. 

Because of inheritence, guarddogs also have the speak method, but we can also give them new methods like guard and train. The guard method, like the speak method earlier, just returns a value. However, the train method actually modifies the training_level attribute of the object in-place. This means that, if we want to train a guarddog, we do not have to assign the trained dog to a new variable to preserve the trained state. 

Let's make an instance of guarddog called rex and we can see how this works. 
</aside>

---

Now that we have a dog (rex), we can call on any of the methods/attributes introduced in the `Dog` class as well as the new `GuardDog` class.

Using methods from the base class: 
```{python}
#| slide-type: fragment
#| output-location: fragment
print(rex.speak())
rex
```

. . . 

<span style="font-size: 1.15em; color: #007acc"><strong>This is the power of inheritance</strong></span>‚Äîwe don‚Äôt have to rewrite everything from scratch!

. . . 

Using a method from the child class: 
```{python}
#| slide-type: fragment
#| output-location: fragment
print(f"{rex.name}'s training level is {rex.training_level}.")
print(rex.guard()) 
```

<aside class='notes'>

Of course we can use the speak() and repr() methods that we defined for the dog class. We can also use the training method that we just defined. **adv**

to successfully guard, a rex needs a training level greater than 5. Currently his level is 5, so he needs more training before guarding.

</aside>

---

Unlike standalone functions, methods in Python often update objects in-place‚Äîmeaning they modify the object itself rather than returning a new one.

We can use the `.train()` method to increase rex's training level. 
```{python}
#| output-location: fragment
print(rex.train())
```


:::{.callout-important}  
## Be Careful!!!

Calling rex.train() within a print statement still updates rex's training level. If we were to do this instead:
```{.python}
rex.train()
print(rex.train())
```
 it would train rex twice!
:::

. . . 

Now if we check, 
```{python}
#| output-location: fragment
print(f"{rex.name}'s training level is {rex.training_level}.")
print(rex.guard()) 
```


. . . 

As with Rex, <span style="color: #007acc"><strong>child classes inherit all attributes (`.name` and `.breed`) and methods (`.speak()` `__repr__()`) from parent classes.</strong></span> They can also have new methods (`.train()`).

<aside class='notes'>
Unlike standalone functions, methods like train() update objects in-place instead of returning a new object. This allows us to increase rex's training level without storing rex to a new variable. 

Please note that even though we're calling rex.train() within a print statement, it still updates rex's training level! 

After training rex once, we can see that his training level is sufficient to allow him to guard! **adv**

as we saw with rex, child classes inherit all attributes and methods from their parent classes, but they can also have new ones. 

</aside>


## Mixins
<div class="clean-text">  

A **mixin** is a special kind of class designed to add **functionality** to another class. Unlike base classes, mixins aren‚Äôt used alone.  

:::{.fragment}

For example, scikit-learn uses mixins like:  
  - `sklearn.base.ClassifierMixin` (adds classifier-specific methods)  
  - `sklearn.base.RegressorMixin` (adds regression-specific methods)  

which it adds to the `BaseEstimator` class to add functionality. <br> <br>

To finish up our dog example, we are going to define a mixin class that adds learning tricks to the base `Dog` class and use it to create a new class called `SmartDog`.
:::
</div>

<aside class="notes">
Mixins area special kind of class that take advantage of the inheritence property to add functionality to other classes without needing to re-define the initial class structure. Unlike base classes, mixins are not useful on their own. 

Some examples of mixin classes are ClassifierMixin and RegressorMixin from scikit learn, which add functionality to the BaseEstimator class. To finish up our dog example, we are going to define a mixin class that adds learning tricks to the base `Dog` class and use it to create a new class called `SmartDog`. 
</aside>

---

When creating a mixin class, we let the other base classes carry most of the initialization

```{python}
#| code-line-numbers: "1-2,3|4|6-20"
class TrickMixin: ## mixin that will let us teach a dog tricks
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)  # Ensures proper initialization in multi inheritance
        self.tricks = []  # Add attribute to store tricks

## add trick methods
    def learn_trick(self, trick):
        """Teaches the dog a new trick."""
        if trick not in self.tricks:
            self.tricks.append(trick)
            return f"{self.name} learned a new trick: {trick}!"
        return f"{self.name} already knows {trick}!"

    def perform_tricks(self):
        """Returns a list of tricks the dog knows."""
        if self.tricks:
            return f"{self.name} can perform: {', '.join(self.tricks)}."
        return f"{self.name} hasn't learned any tricks yet."

## note: the TrickMixin class is not a standalone class!
```

<aside class="notes">
When we create a mixin class, we let the base class carry most of the initialization. 

A mixin class can add both attributes **adv** and methods, and it can potentially work with multiple base classes. 
</aside>

---

By including both `Dog` and `TrickMixin` as base classes, we give objects of class `SmartDog` the ability to speak and learn tricks!

```{python}
#| slide-type: fragment
#| output-location: fragment
class SmartDog(Dog, TrickMixin):
    def __init__(self, name, breed):
        super().__init__(name, breed)  # Initialize Dog class
        TrickMixin.__init__(self)  # Initialize TrickMixin separately

# a SmartDog object can use methods from both parent object `Dog` and mixin `TrickMixin`.
my_smart_dog = SmartDog("Buddy", "Border Collie")
print(my_smart_dog.speak()) 
```

<br>

```{python}
#| slide-type: fragment
#| output-location: fragment
print(my_smart_dog.learn_trick("Sit"))  
print(my_smart_dog.learn_trick("Roll Over")) 
print(my_smart_dog.learn_trick("Sit"))  

```
<br>

```{python}
#| slide-type: fragment
#| output-location: fragment
print(my_smart_dog.perform_tricks()) 
```

<aside class="notes">

By including both dog and trickmixin as base classes, we give smartdog objects the ability to 'speak' and learn tricks. **-adv through-**

</aside>

## Duck Typing 


<div class="clean-text"> 

>ü¶Ü "If it quacks like a duck and walks like a duck, it's a duck." ü¶Ü

Python's duck typing makes our lives a lot easier, and is one of the main benefits of methods over functions:
<ul>
    <li><b>Inheritence</b> - objects inherit methods from base classes</li>
    <li><b>Repurposing old code</b> - methods by the same name work the same for different model types</li>
    <li><b>Not necessary to check types before using methods</b> - methods are assumed to work on the object they're attached to</li>
</ul>


</div>  

We can demonstrate duck typing by defining two new base classes that are different than `Dog` but also have a `speak()` method.

. . . 

```{python}
#| code-line-numbers: "|5-6,12-13"
class Human:
    def __init__(self, name):
        self.name = name

    def speak(self):
        return f"{self.name} says hello!"

class Parrot:
    def __init__(self, name):
        self.name = name

    def speak(self):
        return f"{self.name} says squawk!"

```

<aside class="notes">

In my opinion python's ducktyping makes our lives a lot easier and is a good reason to use methods when possible. Duck typing makes it easy to repurpose old code with few changes - because methods by the same name work the same for different model types, even if the underlying method code is very different. Models from the same package likely share a base class, or at least can be expected to have a similar API. Duck typing also stops us from having to check object types before performing operations as methods are assumed to work on the object they're attached to. 

We can demonstrate this by defining two new base classes that are different than dog but also have a speak method. **-adv-**

The human and parrot class here both have a speak method and a name attribute like the dog class. 

</aside>

## Duck Typing in Action
<div class="clean-text">  

Even though `Dog`, `Human` and `Parrot` are entirely different classes...

</div>

```{python}
#| output-location: fragment
def call_speaker(obj):
    print(obj.speak())

call_speaker(Dog("Fido", "Labrador"))
call_speaker(Human("Alice"))
call_speaker(Parrot("Polly"))
```

. . .  

They all implement `.speak()`, so Python treats them the same!

In the context of our work, this would allow us to make a pipeline using models from different libraries that have the same methods. 

<span style="font-size: 1.2em; color: #007acc"><strong>While our dog example was very simple, this is the same way that model classes work in python!</strong></span> 

:::{.callout-warning}  

With duck typing, Python lets us <b><i>use methods</i></b> without breaking. It does not mean that any given method is correct to use in all cases, or that all similar objects will have the same methods.

:::

<aside class="notes">

Even though the three classes are different, we can still write a function that performs 'obj.speak()' and pass in any object with this method without having to worry about the object's class. 

</aside>

## **Example: OOP in Machine Learning and Modeling**  

<div class="clean-text">  

Machine learning models in Python are implemented as **classes**.    
<ul>
 <li>When you create a model, you‚Äôre **instantiating an object** of a predefined class (e.g., `LogisticRegression()`).</li>    
 <li>That model has attributes (parameters, coefficients) and methods (like `.fit()` and `.predict()`).</li>
</ul> 

For example `LogisticRegression` is a model class that inherits from `SparseCoefMixin` and `BaseEstimator`.

```{.python}
class LogisticRegression(LinearClassifierMixin, SparseCoefMixin, BaseEstimator):
  
```

<span style="top-padding: .35em;">To perform logistic regression, we create an instance of the `LogisticRegression` class.</span>

```{.python}
## Example: 
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()  # Creating an instance of the LogisticRegression class
model.fit(X_train, y_train)   # Calling a method to train the model
predictions = model.predict(X_test)  # Calling a method to make predictions
coefs = model.coef_ # Access model coefficients using attribute
``` 

</div>

<aside class="notes">

Because models in python are defined by classes, when you create a model you are instantiating an object of a predefined class. When you fit a model and use it to make predictions, you are using methods! You can also access attributes like .coef_ to get coefficients. 

</aside>

## **Key Benefits of OOP in Machine Learning**  
<div class="clean-text">  

1. **Encapsulation** ‚Äì Models store parameters and methods inside a single object.  
2. **Inheritance** ‚Äì New models can build on base models, reusing existing functionality.  
3. **Abstraction**  ‚Äì `.fit()` should work as expected, regardless of complexity of underlying implimentation.
4. **Polymorphism (Duck Typing)** ‚Äì Different models share the same method names (`.fit()`, `.predict()`), making them easy to use interchangeably, particularly in analysis pipelines. 

Understanding **base classes** and **mixins** is especially important when working with deep learning frameworks like **PyTorch and TensorFlow**, which require us to create our own model classes.  

</div>

<aside class="notes">

To recap: python's oject oriented nature has some notable benefits for modeling and machine learning. 

</aside>

# Part B - Demo Projects

**Apply knowledge of OOP to modeling using scikit-learn**

---

## üêß Mini Project: Classifying Penguins with scikit-learn

<div class="clean-text">  

Now that you understand **classes** and **data structures** in Python, let‚Äôs apply that knowledge to classify **penguin species** using two features:  
<ul>
<li>`bill_length_mm`</li>  
<li>`bill_depth_mm`</li>
</ul> 


We‚Äôll explore:  

<ul>
<li> **Unsupervised learning** with **K-Means** clustering (model doesn't 'know' y)</li>
<li>**Supervised learning** with a **k-NN classifier** (model trained w/ y information)</li>
</ul>

All `scikit-learn` models are designed to have 

::: columns
::: column
**Common Methods:**  
<ul>
<li>`.fit()` ‚Äî Train the model </li>  
<li>`.predict()` ‚Äî Make predictions  </li>
</ul>    

:::
::: column
**Common Attributes:** 
<ul>
<li>`.classes_`, `.n_clusters_`, etc.</li>
</ul>  

:::
:::
> This is true of the scikit-survival package too! 

</div>

<aside class="notes">

For the rest of this session, we'll be going through 2 mini projects to classify penguin species using bill length and bill depth using kmeans and knn models from scikit-learn. In terms of API, all sklearn models are designed to have common methods like fit and predict and have some attributes like .classes_ or .n_clusters_ that store information needed to specify the model. 

</aside>

## Import Libraries
<div class="clean-text">  

<span style="font-size: 1.2em; color: #007acc"><strong>Before any analysis, we must import the necessary libraries.</strong></span>  

For large libraries like **scikit-learn**, **PyTorch**, or **TensorFlow**, we usually do **not** import the entire package. Instead, we selectively import the **classes** and **functions** we need.

::: columns
::: column

**Classes**  
- `StandardScaler` ‚Äî for feature scaling  
- `KNeighborsClassifier` ‚Äî for supervised k-NN classification  
- `KMeans` ‚Äî for unsupervised clustering

<br>

> üî§ **Naming Tip**:  
> - `CamelCase` = **Classes**  
> - `snake_case` = **Functions**

:::
::: column

**Functions**  
- `train_test_split()` ‚Äî to split data into training and test sets  
- `accuracy_score()` ‚Äî to evaluate classification accuracy  
- `classification_report()` ‚Äî to print precision, recall, F1 (balance of precision and recall), Support (number of true instances per class)
- `adjusted_rand_score()` ‚Äî to evaluate clustering performance
:::
:::

</div>

<aside class="notes">

Our first step is always to import any necessary libraries - but in this case, we will import specific classes and functions we plan to use. For our classes, we'll need standardscaler as well as our two model classes. We'll also import some useful functions for splitting our dataset into training/test sets and computing classification scores. 

A quick tip: for most python libraries, classes use CamelCase and functions use snake_case. 

</aside>

## Import Libraries

```{python}
#| code-line-numbers: "|13-17|18-21"
## imports
import pandas as pd
import numpy as np

from plotnine import *
import seaborn as sns
import matplotlib.pyplot as plt

from great_tables import GT

## sklearn imports

## import classes
from sklearn.preprocessing import StandardScaler 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans

## import functions
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, adjusted_rand_score
```  

## Data Preparation 

```{python}
#| code-line-numbers: "1,2|4-11|13-16|18-19"
#| output-location: fragment
# Load the Penguins dataset
penguins = sns.load_dataset("penguins").dropna()

# Make a summary table for the penguins dataset, grouping by species. 
summary_table = penguins.groupby("species").agg({
    "bill_length_mm": ["mean", "std", "min", "max"],
    "bill_depth_mm": ["mean", "std", "min", "max"],
    "sex": lambda x: x.value_counts().to_dict()  # Count of males and females
})

# Round numeric values to 1 decimal place (excluding the 'sex' column)
for col in summary_table.columns:
    if summary_table[col].dtype in [float, int]:
        summary_table[col] = summary_table[col].round(1)

# Display the result
display(summary_table)
```


<aside class="notes">

We're using the penguins dataset from the seaborn package, so we can use the load_dataset function to get that. We can also drop any na values with the .dropna() method. 

We can then make a quick summary table for the dataset using some of the methods covered last session. 

</aside>

## Data Visualization

<div class="small-clean-text"> 

To do visualization, we can use either seaborn or plotnine. `plotnine` mirrors `ggplot2` syntax from R and is great for layered grammar-of-graphics plots, while `seaborn` is more convienient if you want to put multiple plots on the same figure. <br>


### Plotting with Plotnine vs Seaborn

::: columns
::: column

<span style="color: #007acc; font-weight: 600;">Plotnine (like ggplot2 in R)</span><br>
The biggest differences between `plotnine` and `ggplot2` syntax are: 
<ul> 
<li> With `plotnine` the whole call is wrapped in `()` parentheses</li>  
<li> Variables are called with strings (`""` are needed!) </li>
<li> If you don't use `from plotnine import *`, you will need to import each individual function you plan to use!</li>
</ul>

:::

::: column

<span  style="color: #007acc; font-weight: 600;">Seaborn (base matplotlib + enhancements)</span><br>
<ul>
<li> Designed for **quick, polished plots** </li> 
<li> Works well with **pandas DataFrames** or **NumPy arrays** </li> 
<li> Integrates with `matplotlib` for customization </li> 
<li> Good for things like **decision boundaries** or **heatmaps**</li> 
<li> Harder to customize than plotnine plots </li> 
</ul>

:::
:::

</div>

<aside class="notes">

To visually check out our data, we can use either plotnine (which is designed to work like ggplot) or seaborn (base matplotlib + enhancements). 
For single plots, plotnine can be more convenient but seaborn has better support for putting multiple plots on the same figure. 

</aside>

## Scatterplot with plotnine
<div class="clean-text"> 

To take a look at the distribution of our species by bill length and bill depth before clustering...
</div>

```{python}
#| fig-dpi: 600
#| output-location: fragment
plot1 = (ggplot(penguins, aes(x="bill_length_mm", y="bill_depth_mm", color="species"))
 + geom_point()
 + ggtitle("Penguin Species")
 + theme_bw())

display(plot1)
```

<aside class="notes">

To plot our bill length and depth with plotnine, we can pretty much use ggplot syntax, but wrap the whole statement in parentheses. However, our variable names need to be entered as strings. 

</aside>

## Scatterplot with seaborn

We can make a similar plot in seaborn. This time, let's include sex by setting the point style
```{python}
#| code-line-numbers: "1-2|4-12|14-18|20-22"
#| output-location: slide
# Create the figure and axes obects
fig, ax = plt.subplots(figsize=(10, 8))

# Create a plot 
sns.scatterplot(
    data=penguins, x="bill_length_mm", y="bill_depth_mm",
    hue="species", ## hue = fill
    style="sex",  ## style = style of dots
    palette="Set2", ## sets color pallet
    edgecolor="black", s=300, ## line color and point size 
    ax=ax              ## Draw plot on ax      
)

# Use methods on ax to set title, labels
ax.set_title("Penguin Bill Length vs Depth by Species")
ax.set_xlabel("Bill Length (mm)")
ax.set_ylabel("Bill Depth (mm)")
ax.legend(title="Species")

# Plot the figure
fig.tight_layout() 
#fig.show() -> if not in interactive
```

<aside class="notes">

If we want to do the same thing in seaborn, we can use sns.scatterplot() directly (which doesn't save the figure to a variable), 
or we can create figure and axis objects first and then draw the plot on the created axis. We then use methods on the axis object to set the titles and labels. 

Because we are working in an interactive notebook, we don't have to explicitly use plt.plot() to display the figure. 

</aside>


## Scaling the data - Understanding the Standard Scaler class
<div class="clean-text">  

For our clustering to work well, the predictors should be on the same scale. To achieve this, we use an instance of the `StandardScaler` class. 

```python
class sklearn.preprocessing.StandardScaler(*, copy=True, with_mean=True, with_std=True)
```
</div>
. . .   

**Parameters** are supplied by user  
- *copy*, *with_mean*, *with_std* <br>

**Attributes** contain the `data` of the object  
- `scale_`: scaling factor  
- `mean_`: mean value for each feature  
- `var_`: variance for each feature  
- `n_features_in_`: number of features seen during fit  
- `n_samples_seen`: number of samples processed for each feature <br>

**Methods** describe the `behaviors` of the object and/or `modify` its attributes  
- `fit(X)`: computes mean and std used for scaling and 'fits' scaler to data X  
- `transform(X)`: performs standardization by centering and scaling X with fitted scaler  
- `fit_transform(X)`: does both

<aside class="notes">

For our clustering to work well, we typically want to put all of the predictors on the same scale. We can do this using an instance of the StandardScaler class. 

We supply the parameters that dictate the behavior of the scaler at instantiation. The new scaler object has attributes that contain the information the object needs to perform its functions, like the scaling factor, feature means and variances, etc. 

It also has methods that can modify its attributes, like the fit method, which calculates the mean and variance used for scaling based on data X, and the tranform method which uses the calculated values to scale X. 

For convenience there is also the fit_transform() method that does both to the same data. When we look at our model classes, we'll see that they also have fit methods. 

</aside>

## Scaling Data

<div class="clean-text">  

```{python}
#| code-line-numbers: "|5-7"
# Selecting features for clustering -> let's just use bill length and bill depth.
X = penguins[["bill_length_mm", "bill_depth_mm"]]
y = penguins["species"]

# Standardizing the features for better clustering performance
scaler = StandardScaler() ## create instance of StandardScaler
X_scaled = scaler.fit_transform(X) 
```
:::{.fragment}

```{python}
#| echo: false
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

# Compute summary statistics and round to 2 sig figs
original_stats = X.agg(["mean", "std"])
scaled_stats = X_scaled_df.agg(["mean", "std"])

# Combine into a single table with renamed columns
summary_table = pd.concat([original_stats, scaled_stats], axis=1)
summary_table.columns = ["Bill_Length_o", "Bill_Depth_o", "Bill_Length_s", "Bill_Depth_s"]
summary_table.index.name = "Feature"

# Display nicely with great_tables
(
    GT(summary_table.reset_index()).tab_header("Original vs Scaled Features")
    .fmt_number(columns =  ["Bill_Length_o", "Bill_Depth_o", "Bill_Length_s", "Bill_Depth_s"], decimals=0)
    .tab_spanner(label="Original", columns=["Bill_Length_o", "Bill_Depth_o"])
    .tab_spanner(label="Scaled", columns=["Bill_Length_s", "Bill_Depth_s"])
    .cols_label(Bill_Length_o = "Bill Length", Bill_Depth_o = "Bill Depth", Bill_Length_s = "Bill Length", Bill_Depth_s = "Bill Depth")
    .tab_options(table_font_size = 16)
)
```
```{python}
#| echo: true
#| eval: false
#| code-fold: true
#| code-summary: "Show table code"
## Make X_scaled a pandas df
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

# Compute summary statistics and round to 2 sig figs
original_stats = X.agg(["mean", "std"])
scaled_stats = X_scaled_df.agg(["mean", "std"])

# Combine into a single table with renamed columns
summary_table = pd.concat([original_stats, scaled_stats], axis=1)
summary_table.columns = ["Bill_Length_o", "Bill_Depth_o", "Bill_Length_s", "Bill_Depth_s"]
summary_table.index.name = "Feature"

# Display nicely with great_tables
(
    GT(summary_table.reset_index()).tab_header("Original vs Scaled Features")
    .fmt_number(columns =  ["Bill_Length_o", "Bill_Depth_o", "Bill_Length_s", "Bill_Depth_s"], decimals=0)
    .tab_spanner(label="Original", columns=["Bill_Length_o", "Bill_Depth_o"])
    .tab_spanner(label="Scaled", columns=["Bill_Length_s", "Bill_Depth_s"])
    .cols_label(Bill_Length_o = "Bill Length", Bill_Depth_o = "Bill Depth", Bill_Length_s = "Bill Length", Bill_Depth_s = "Bill Depth")
    .tab_options(table_font_size = 16)
)
```
:::

</div>

<aside class="notes">

To keep things simple, we will just subset the penguins dataset and keep only bill length and depth for our X dataframe. For y, we'll grab the species as a pandas series. 

The next step is to create the scaler object and use fit_transform to both fit the scaler to X and return the transformed X matrix. One of the nice things about using a scaler object rather than a function is that the object saves the scaling parameters so we can reuse them later. 

This table here just shows how the mean and standard deviation change between the original and scaled features. It was made with great_tables and the code is included below. 
</aside>

## Understanding the KMeans model class
<div class="clean-text"> 

```python
class sklearn.cluster.KMeans(n_clusters=8, *, init='k-means++', n_init='auto', max_iter=300, 
tol=0.0001, verbose=0, random_state=None, copy_x=True, algorithm='lloyd')
```

**Parameters**: Set by user at time of instantiation  
- n_clusters, max_iter, algorithm  <br>

**Attributes**: Store object data    
- `cluster_centers_`: stores coordinates of cluster centers  
- `labels_`: stores labels of each point 
- `n_iter_`: number of iterations run (will be changed during method run)  
- `n_features_in` and `feature_names_in_`: store info about features seen during fit  <br>

**Methods**: Define object behaviors      
- `fit(X)`: fits model to data X
- `predict(X)`: predicts closest cluster each sample in X belongs to  
- `transform(X)`: transforms X to cluster-distance space  
</div>

<aside class="notes">

Now that we have our scaled data, the next step is to create and fit the kmeans model. Like the standardScaler class earlier, we have parameters set by the user. This is where we would specify the number of clusters we want, etc. 

There are also attributes that store information like the location of cluster centers and the cluster labels associated with each point. These attributes are set by the fit method and wont exist for a model that has not been fit yet.

And we have the fit and tranasform methods like the scaler class. We also get a predict method that lets us predict cluster labels for any new input data in the same cluster-space. 

</aside>

---

<div class="clean-text"> 

### Create model

```{python}
## Choosing 3 clusters b/c we have 3 species
kmeans = KMeans(n_clusters=3, random_state=42) ## make an instance of the K means class
kmeans
```
<br>

### Fit model to data

```{python}
#| output-location: fragment
## the fit
penguins["kmeans_cluster"] = kmeans.fit_predict(X_scaled)

## now that we fit the model, we should have cluster centers
print("Coordinates of cluster centers:", kmeans.cluster_centers_)

## shows that model is fitted
kmeans
```
</div>

<aside class="notes">

Now we create our model, setting the number of clusters to 3 because we have 3 species. One of the fun things about sklearn is that, the repr method gives us this cute little block representation of our model. It shows the parameters that we set during instantiation as well as the model status (the i icon) and link to the model documentation (the ? icon).
For non-fitted models, the box is orange and, once we fit the model, the box will be blue!

We can use the fit-predict method to fit the model to X_scaled and return the cluster labels, which we can store in the penguins dataframe as 'kmeans_cluster'.

Now that the model has been fit, we can check out the cluster centers (remember that this is in scaled X space) and also look at our model representation again to see that it's turned blue. 

</aside>

## Use function to calculate ARI
<div class="clean-text"> 

To check how good our model is, we can use one of the functions included in the sklearn library.

The `adjusted_rand_score()` function evaluates how well the cluster groupings agree with the species groupings while adjusting for chance. 

```{python}
#| output-location: fragment
# Calculate clustering performance using Adjusted Rand Index (ARI)
kmeans_ari = adjusted_rand_score(penguins['species'], penguins["kmeans_cluster"])
print(f"k-Means Adjusted Rand Index: {kmeans_ari:.2f}")
```
</div>

<aside class="notes">

So far, we've mostly used methods in our analysis, but for things that are not model-specific, functions can be useful! We can use the adjusted_rand_score() function to evaluate how well the cluster groupings agree with the species groupings. 

Since this function just needs the 'true' labels and the cluster labels, with no actual model information, it makes sense for it to be a function and not a method. 

</aside>

---

### We can also use methods on our data structure to create new data

<div class="clean-text"> 
- We can use the `.groupby()` method to help us plot cluster agreement with species label as a heatmap
- If we want to add sex as a variable to see if that is why our clusters don't agree with our species, we can use a scatterplot
- Using seaborn and matplotlib, we can easily put both of these plots on the same figure. 
<br>

```{python}
#| code-line-numbers: "1-6|8-14|16-17"
# Count occurrences of each species-cluster-sex combination
# (.size gives the count as index, use reset_index to get count column.)
scatter_data = (penguins.groupby(["species", "kmeans_cluster", "sex"])
                .size()
                .reset_index(name="count"))
species_order = list(scatter_data['species'].unique()) ## defining this for later

# Create a mapping to add horizontal jitter for each sex for scatterplot
sex_jitter = {'Male': -0.1, 'Female': 0.1}
scatter_data['x_jittered'] = scatter_data.apply(
    lambda row: scatter_data['species'].unique().tolist().index(row['species']) +
     sex_jitter.get(row['sex'], 0),
    axis=1
)

heatmap_data = scatter_data.pivot_table(index="kmeans_cluster", columns="species", 
values="count", aggfunc="sum", fill_value=0)
```
</div>

<aside class="notes">

If we want to visualize the cluster agreement, we can group the data by species, cluster and sex and use the .size() method to get the number of penguins in each 'group'. This yields long-form data that is good for scatterplots. 

As far as I know, there isn't an attached method for horizontal jitter for seaborn, so we can add the jitter manually to separate the 'male' and 'female' points slightly for our scatterplot. 

Finally, we can use the .pivot_table method to generate the count data for the heatmap as wide-form data. 

</aside>

## Scatter data & Heatmap Data

```{python}
display(scatter_data.head(3))
```

```{python}
display(heatmap_data)
```

<aside class="notes">
After the transformations we performed, this is what our scatter data and heatmap data look like. 
</aside>

## Creating Plots 

```{python}
#| code-line-numbers: "1-2|4-9|11-24"
#| output-location: slide
# Prepare the figure with 2 subplots; the axes object will contain both plots
fig2, axes = plt.subplots(1, 2, figsize=(16, 7)) ## 1 row 2 columns

# Plot heatmap on the first axis
sns.heatmap(data = heatmap_data, cmap="Blues", linewidths=0.5, linecolor='white', annot=True, 
fmt='d', ax=axes[0]) ## fmt='d' = decimal (base10) integer, use fmt='f' for floats 
axes[0].set_title("Heatmap of KMeans Clustering by Species")
axes[0].set_xlabel("Species")
axes[0].set_ylabel("KMeans Cluster")

# Scatterplot with jitter
sns.scatterplot(data=scatter_data, x="x_jittered", y="kmeans_cluster",
    hue="species", style="sex", size="count", sizes=(100, 500),
    alpha=0.8, ax=axes[1], legend="brief")
axes[1].set_xticks(range(len(species_order)))
axes[1].set_xticklabels(species_order)
axes[1].set_title("Cluster Assignment by Species and Sex (Jittered)")
axes[1].set_ylabel("KMeans Cluster")
axes[1].set_xlabel("Species")
axes[1].set_yticks([0, 1, 2])
axes[1].legend(bbox_to_anchor=(1.05, 0.5), loc='center left', borderaxespad=0.0, title="Legend")

fig2.tight_layout()
#fig2.show()
```

<aside class="notes">
Now we can create our plots on this new figure, fig2. Because we want to create more than 1 plot, we have additional arguments in plt.subplots. 1, 2 means 1 row 2 columns. 

This time, our 'axis' is actually a 1d array of axis objects. If we had more than 1 row, it would be a 2d array (like a matrix). **-adv-** 
If we want to plot the heatmap on the first 'ax', we can set ax=axes[0]. Here, we don't have to explicitly set X and Y, we can just pass the data into the heatmap function.

**-adv-** For our scatterplot, we need to set x and y explicitly, using the x_jittered column so our points don't overlap. 
To make things clean here, we can set the xticks using the species_order we defined previously and put our legend to the left of the plot.

Note: bbox_to_anchor -> x coord, y coord in terms of plot size. Aka a little more than 1 plot to the left and half a plot up. 


</aside>

## Project 2: KNN classification

<div class="clean-text"> 

For our KNN classification, the model is **supervised** (meaning it is dependent on the outcome 'y' data). This time, we need to split our data into a training and test set. <br>

</div>

. . . 

The **function** `train_test_split()` from scikit-learn is helpful here! 

```{python}
# Splitting dataset into training and testing sets (still using scaled X!)
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)
```

. . . 

>Unlike R functions, which return a single object (often a list when multiple outputs are needed), Python functions can return multiple values as a tuple‚Äîletting you unpack them directly into separate variables.

<aside class="notes">
Now we can move on to our KNN classification. Because this model is supervised, it is dependent on the species data. To have a meaningful model, we need to split our data into a training and test set, 
which we can do using the function `train_test_split()`. We are still using the scaled X from before, but we want to reserve 30% of our data for testing. 

We can assign the outputs directly to different variables. 

(help(train_test_split) will show what the output order is)
</aside>

## Understanding KNeighborsClassifier class

```{.python}
class sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, *, weights='uniform', 
algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None)
```
. . . 

**Parameters**: Set by user at time of instantiation    
- n_neigbors, weights, algorithm, etc.  <br>

**Attributes**: Store object data     
- `classes_`: class labels known to the classifier  
- `effective_metric_`: distance metric used  
- `effective_metric_params_`: parameters for the metric function  
- `n_features_in` and `feature_names_in_`: store info about features seen during fit    
- `n_samples_fit_`: number of samples in fitted data  <br> 

**Methods**: Define object behaviors      
- `.fit(X, y)`: fit knn classifier from training dataset (X and y)  
- `.predict(X)`: predict class labels for provided data X  
- `.predict_proba(X)`: return probability estimates for test data X  
- `.score(X, y)`: return mean accuracy on given test data X and labels y  

<aside class="notes">
The KNeighbors classifier class is similar to the kmeans class, with a notable difference. The fit method here requires both X and y. 
Technically, the fit method in kmeans can accept a y value, it just won't use it. This is done to allow for interchangable models in pipelines. 

The neighbors classifier also lacks the transform method because it doesn't create a 'space' like the kmeans cluster-distance space. 
Instead, it has the .predict_proba method, which gives class probability estimates for input data. 

</aside>

## Making an instance of KNeighborsClassifier and fitting to training data
- For a supervised model, y_train is included in `.fit()`!
```{python}
#| output: true
## perform knn classification
# Applying k-NN classification with 5 neighbors
knn = KNeighborsClassifier(n_neighbors=5) ## make an instance of the KNeighborsClassifier class
# and set the n_neighbors parameter to be 5. 

# Use the fit method to fit the model to the training data
knn.fit(X_train, y_train)
knn
```

<aside class="notes">
Fitting the knn model works the same as fitting the kmeans model, except we have to supply the 'y' values. 
</aside>

## Once the model is fit...

-We can look at its attributes (ex: `.classes_`) which gives the class labels as known to the classifier

```{python}
#| output-location: fragment
print(knn.classes_)
```

. . . 

-And use fitted model to predict species for test data

```{python}
#| output-location: fragment
# Use the predict method on the test data to get the predictions for the test data
y_pred = knn.predict(X_test)

# Also can take a look at the prediction probabilities, 
# and use the .classes_ attribute to put the column labels in the right order
probs = pd.DataFrame(
    knn.predict_proba(X_test),
    columns = knn.classes_)
probs['y_pred'] = y_pred

print("Predicted probabilities: \n", probs.head())
```

<aside class="notes">
After fitting, we can see that the classifier has learned the labels from the dataset, and we are able to use it to predict species labels for the test data. 

We use the .predict method to get the predicted classes and the .predict proba method to get the actual probabilities. 
We can store both of these in a pandas dataframe, which we can use to make a scatterplot to compare the actual and predicted classes. 
</aside>

## Scatterplot for k-NN classification of test data

<div class="clean-text"> 

- Create dataframe of unscaled X_test, `bill_length_mm`, and `bill_depth_mm`.
- Add to it the actual and predicted species labels

```{python}
#| output-location: fragment
## First unscale the test data
X_test_unscaled = scaler.inverse_transform(X_test)

## create dataframe 
penguins_test = pd.DataFrame(
    X_test_unscaled,
    columns=['bill_length_mm', 'bill_depth_mm']
)

## add actual and predicted species 
penguins_test['y_actual'] = y_test.values
penguins_test['y_pred'] = y_pred
penguins_test['correct'] = penguins_test['y_actual'] == penguins_test['y_pred']

print("Results: \n", penguins_test.head())
```

<aside class="notes">
Using the scaler object we created previously, we can un-scale the test data and create a new dataframe that contains the unscaled predictors as well as the true and predicted species. 

Line 13 here creates a boolean (true/false) column for agreement between the actual and predicted species. 
</aside>

</div>

## Plotnine scatterplot for k-NN classification of test data

To see how well our model did at classifying the remaining penguins...

```{python}
#| output-location: slide
## Build the plot
plot3 = (ggplot(penguins_test, aes(x="bill_length_mm", y="bill_depth_mm", 
color="y_actual", fill = 'y_pred', shape = 'correct'))
 + geom_point(size=4, stroke=1.1)  # Stroke controls outline thickness
 + scale_shape_manual(values={True: 'o', False: '^'})  # Circle and triangle
 + ggtitle("k-NN Classification Results")
 + theme_bw())

display(plot3)
```

<aside class="notes">
Now we can put all of this together into a plot. This plot is overely complicated, but I wanted to use it to show some more of the plot options in plotnine. 
</aside>

## Visualizing Decision Boundary with seaborn and matplotlib

```{python}
#| code-line-numbers: "1-6|8-22|24-27|"
#| output-location: slide
from sklearn.inspection import DecisionBoundaryDisplay
from sklearn.preprocessing import LabelEncoder

# Create and fit label encoder for y (just makes y numeric because it makes the scatter plot happy)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Create the plot objects
fig, ax = plt.subplots(figsize=(12, 8))

# Create display object
disp = DecisionBoundaryDisplay.from_estimator(
    knn,
    X_test,
    response_method = 'predict',
    plot_method = 'pcolormesh',
    xlabel = "bill_length_scaled",
    ylabel = "bill_depth_scaled",
    shading = 'auto',
    alpha = 0.5,
    ax = ax
)

# Use method from display object to create scatter plot
scatter = disp.ax_.scatter(X_scaled[:,0], X_scaled[:,1], c=y_encoded, edgecolors = 'k')
disp.ax_.legend(scatter.legend_elements()[0], knn.classes_, loc = 'lower left', title = 'Species')
_ = disp.ax_.set_title("Penguin Classification")

fig.show()
```

<aside class="notes">
To visualize the decision boundary of the modelm we can use another sklearn class called DecisionBoundaryDisplay, 
which creates a display object that has matplotlib-style plotting methods. 
We assign our axis in the creation of our display object and use the attached .ax_. methods to create the scatter plot and legend as well as set the title. 



</aside>


## Evaluate KNN performance

<div class="clean-text"> 
To check the performance of our KNN classifier, we can check the accuracy score and print a classification report.  
- `accuracy_score` and `classification_report` are both functions!  
- They are not unique to scikit-learn classes so it makes sense for them to be functions not methods  

</div> 

```{python}
#| output-location: fragment
## eval knn performance
knn_accuracy = accuracy_score(y_test, y_pred)
print(f"k-NN Accuracy: {knn_accuracy:.2f}")
print("Classification Report: \n", classification_report(y_test, y_pred))
```

## Make a Summary Table of Metrics for Both Models

```{python}
#| output-location: fragment
summary_table = pd.DataFrame({
    "Metric": ["k-Means Adjusted Rand Index", "k-NN Accuracy"],
    "Value": [kmeans_ari, knn_accuracy]
})
(
    GT(summary_table)
    .tab_header(title = "Model Results Summary")
    .fmt_number(columns = "Value", n_sigfig = 2)
    .tab_options(table_font_size = 20)
)
```


## Key Takeaways from This Session

<div class="clean-text"> 
<p style="font-size: 1.25em"> 
</p>
<ul style="font-size: 1.1em; margin: 0 auto;">
  <li><strong>Python workflows rely on object-oriented structures in addition to functions:</strong><br>
   Understanding the OOP paradigm makes Python a lot easier!</li>
  <li><strong>Everything is an object!</strong></li>
  <li><strong>Duck Typing:</strong><br>
  If an object has a method, that method can be called regardless of the object type. Caveat being, make sure the arguments (if any) in the method are specified correctly for all objects!
  </li>
    <li>Python packages use <strong>common methods</strong> that make it easy to change between model types without changing a lot of code. 
  </li>
</ul>
</div>

## Additional Insights

<div class="clean-text" style="font-size: 1.0em; line-height: 1.4;">
  <ul>
    <li>
      <strong>Predictable APIs enable seamless model switching:</strong><br>
      Swapping models like <code>LogisticRegression</code> ‚Üí <code>RandomForestClassifier</code> usually requires minimal code changes.
    </li>
    <li style="margin-top: .7em;">
      <strong>scikit-learn prioritizes interoperability:</strong><br>
      Its consistent class design integrates with tools like <code>Pipeline</code>, <code>GridSearchCV</code>, and <code>cross_val_score</code>.
    </li>
    <li style="margin-top: .7em;">
      <strong>Class attributes improve model transparency:</strong><br>
      Access attributes like <code>.coef_</code>, <code>.classes_</code>, and <code>.feature_importances_</code> for model interpretation and debugging.
    </li>
    <li style="margin-top: .7em;">
      <strong>Custom classes are central to deep learning:</strong><br>
      Frameworks like PyTorch and TensorFlow require you to define your own model classes by subclassing base models.
    </li>
    <li style="margin-top: .7em;">
      <strong>Mixins support modular design:</strong><br>
      Mixins (e.g., <code>ClassifierMixin</code>) let you add specific functionality without duplicating code.
    </li>
  </ul>
</div>

## **Pre-Reading for This Session**  

<div class="clean-text">

- [Scikit-learn Documentation](https://scikit-learn.org/stable/user_guide.html)  
- [Introduction to OOP in Python (Real Python)](https://realpython.com/python3-object-oriented-programming/)  
- [Plotnine Reference](https://plotnine.org/reference/)
- [Seaborn Reference](https://seaborn.pydata.org/tutorial/function_overview.html)

</div>